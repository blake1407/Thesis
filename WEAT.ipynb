{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTest: Take out 20 randomly (same length in each bucket), \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "from itertools import combinations\n",
    "from collections import OrderedDict\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import spatial\n",
    "from math import sqrt\n",
    "\n",
    "\"\"\"\n",
    "Test: Take out 20 randomly (same length in each bucket), \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_std(values, weights):\n",
    "    # values = numpy ndarray with the same shape as weights\n",
    "    # weights = numpy ndarray with the same shape as values\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    # Small sample size bias correction:\n",
    "    variance_ddof1 = variance*len(values)/(len(values)-1)\n",
    "    return sqrt(variance_ddof1)\n",
    "\n",
    "def within_group_cohesion(X):\n",
    "    # X = 2D numpy array of vectors for all words belonging to group X\n",
    "    dist = spatial.distance.pdist(X, 'cosine')\n",
    "    return dist.mean()\n",
    "\n",
    "def sim(x, A, B):\n",
    "    # x = ndarray for each word x in category X\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    x = np.array(x)\n",
    "    x_ = x.reshape(1, -1)\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    # print(f\"Shape of x: {x.shape}\")\n",
    "    # print(f\"Shape of A: {A.shape}\")\n",
    "    # print(f\"Shape of B: {B.shape}\")\n",
    "    results_A = spatial.distance.cdist(x_, A, 'cosine')\n",
    "    sum_A = (1 - results_A).sum()\n",
    "    results_B = spatial.distance.cdist(x_, B, 'cosine')\n",
    "    sum_B = (1 - results_B).sum()\n",
    "    difference = sum_A/len(A) - sum_B/len(B)\n",
    "\n",
    "    return difference\n",
    "\n",
    "def group_cohesion_test(X, Y, perm_n = 1000, permtype = 1):\n",
    "    # X = 2D numpy array of vectors for all words belonging to group X\n",
    "    # Y = 2D numpy array of vectors for all words belonging to group Y\n",
    "    # perm_n = number of permutations\n",
    "    # permtype = permutation type.\n",
    "    test_statistic = np.average((within_group_cohesion(X), within_group_cohesion(Y)), weights = (len(X), len(Y)))\n",
    "    jointlist = np.concatenate((X,Y))\n",
    "    permutations = np.array([])\n",
    "    if permtype == 1:\n",
    "        count = 0\n",
    "        cutpoint = len(X)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations = np.append(permutations, \n",
    "                                     np.average([within_group_cohesion(set1), within_group_cohesion(set2)], \n",
    "                                                weights = [len(set1), len(set2)]))\n",
    "            count += 1\n",
    "    else:\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(X)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations = np.append(permutations, \n",
    "                                     np.average([within_group_cohesion(set1), within_group_cohesion(set2)], \n",
    "                                                weights = [len(set1), len(set2)]))\n",
    "    P_val = (sum(i <= test_statistic for i in permutations)+1)/(len(permutations)+1)\n",
    "    return P_val\n",
    "\n",
    "def diff_sim(X, A, B, effect=1, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    # effect = boolean for whether standard deviation & effect size need to be calculated\n",
    "    # Y = optional. ndarray for words in category Y\n",
    "    if Y:\n",
    "        sum_X = 0\n",
    "        sum_Y = 0\n",
    "\n",
    "        # print(f\"Len X: {len(X)}\")\n",
    "        # print(f\"Len Y: {len(Y)}\")\n",
    "        for x in X:\n",
    "            x = np.array([x])\n",
    "            sum_X += sim(x, A, B)\n",
    "        for y in Y:\n",
    "            y = np.array([y])\n",
    "            sum_Y += sim(y, A, B)\n",
    "        # difference = sum_X/len(X) - sum_Y/len(Y)\n",
    "        difference = sum_X/len(X) - sum_Y/len(Y)\n",
    "        all_sims = []\n",
    "        for w in (np.concatenate((X,Y))):\n",
    "            all_sims.append(sim(w, A, B))\n",
    "        # For SD calculation, assign weights based on frequency of opposite category\n",
    "        weights = [len(Y) for num in range(len(X))] + [len(X) for num in range(len(Y))]\n",
    "        standard_dev = weighted_std(all_sims, weights)\n",
    "        if standard_dev == 0:\n",
    "            effect_size = 0\n",
    "        else:\n",
    "            effect_size = difference/standard_dev\n",
    "    else:\n",
    "        sum_A = 0\n",
    "        sum_B = 0\n",
    "        all_sims = []\n",
    "        for a in A:\n",
    "            a_ = a.reshape(1, -1)\n",
    "            results = spatial.distance.cdist(a_, X, 'cosine')\n",
    "            sum_X = (1 - results).sum()\n",
    "            val = sum_X/len(X)\n",
    "            sum_A += val\n",
    "            all_sims.append(val)\n",
    "        ave_A = sum_A/len(A)\n",
    "        for b in B:\n",
    "            b_ = b.reshape(1, -1)\n",
    "            results = spatial.distance.cdist(b_, X, 'cosine')\n",
    "            sum_X = (1 - results).sum()\n",
    "            val = sum_X/len(X)\n",
    "            sum_B += val\n",
    "            all_sims.append(val)\n",
    "        ave_B = sum_B/len(B)\n",
    "        difference = ave_A - ave_B\n",
    "        standard_dev = np.std(all_sims, ddof=1)\n",
    "        if standard_dev == 0:\n",
    "            effect_size = 0\n",
    "        else:\n",
    "            effect_size = difference/standard_dev\n",
    "    if effect == 1:\n",
    "        return difference, standard_dev, effect_size\n",
    "    else:\n",
    "        return difference    \n",
    "\n",
    "def permutation_test(X, A, B, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # Y = ndarray for words in category Y\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    if Y:\n",
    "        jointlist = np.array(list(X) + list(Y))\n",
    "        permutations = []\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(X)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations.append(diff_sim(set1, set2, A, B))\n",
    "    else:\n",
    "        jointlist = np.array(list(A) + list(B))\n",
    "        permutations = []\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(A)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations.append(diff_sim(X, set1, set2, effect=0))\n",
    "    return permutations\n",
    "\n",
    "def rand_test(X, A, B, perm_n, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # Y = ndarray for words in category Y\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    # perm_n = number of permutations\n",
    "    if Y:\n",
    "        jointlist = np.array(list(X) + list(Y))\n",
    "        np.random.shuffle(jointlist)\n",
    "        permutations = []\n",
    "        count = 0\n",
    "        cutpoint = len(X)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations.append(diff_sim(set1, set2, A, B))\n",
    "            count += 1\n",
    "    else:\n",
    "        jointlist = np.array(list(A) + list(B))\n",
    "        np.random.shuffle(jointlist)\n",
    "        permutations = []\n",
    "        count = 0\n",
    "        cutpoint = len(A)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations.append(diff_sim(X, set1, set2, effect=0))\n",
    "            count += 1\n",
    "    return permutations\n",
    "\n",
    "### DOUBLE CATEGORY WEAT\n",
    "\n",
    "def weat(X_name, X, Y_name, Y, A_name, A, B_name, B, \n",
    "         permt=0, perm_n=10000, cohesion_test=False, cohesion_permutations=1000, cohesion_type=2):\n",
    "    # X_name = name of category 1. Will be used in result output.\n",
    "    # X = category 1. Input should be iterable and contain numpy array(s) for words in category 1\n",
    "    # Y_name = name of category 2. Will be used in result output.\n",
    "    # Y = category 2. Input should be iterable and contain numpy array(s) for words in category 2\n",
    "    # A_name = name of attribute 1. Will be used in result output.\n",
    "    # A = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 1\n",
    "    # B_name = name of attribute 2. Will be used in result output.\n",
    "    # B = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 2\n",
    "    # permt = do you want to perform a permutation test? 0 = no, 1 = yes, 2 = yes, with the perm_n specified\n",
    "    # perm_n = number of permutations\n",
    "    # cohesion_test = boolean for testing within-category cohesion\n",
    "    # cohesion_permutations = number of permutations for cohesion test\n",
    "    # cohesion_type = type of cohesion test. 1 = test cohesion of only one group, 2 = test cohesion of both groups\n",
    "    \n",
    "    # Calculate effect size\n",
    "    difference, standard_dev, effect_size = diff_sim(X=X, Y=Y, A=A, B=B, effect=1)\n",
    "    decimal = 6\n",
    "    result_dict = OrderedDict({\"categories\": [X_name, Y_name],\n",
    "                               \"attributes\": [A_name, B_name],\n",
    "                               \"difference\": round(difference,decimal),\n",
    "                               \"standard_dev\": round(standard_dev,decimal),\n",
    "                               \"effect_size\": round(effect_size, decimal)})\n",
    "    \n",
    "    # Permutations if permt is not 0\n",
    "    if permt == 1 or permt == 2:\n",
    "        if permt == 1:\n",
    "            permutations = np.array(permutation_test(X=X, Y=Y, A=A, B=B))\n",
    "        elif permt == 2:\n",
    "            permutations = np.array(rand_test(X=X, Y=Y, A=A, B=B, perm_n=perm_n))\n",
    "        perm_mean = np.mean(permutations)\n",
    "        permutations = permutations - perm_mean\n",
    "        sum_c = effect_size - perm_mean\n",
    "        Pleft = (sum(i <= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Pright = (sum(i >= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Ptot = (sum(abs(i) >= abs(sum_c) for i in permutations)+1)/(len(permutations)+1)\n",
    "        se = np.std(permutations)\n",
    "        result_dict[\"Pleft\"] = round(Pleft,decimal)\n",
    "        result_dict[\"Pright\"] = round(Pright,decimal)\n",
    "        result_dict[\"Ptot\"] = round(Ptot,decimal)\n",
    "        result_dict[\"se\"] = round(se, decimal)\n",
    "    \n",
    "    # Cohesion test if cohesion_test is true\n",
    "    if cohesion_test == True:\n",
    "        cohesion_categories = group_cohesion_test(X=X, Y=Y, perm_n=cohesion_permutations, permtype=cohesion_type)\n",
    "        cohesion_attributes = group_cohesion_test(X=A, Y=B, perm_n=cohesion_permutations, permtype=cohesion_type)\n",
    "        result_dict[\"cohesion_categories\"] = cohesion_categories\n",
    "        result_dict[\"cohesion_attributes\"] = cohesion_attributes\n",
    "        \n",
    "    return result_dict    \n",
    "\n",
    "### SINGLE CATEGORY WEAT\n",
    "\n",
    "def s_weat(X_name, X, A_name, A, B_name, B, permt = 0, perm_n = 10000):\n",
    "    # X_name = name of category 1. Will be used in result output.\n",
    "    # X = category 1. Input should be iterable and contain numpy array(s) for words in category 1\n",
    "    # A_name = name of attribute 1. Will be used in result output.\n",
    "    # A = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 1\n",
    "    # B_name = name of attribute 2. Will be used in result output.\n",
    "    # B = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 2\n",
    "    # permt = do you want to perform a permutation test? 0 = no, 1 = yes, 2 = yes, with the perm_n specified\n",
    "    # perm_n = number of permutations\n",
    "\n",
    "    difference, standard_dev, effect_size = diff_sim(X=X, A=A, B=B)\n",
    "    decimal = 6\n",
    "    result_dict = OrderedDict({\"category\": [X_name],\n",
    "                               \"attributes\": [A_name, B_name],\n",
    "                               \"difference\": round(difference, decimal),\n",
    "                               \"standard_dev\": round(standard_dev,6),\n",
    "                               \"effect_size\": round(effect_size, decimal)})\n",
    "    if permt == 1 or permt == 2:\n",
    "        if permt == 1:\n",
    "            permutations = np.array(permutation_test(X, A, B))\n",
    "        elif permt == 2:\n",
    "            permutations = np.array(rand_test(X, A, B, perm_n = perm_n))\n",
    "        perm_mean = np.mean(permutations)\n",
    "        permutations = permutations - perm_mean\n",
    "        sum_c = difference - perm_mean\n",
    "        Pleft = (sum(i <= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Pright = (sum(i >= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Ptot = (sum(abs(i) >= abs(sum_c) for i in permutations)+1)/(len(permutations)+1)\n",
    "        result_dict[\"Pleft\"] = round(Pleft, decimal)\n",
    "        result_dict[\"Pright\"] = round(Pright,decimal)\n",
    "        result_dict[\"Ptot\"] = round(Ptot, decimal)\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stereotype dictionary\n",
    "dictionary = \"Create Dictionary/Stereotype_Dictionary.json\"\n",
    "with open(dictionary, 'r') as f:\n",
    "    stereotype_dict = json.load(f)\n",
    "\n",
    "stereotype_df = pd.DataFrame(list(stereotype_dict.items()), columns=['category', 'word'])\n",
    "stereotype_df = stereotype_df.explode(\"word\")\n",
    "stereotype_df[\"word\"] = stereotype_df[\"word\"].str.lower()\n",
    "\n",
    "stereotype_list = stereotype_df.word.tolist()\n",
    "\n",
    "stereotype_df.head()\n",
    "\n",
    "def load_word2vec_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    word2vec_model = {}\n",
    "    with open(File,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0].lower().strip()\n",
    "            if word in stereotype_list:\n",
    "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                embedding = np.round(embedding, 10)\n",
    "                word2vec_model[word] = embedding\n",
    "    print(f\"{len(word2vec_model)} words loaded!\")\n",
    "    word2vec_df = pd.DataFrame(list(word2vec_model.items()), columns=['word', 'vector'])\n",
    "    merged_df = word2vec_df.merge(stereotype_df, on=\"word\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "158 words loaded!\n",
      "Loading Glove Model\n",
      "28 words loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>far</td>\n",
       "      <td>[-0.48308292, -0.483188093, -0.0779211596, 0.2...</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>far</td>\n",
       "      <td>[0.134767666, -0.435717076, 0.912166715, -0.47...</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>far</td>\n",
       "      <td>[-0.167683467, -0.322603464, 0.395895481, 0.13...</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>far</td>\n",
       "      <td>[-0.241728961, 0.0443804562, 0.278668374, -0.4...</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>far</td>\n",
       "      <td>[0.0780541077, -0.379881799, 0.224797666, -0.1...</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id word                                             vector category\n",
       "0          87  far  [-0.48308292, -0.483188093, -0.0779211596, 0.2...     Cold\n",
       "1          87  far  [0.134767666, -0.435717076, 0.912166715, -0.47...     Cold\n",
       "2          87  far  [-0.167683467, -0.322603464, 0.395895481, 0.13...     Cold\n",
       "3          87  far  [-0.241728961, 0.0443804562, 0.278668374, -0.4...     Cold\n",
       "4          87  far  [0.0780541077, -0.379881799, 0.224797666, -0.1...     Cold"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_after_vecs = load_word2vec_model(\"Tokenized Data/After_word2vec_vectors.txt\")\n",
    "word2vec_before_vecs = load_word2vec_model(\"Tokenized Data/Before_word2vec_vectors.txt\")\n",
    "\n",
    "BERT_after_vecs = pd.read_csv(\"Tokenized Data/BERT_After_word_vectors.csv\")\n",
    "BERT_before_vecs = pd.read_csv(\"Tokenized Data/BERT_Before_word_vectors.csv\")\n",
    "\n",
    "BERT_after_vecs = BERT_after_vecs.merge(stereotype_df, on=\"word\")\n",
    "BERT_before_vecs = BERT_before_vecs.merge(stereotype_df, on=\"word\")\n",
    "\n",
    "BERT_after_vecs.head()\n",
    "# sterotypes_df = pd.read_csv(\"Create Dictionary/word2vec_Stereotype_word_embeddings.csv\")\n",
    "# sterotypes_df.head()\n",
    "\n",
    "def fix_format(vector_str):\n",
    "    # Add a comma between numbers using regex\n",
    "    fixed_str = re.sub(r\"(?<=[0-9])\\s+(?=-?\\d)\", \", \", vector_str)\n",
    "    return fixed_str\n",
    "\n",
    "\n",
    "def safe_literal_eval(vector_str):\n",
    "    try:\n",
    "        return ast.literal_eval(vector_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating: {vector_str} | Error: {e}\")\n",
    "        return np.nan\n",
    "    \n",
    "BERT_after_vecs[\"vector\"] = BERT_after_vecs[\"vector\"].apply(fix_format)\n",
    "BERT_after_vecs[\"vector\"] = BERT_after_vecs[\"vector\"].apply(safe_literal_eval)\n",
    "BERT_before_vecs[\"vector\"] = BERT_before_vecs[\"vector\"].apply(fix_format)\n",
    "BERT_before_vecs[\"vector\"] = BERT_before_vecs[\"vector\"].apply(safe_literal_eval)\n",
    "\n",
    "#Round the vector by 10 decimal\n",
    "BERT_after_vecs[\"vector\"] = BERT_after_vecs[\"vector\"].apply(\n",
    "    lambda x: np.round(np.array(x), decimals=10).tolist() if isinstance(x, list) else x\n",
    ")\n",
    "BERT_before_vecs[\"vector\"] = BERT_before_vecs[\"vector\"].apply(\n",
    "    lambda x: np.round(np.array(x), decimals=10).tolist() if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "BERT_after_vecs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m\n\u001b[0;32m     28\u001b[0m competence_avg \u001b[38;5;241m=\u001b[39m calculate_average_vector([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompetence\u001b[39m\u001b[38;5;124m'\u001b[39m], combined_df)\n\u001b[0;32m     29\u001b[0m incompetence_avg \u001b[38;5;241m=\u001b[39m calculate_average_vector([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncompetence\u001b[39m\u001b[38;5;124m'\u001b[39m], combined_df)\n\u001b[1;32m---> 33\u001b[0m BERT_avg \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArabic/Muslim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIsraeli/Jewish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWarm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCompetence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIncompetence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43marabic_muslim_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misraeli_jewish_avg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Can you run the WEAT with the before and after embeddings?\n",
    "# So in a double category weat your group A is Jewish/Israeli pre, \n",
    "# group B Jewish/Israeli post, attribute A warmth pre and attribute B warmth post. \n",
    "# Then you can do an average of the vectors for Israeli/Jewish and Arabic/Muslim from the pre and post periods. \n",
    "# Now run the double category WEAT with mean Israeli/Jewish as group A, \n",
    "# mean Arabic/Muslim as group B, warmth pre as attribute A and warmth post as attribute B. \n",
    "# Do these two tests for each of the qualities.\n",
    "\n",
    "# Function to calculate average vector\n",
    "def calculate_average_vector(categories, vectors_df):\n",
    "    # Filter the dataframe for the specific category\n",
    "    category_df = vectors_df[vectors_df['category'].isin(categories)]\n",
    "    # Convert vectors to numpy arrays and calculate mean\n",
    "    if len(category_df) > 0:\n",
    "        # Ensure all vectors are numpy arrays\n",
    "        vectors = np.array(category_df['vector'].tolist())\n",
    "        avg_vector = np.mean(vectors, axis=0)\n",
    "        return avg_vector\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "combined_df = pd.concat([BERT_before_vecs, BERT_after_vecs])\n",
    "\n",
    "arabic_muslim_avg = calculate_average_vector(['Arabic', 'Muslim'], combined_df)\n",
    "israeli_jewish_avg = calculate_average_vector(['Israeli', 'Jewish'], combined_df)\n",
    "warm_avg = calculate_average_vector(['Warm'], combined_df)\n",
    "cold_avg = calculate_average_vector(['Cold'], combined_df)\n",
    "competence_avg = calculate_average_vector(['Competence'], combined_df)\n",
    "incompetence_avg = calculate_average_vector(['Incompetence'], combined_df)\n",
    "\n",
    "\n",
    "\n",
    "BERT_avg = pd.DataFrame({\n",
    "    'category': ['Arabic/Muslim', 'Israeli/Jewish', 'Warm', 'Cold', 'Competence', 'Incompetence'],\n",
    "    'vector': [arabic_muslim_avg, israeli_jewish_avg, warm_avg, cold_avg, competence_avg, incompetence_avg],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     39\u001b[0m         B\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray((array)))\n\u001b[1;32m---> 41\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m \u001b[43mweat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mY_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mA_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mB_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mpermt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcohesion_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcohesion_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcohesion_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_dict)\n\u001b[0;32m     48\u001b[0m results[(group, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m=\u001b[39m result_dict\n",
      "Cell \u001b[1;32mIn[2], line 212\u001b[0m, in \u001b[0;36mweat\u001b[1;34m(X_name, X, Y_name, Y, A_name, A, B_name, B, permt, perm_n, cohesion_test, cohesion_permutations, cohesion_type)\u001b[0m\n\u001b[0;32m    210\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(permutation_test(X\u001b[38;5;241m=\u001b[39mX, Y\u001b[38;5;241m=\u001b[39mY, A\u001b[38;5;241m=\u001b[39mA, B\u001b[38;5;241m=\u001b[39mB))\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m permt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mrand_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperm_n\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    213\u001b[0m perm_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(permutations)\n\u001b[0;32m    214\u001b[0m permutations \u001b[38;5;241m=\u001b[39m permutations \u001b[38;5;241m-\u001b[39m perm_mean\n",
      "Cell \u001b[1;32mIn[2], line 164\u001b[0m, in \u001b[0;36mrand_test\u001b[1;34m(X, A, B, perm_n, Y)\u001b[0m\n\u001b[0;32m    162\u001b[0m         set1 \u001b[38;5;241m=\u001b[39m jointlist[:cutpoint]\n\u001b[0;32m    163\u001b[0m         set2 \u001b[38;5;241m=\u001b[39m jointlist[cutpoint:]\n\u001b[1;32m--> 164\u001b[0m         permutations\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdiff_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    165\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mdiff_sim\u001b[1;34m(X, A, B, effect, Y)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m B:\n\u001b[0;32m    107\u001b[0m     b_ \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mspatial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     sum_X \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m results)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    110\u001b[0m     val \u001b[38;5;241m=\u001b[39m sum_X\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py:2939\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2938\u001b[0m     cdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mcdist_func\n\u001b[1;32m-> 2939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2941\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\spatial\\distance.py:1667\u001b[0m, in \u001b[0;36mCDistMetricWrapper.__call__\u001b[1;34m(self, XA, XB, out, **kwargs)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;66;03m# get cdist wrapper\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m cdist_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_distance_wrap, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdist_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wrap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1667\u001b[0m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dm\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "groups = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=x, X=X, \n",
    "                           Y_name=y, Y=Y, \n",
    "                           A_name=att1, A=A, \n",
    "                           B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_after_BERT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Warm', 'Cold']), ('difference', 0.093376), ('standard_dev', 0.070109), ('effect_size', 1.331872), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.036346)])\n",
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Competence', 'Incompetence']), ('difference', 0.021478), ('standard_dev', 0.037135), ('effect_size', 0.578397), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.027952)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Warm', 'Cold']), ('difference', -0.02759), ('standard_dev', 0.040129), ('effect_size', -0.687531), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.057282)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Competence', 'Incompetence']), ('difference', -0.000577), ('standard_dev', 0.0222), ('effect_size', -0.025995), ('Pleft', 0.157842), ('Pright', 0.843157), ('Ptot', 0.187812), ('se', 0.059885)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "groups = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    if y == \"Hamas\":\n",
    "        continue\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=x, X=X, \n",
    "                           Y_name=y, Y=Y, \n",
    "                           A_name=att1, A=A, \n",
    "                           B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_before_BERT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('categories', ['Israeli_Before', 'Israeli_After']), ('attributes', ['Warm_Before', 'Warm_After']), ('difference', 0.026992), ('standard_dev', 0.024558), ('effect_size', 1.099121), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.017202)])\n",
      "OrderedDict([('categories', ['Israeli_Before', 'Israeli_After']), ('attributes', ['Cold_Before', 'Cold_After']), ('difference', -0.015183), ('standard_dev', 0.029924), ('effect_size', -0.507408), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.017245)])\n",
      "OrderedDict([('categories', ['Israeli_Before', 'Israeli_After']), ('attributes', ['Competence_Before', 'Competence_After']), ('difference', -0.005884), ('standard_dev', 0.015108), ('effect_size', -0.38944), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.016952)])\n",
      "OrderedDict([('categories', ['Israeli_Before', 'Israeli_After']), ('attributes', ['Incompetence_Before', 'Incompetence_After']), ('difference', -0.003019), ('standard_dev', 0.025584), ('effect_size', -0.118021), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.018604)])\n",
      "OrderedDict([('categories', ['Jewish_Before', 'Jewish_After']), ('attributes', ['Warm_Before', 'Warm_After']), ('difference', -0.00403), ('standard_dev', 0.023833), ('effect_size', -0.169089), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.00493)])\n",
      "OrderedDict([('categories', ['Jewish_Before', 'Jewish_After']), ('attributes', ['Cold_Before', 'Cold_After']), ('difference', 0.011355), ('standard_dev', 0.037303), ('effect_size', 0.304392), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.006405)])\n",
      "OrderedDict([('categories', ['Jewish_Before', 'Jewish_After']), ('attributes', ['Competence_Before', 'Competence_After']), ('difference', -0.00099), ('standard_dev', 0.019081), ('effect_size', -0.051888), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.004382)])\n",
      "OrderedDict([('categories', ['Jewish_Before', 'Jewish_After']), ('attributes', ['Incompetence_Before', 'Incompetence_After']), ('difference', 0.013653), ('standard_dev', 0.039065), ('effect_size', 0.349479), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.006874)])\n",
      "OrderedDict([('categories', ['Arabic_Before', 'Arabic_After']), ('attributes', ['Warm_Before', 'Warm_After']), ('difference', -0.026331), ('standard_dev', 0.030339), ('effect_size', -0.867896), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.014633)])\n",
      "OrderedDict([('categories', ['Arabic_Before', 'Arabic_After']), ('attributes', ['Cold_Before', 'Cold_After']), ('difference', 0.049222), ('standard_dev', 0.04558), ('effect_size', 1.079903), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.017968)])\n",
      "OrderedDict([('categories', ['Arabic_Before', 'Arabic_After']), ('attributes', ['Competence_Before', 'Competence_After']), ('difference', -0.017719), ('standard_dev', 0.020349), ('effect_size', -0.87077), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.016447)])\n",
      "OrderedDict([('categories', ['Arabic_Before', 'Arabic_After']), ('attributes', ['Incompetence_Before', 'Incompetence_After']), ('difference', 0.0343), ('standard_dev', 0.046761), ('effect_size', 0.733529), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.018981)])\n",
      "OrderedDict([('categories', ['Muslim_Before', 'Muslim_After']), ('attributes', ['Warm_Before', 'Warm_After']), ('difference', 0.016251), ('standard_dev', 0.020074), ('effect_size', 0.809538), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.024825)])\n",
      "OrderedDict([('categories', ['Muslim_Before', 'Muslim_After']), ('attributes', ['Cold_Before', 'Cold_After']), ('difference', -0.021335), ('standard_dev', 0.022062), ('effect_size', -0.967051), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.028881)])\n",
      "OrderedDict([('categories', ['Muslim_Before', 'Muslim_After']), ('attributes', ['Competence_Before', 'Competence_After']), ('difference', -0.003082), ('standard_dev', 0.016107), ('effect_size', -0.191351), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.029185)])\n",
      "OrderedDict([('categories', ['Muslim_Before', 'Muslim_After']), ('attributes', ['Incompetence_Before', 'Incompetence_After']), ('difference', -0.00456), ('standard_dev', 0.019777), ('effect_size', -0.230552), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.028616)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Warm\"),\n",
    "         (\"Cold\", \"Cold\"), \n",
    "         (\"Competence\", \"Competence\"),\n",
    "         (\"Incompetence\", \"Incompetence\")]\n",
    "groups = [(\"Israeli\", \"Israeli\"),\n",
    "          (\"Jewish\", \"Jewish\"),\n",
    "          (\"Arabic\", \"Arabic\"), \n",
    "          (\"Muslim\", \"Muslim\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    if y == \"Hamas\":\n",
    "        continue\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=f\"{x}_Before\", X=X, \n",
    "                           Y_name=f\"{y}_After\", Y=Y, \n",
    "                           A_name=f\"{att1}_Before\", A=A, \n",
    "                           B_name=f\"{att2}_After\", B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_before&after_single_BERT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('categories', ['Arabic/Muslim', 'Israeli/Jewish']), ('attributes', ['Warm_Before', 'Warm_After']), ('difference', -0.000818), ('standard_dev', 0.000578), ('effect_size', -1.414214), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.003635)])\n",
      "OrderedDict([('categories', ['Arabic/Muslim', 'Israeli/Jewish']), ('attributes', ['Cold_Before', 'Cold_After']), ('difference', -0.008376), ('standard_dev', 0.005923), ('effect_size', -1.414214), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.006277)])\n",
      "OrderedDict([('categories', ['Arabic/Muslim', 'Israeli/Jewish']), ('attributes', ['Competence_Before', 'Competence_After']), ('difference', -0.002263), ('standard_dev', 0.0016), ('effect_size', -1.414214), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.004504)])\n",
      "OrderedDict([('categories', ['Arabic/Muslim', 'Israeli/Jewish']), ('attributes', ['Incompetence_Before', 'Incompetence_After']), ('difference', -0.010647), ('standard_dev', 0.007528), ('effect_size', -1.414214), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.007587)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Warm\"),\n",
    "         (\"Cold\", \"Cold\"), \n",
    "         (\"Competence\", \"Competence\"),\n",
    "         (\"Incompetence\", \"Incompetence\")]\n",
    "groups = [(\"Arabic/Muslim\", \"Israeli/Jewish\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = BERT_avg[BERT_avg[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = BERT_avg[BERT_avg[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=f\"{x}\", X=X, \n",
    "                           Y_name=f\"{y}\", Y=Y, \n",
    "                           A_name=f\"{att1}_Before\", A=A, \n",
    "                           B_name=f\"{att2}_After\", B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_conflated_before&afterattributes_BERT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Warm', 'Cold']), ('difference', 0.011488), ('standard_dev', 0.009914), ('effect_size', 1.158856), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.040816)])\n",
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Competence', 'Incompetence']), ('difference', 0.00363), ('standard_dev', 0.006322), ('effect_size', 0.574143), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.03427)])\n",
      "OrderedDict([('categories', ['IDF', 'Hamas']), ('attributes', ['Warm', 'Cold']), ('difference', 0.01086), ('standard_dev', 0.009616), ('effect_size', 1.129419), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.017364)])\n",
      "OrderedDict([('categories', ['IDF', 'Hamas']), ('attributes', ['Competence', 'Incompetence']), ('difference', -0.003002), ('standard_dev', 0.00855), ('effect_size', -0.351095), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.014789)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Warm', 'Cold']), ('difference', -0.002378), ('standard_dev', 0.00529), ('effect_size', -0.449539), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.058786)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Competence', 'Incompetence']), ('difference', 0.006478), ('standard_dev', 0.006281), ('effect_size', 1.031493), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.044526)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "groups = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=x, X=X, \n",
    "                           Y_name=y, Y=Y, \n",
    "                           A_name=att1, A=A, \n",
    "                           B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_after_word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Warm', 'Cold']), ('difference', 0.044281), ('standard_dev', 0.027412), ('effect_size', 1.615384), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.011293)])\n",
      "OrderedDict([('categories', ['Israeli', 'Arabic']), ('attributes', ['Competence', 'Incompetence']), ('difference', -0.039484), ('standard_dev', 0.031243), ('effect_size', -1.263766), ('Pleft', 0.000999), ('Pright', 1.0), ('Ptot', 0.000999), ('se', 0.004942)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Warm', 'Cold']), ('difference', 0.031849), ('standard_dev', 0.039755), ('effect_size', 0.801116), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.00076)])\n",
      "OrderedDict([('categories', ['Jewish', 'Muslim']), ('attributes', ['Competence', 'Incompetence']), ('difference', 0.085012), ('standard_dev', 0.054048), ('effect_size', 1.572896), ('Pleft', 1.0), ('Pright', 0.000999), ('Ptot', 0.000999), ('se', 0.002096)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "groups = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "\n",
    "for x,y in groups:\n",
    "    if x == \"IDF\":\n",
    "        continue\n",
    "    group = f\"{x, y}\"\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == x].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "        Y = []\n",
    "        Y_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == y].vector.tolist()\n",
    "        for array in Y_raw:\n",
    "            try:\n",
    "                Y.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                Y.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = weat(X_name=x, X=X, \n",
    "                           Y_name=y, Y=Y, \n",
    "                           A_name=att1, A=A, \n",
    "                           B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, \n",
    "                           cohesion_permutations=100, cohesion_type=0)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "weat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "weat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot','se']\n",
    "\n",
    "weat_df.to_csv('weat_before_word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single category WEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Post corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('category', ['Warm']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.021141), ('standard_dev', 0.013747), ('effect_size', 1.537886), ('Pleft', 1.0), ('Pright', 0.336663), ('Ptot', 0.336663)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['Jewish', 'Muslim']), ('difference', 0.048585), ('standard_dev', 0.041101), ('effect_size', 1.182072), ('Pleft', 1.0), ('Pright', 0.332667), ('Ptot', 0.653347)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.02314), ('standard_dev', 0.01728), ('effect_size', -1.339095), ('Pleft', 0.318681), ('Pright', 1.0), ('Ptot', 0.667333)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Jewish', 'Muslim']), ('difference', 0.016736), ('standard_dev', 0.013877), ('effect_size', 1.206045), ('Pleft', 1.0), ('Pright', 0.327672), ('Ptot', 0.672328)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.034481), ('standard_dev', 0.020906), ('effect_size', -1.649326), ('Pleft', 0.335664), ('Pright', 1.0), ('Ptot', 0.335664)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Jewish', 'Muslim']), ('difference', 0.03854), ('standard_dev', 0.039308), ('effect_size', 0.980464), ('Pleft', 1.0), ('Pright', 0.350649), ('Ptot', 0.662338)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.005003), ('standard_dev', 0.016715), ('effect_size', 0.299309), ('Pleft', 0.66034), ('Pright', 0.701299), ('Ptot', 1.0)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.046472), ('standard_dev', 0.031051), ('effect_size', -1.496645), ('Pleft', 0.325674), ('Pright', 1.0), ('Ptot', 0.325674)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "# groups = [\"Israeli\", \"Arabic\", \"IDF\", \"Hamas\", \"Jewish\", \"Muslim\"]\n",
    "\n",
    "tests = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "groups = [\"Warm\", \"Cold\", \"Competence\", \"Incompetence\"]\n",
    "\n",
    "for group in groups:\n",
    "    # if group == \"IDF\":\n",
    "    #     continue\n",
    "    for att1, att2 in tests:\n",
    "        if att1 == \"IDF\":\n",
    "            continue\n",
    "        X = []\n",
    "        X_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == group].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = word2vec_before_vecs[word2vec_before_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = s_weat(X_name=group, X=X, \n",
    "            A_name=att1, A=A, \n",
    "            B_name=att2, B=B, \n",
    "            permt=2, perm_n=1000)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "sweat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "sweat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat_before_word2vec.csv', index=False)\n",
    "sweat_df.to_csv('sweat_before_word2vec_fixed_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('category', ['Warm']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.140502), ('standard_dev', 0.119106), ('effect_size', 1.179639), ('Pleft', 0.975025), ('Pright', 0.036963), ('Ptot', 0.088911)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.007888), ('standard_dev', 0.074402), ('effect_size', -0.106025), ('Pleft', 0.495504), ('Pright', 0.508492), ('Ptot', 0.925075)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.021922), ('standard_dev', 0.102882), ('effect_size', -0.213076), ('Pleft', 0.31968), ('Pright', 0.682318), ('Ptot', 0.677323)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.129014), ('standard_dev', 0.114126), ('effect_size', 1.130449), ('Pleft', 0.965035), ('Pright', 0.047952), ('Ptot', 0.085914)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.018749), ('standard_dev', 0.074821), ('effect_size', -0.250585), ('Pleft', 0.355644), ('Pright', 0.65035), ('Ptot', 0.717283)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.019544), ('standard_dev', 0.101921), ('effect_size', -0.191753), ('Pleft', 0.34965), ('Pright', 0.651349), ('Ptot', 0.719281)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.06675), ('standard_dev', 0.066849), ('effect_size', 0.998523), ('Pleft', 0.91009), ('Pright', 0.100899), ('Ptot', 0.196803)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.006169), ('standard_dev', 0.041794), ('effect_size', -0.1476), ('Pleft', 0.440559), ('Pright', 0.561439), ('Ptot', 0.848152)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.008837), ('standard_dev', 0.056039), ('effect_size', -0.157685), ('Pleft', 0.396603), ('Pright', 0.605395), ('Ptot', 0.82018)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.063121), ('standard_dev', 0.063894), ('effect_size', 0.987893), ('Pleft', 0.856144), ('Pright', 0.157842), ('Ptot', 0.283716)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.003167), ('standard_dev', 0.047744), ('effect_size', -0.066332), ('Pleft', 0.494505), ('Pright', 0.508492), ('Ptot', 0.929071)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.015315), ('standard_dev', 0.059707), ('effect_size', -0.256505), ('Pleft', 0.345654), ('Pright', 0.655345), ('Ptot', 0.702298)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "# groups = [\"Israeli\", \"Arabic\", \"IDF\", \"Hamas\", \"Jewish\", \"Muslim\"]\n",
    "\n",
    "tests = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "groups = [\"Warm\", \"Cold\", \"Competence\", \"Incompetence\"]\n",
    "\n",
    "for group in groups:\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == group].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = word2vec_after_vecs[word2vec_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = s_weat(X_name=group, X=X, \n",
    "            A_name=att1, A=A, \n",
    "            B_name=att2, B=B, \n",
    "            permt=2, perm_n=1000)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "sweat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "sweat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat_after_word2vec.csv', index=False)\n",
    "sweat_df.to_csv('sweat_after_word2vec_fixed_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('category', ['Warm']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.006693), ('standard_dev', 0.044214), ('effect_size', -0.151372), ('Pleft', 0.068931), ('Pright', 0.932068), ('Ptot', 0.121878)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.00363), ('standard_dev', 0.057487), ('effect_size', -0.063147), ('Pleft', 0.34965), ('Pright', 0.651349), ('Ptot', 0.719281)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.003066), ('standard_dev', 0.046636), ('effect_size', -0.065737), ('Pleft', 0.298701), ('Pright', 0.702298), ('Ptot', 0.585415)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.007243), ('standard_dev', 0.054495), ('effect_size', -0.132901), ('Pleft', 0.087912), ('Pright', 0.913087), ('Ptot', 0.173826)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.002639), ('standard_dev', 0.075744), ('effect_size', -0.034839), ('Pleft', 0.464535), ('Pright', 0.536464), ('Ptot', 0.92008)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.006997), ('standard_dev', 0.056827), ('effect_size', -0.123127), ('Pleft', 0.141858), ('Pright', 0.859141), ('Ptot', 0.278721)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.00477), ('standard_dev', 0.042624), ('effect_size', -0.111901), ('Pleft', 0.140859), ('Pright', 0.86014), ('Ptot', 0.277722)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['IDF', 'Hamas']), ('difference', -0.001823), ('standard_dev', 0.055539), ('effect_size', -0.032818), ('Pleft', 0.425574), ('Pright', 0.575425), ('Ptot', 0.889111)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.003958), ('standard_dev', 0.04449), ('effect_size', -0.088956), ('Pleft', 0.234765), ('Pright', 0.766234), ('Ptot', 0.452547)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.006175), ('standard_dev', 0.053437), ('effect_size', -0.115553), ('Pleft', 0.098901), ('Pright', 0.902098), ('Ptot', 0.211788)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['IDF', 'Hamas']), ('difference', 0.000955), ('standard_dev', 0.074423), ('effect_size', 0.012831), ('Pleft', 0.507493), ('Pright', 0.493506), ('Ptot', 0.942058)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.006736), ('standard_dev', 0.055836), ('effect_size', -0.120645), ('Pleft', 0.142857), ('Pright', 0.858142), ('Ptot', 0.274725)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "# groups = [\"Israeli\", \"Arabic\", \"IDF\", \"Hamas\", \"Jewish\", \"Muslim\"]\n",
    "\n",
    "tests = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "groups = [\"Warm\", \"Cold\", \"Competence\", \"Incompetence\"]\n",
    "\n",
    "for group in groups:\n",
    "    for att1, att2 in tests:\n",
    "        X = []\n",
    "        X_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == group].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_after_vecs[BERT_after_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = s_weat(X_name=group, X=X, \n",
    "            A_name=att1, A=A, \n",
    "            B_name=att2, B=B, \n",
    "            permt=2, perm_n=1000)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "sweat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "sweat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat_after_BERT.csv', index=False)\n",
    "sweat_df.to_csv('sweat_after_BERT_fixed_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('category', ['Warm']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.082848), ('standard_dev', 0.098549), ('effect_size', 0.840679), ('Pleft', 0.866134), ('Pright', 0.145854), ('Ptot', 0.301698)])\n",
      "OrderedDict([('category', ['Warm']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.068867), ('standard_dev', 0.086205), ('effect_size', -0.798878), ('Pleft', 0.053946), ('Pright', 0.947053), ('Ptot', 0.17982)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Israeli', 'Arabic']), ('difference', -0.010528), ('standard_dev', 0.048576), ('effect_size', -0.216721), ('Pleft', 0.407592), ('Pright', 0.611389), ('Ptot', 0.851149)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.041277), ('standard_dev', 0.055535), ('effect_size', -0.743266), ('Pleft', 0.155844), ('Pright', 0.845155), ('Ptot', 0.320679)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.036647), ('standard_dev', 0.076146), ('effect_size', 0.48127), ('Pleft', 0.58042), ('Pright', 0.433566), ('Ptot', 0.841159)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.041411), ('standard_dev', 0.072225), ('effect_size', -0.573362), ('Pleft', 0.165834), ('Pright', 0.837163), ('Ptot', 0.331668)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Israeli', 'Arabic']), ('difference', 0.015168), ('standard_dev', 0.045575), ('effect_size', 0.332816), ('Pleft', 0.577423), ('Pright', 0.446553), ('Ptot', 0.838162)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Jewish', 'Muslim']), ('difference', -0.040834), ('standard_dev', 0.055169), ('effect_size', -0.740166), ('Pleft', 0.143856), ('Pright', 0.859141), ('Ptot', 0.2997)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "# groups = [\"Israeli\", \"Arabic\", \"IDF\", \"Hamas\", \"Jewish\", \"Muslim\"]\n",
    "\n",
    "tests = [(\"Israeli\", \"Arabic\"), (\"IDF\", \"Hamas\"), (\"Jewish\", \"Muslim\")]\n",
    "groups = [\"Warm\", \"Cold\", \"Competence\", \"Incompetence\"]\n",
    "\n",
    "for group in groups:\n",
    "    # if group == \"IDF\":\n",
    "    #     continue\n",
    "    for att1, att2 in tests:\n",
    "        if att1 == \"IDF\":\n",
    "            continue\n",
    "        X = []\n",
    "        X_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == group].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = s_weat(X_name=group, X=X, \n",
    "            A_name=att1, A=A, \n",
    "            B_name=att2, B=B, \n",
    "            permt=2, perm_n=1000)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "sweat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "sweat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat_before_BERT.csv', index=False)\n",
    "sweat_df.to_csv('sweat_before_BERT_fixed_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('category', ['Warm']), ('attributes', ['Israeli/Jewish', 'Arabic/Muslim']), ('difference', 0.007286), ('standard_dev', 0.005152), ('effect_size', 1.414214), ('Pleft', 1.0), ('Pright', 0.47952), ('Ptot', 0.47952)])\n",
      "OrderedDict([('category', ['Cold']), ('attributes', ['Israeli/Jewish', 'Arabic/Muslim']), ('difference', 0.012555), ('standard_dev', 0.008878), ('effect_size', 1.414214), ('Pleft', 1.0), ('Pright', 0.523477), ('Ptot', 1.0)])\n",
      "OrderedDict([('category', ['Competence']), ('attributes', ['Israeli/Jewish', 'Arabic/Muslim']), ('difference', 0.009009), ('standard_dev', 0.006371), ('effect_size', 1.414214), ('Pleft', 1.0), ('Pright', 0.492507), ('Ptot', 0.492507)])\n",
      "OrderedDict([('category', ['Incompetence']), ('attributes', ['Israeli/Jewish', 'Arabic/Muslim']), ('difference', 0.015173), ('standard_dev', 0.010729), ('effect_size', 1.414214), ('Pleft', 1.0), ('Pright', 0.503497), ('Ptot', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competence\", \"Incompetence\")]\n",
    "# groups = [\"Israeli\", \"Arabic\", \"IDF\", \"Hamas\", \"Jewish\", \"Muslim\"]\n",
    "\n",
    "tests = [(\"Israeli/Jewish\", \"Arabic/Muslim\")]\n",
    "groups = [\"Warm\", \"Cold\", \"Competence\", \"Incompetence\"]\n",
    "\n",
    "for group in groups:\n",
    "    # if group == \"IDF\":\n",
    "    #     continue\n",
    "    for att1, att2 in tests:\n",
    "        if att1 == \"IDF\":\n",
    "            continue\n",
    "        X = []\n",
    "        X_raw = BERT_before_vecs[BERT_before_vecs[\"category\"] == group].vector.tolist()\n",
    "        for array in X_raw:\n",
    "            try:\n",
    "                X.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                X.append(np.array((array)))\n",
    "\n",
    "        A = []\n",
    "        A_raw = BERT_avg[BERT_avg[\"category\"] == att1].vector.tolist()\n",
    "        for array in A_raw:\n",
    "            try:\n",
    "                A.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                A.append(np.array((array)))\n",
    "\n",
    "        B = []\n",
    "        B_raw = BERT_avg[BERT_avg[\"category\"] == att2].vector.tolist()\n",
    "        for array in B_raw:\n",
    "            try:\n",
    "                B.append(np.array(literal_eval(array)))\n",
    "            except Exception as e:\n",
    "                B.append(np.array((array)))\n",
    "\n",
    "        result_dict = s_weat(X_name=group, X=X, \n",
    "            A_name=att1, A=A, \n",
    "            B_name=att2, B=B, \n",
    "            permt=2, perm_n=1000)\n",
    "        print(result_dict)\n",
    "        results[(group, f\"{att1}_{att2}\")] = result_dict\n",
    "        \n",
    "# Convert results dictionary to DataFrame\n",
    "results_dict = { (group_attr): details for group_attr, details in results.items() }\n",
    "\n",
    "# Create DataFrame\n",
    "sweat_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Update column names to match the number of columns in the DataFrame\n",
    "sweat_df.columns = [\"category\", 'group_attributes', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat_before_BERT.csv', index=False)\n",
    "sweat_df.to_csv('sweat_average_BERT_fixed_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = ['pre', 'post']\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     if dates[i] not in results.keys():\n",
    "#         print(dates[i])\n",
    "#         print(dates[i+1])\n",
    "#         result_dict_prepost = {}\n",
    "#         for att1 in tests:\n",
    "#             X_name=\"Asians\"\n",
    "#             X = []\n",
    "#             A_name=att1\n",
    "#             A = []\n",
    "#             B_name=att1\n",
    "#             B = []\n",
    "#             for category in dict_prepost[dates[i]].keys():\n",
    "#                 if category in X_name:\n",
    "#                     for word in dict_prepost[dates[i]][category].keys():   \n",
    "#                         x = literal_eval(dict_prepost[dates[i]][category][word])\n",
    "#                         x = np.array(x)\n",
    "#                         X.append(x)\n",
    "#             for attribute in dict_prepost[dates[i]].keys():\n",
    "#                 if attribute in A_name:\n",
    "#                     for word in dict_prepost[dates[i]][attribute].keys():\n",
    "#                         a = literal_eval(dict_prepost[dates[i]][attribute][word])\n",
    "#                         a = np.array(a)\n",
    "#                         A.append(a)    \n",
    "#             for attribute in dict_prepost[dates[i+1]].keys():\n",
    "#                 if attribute in B_name:\n",
    "#                     for word in dict_prepost[dates[i+1]][attribute].keys():\n",
    "#                         b = literal_eval(dict_prepost[dates[i+1]][attribute][word])\n",
    "#                         b = np.array(b)\n",
    "#                         B.append(b)\n",
    "#             result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_post\", B=B, \n",
    "#                              permt=2, perm_n=1000)\n",
    "#             result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#             result_dict_prepost[att1] = result_dict\n",
    "            \n",
    "#         results[dates[i]] = result_dict_prepost\n",
    "\n",
    "# sweat_prepost_consecutive = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_prepost_consecutive = sweat_prepost_consecutive.reset_index()\n",
    "# sweat_prepost_consecutive.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_prepost_consecutive.to_csv('sweat_prepost_consecutive.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for date, data in dict_full.items():\n",
    "#     print(date)\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in data.keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in data[category].keys():   \n",
    "#                     x = literal_eval(data[category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     a = literal_eval(data[attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     b = literal_eval(data[attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000)\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "        \n",
    "#     results[date] = result_dict_full\n",
    "\n",
    "# sweat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_df.head()\n",
    "# sweat_df = sweat_df.reset_index()\n",
    "# sweat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     if dates[i] not in results.keys():\n",
    "#         print(dates[i])\n",
    "#         print(dates[i+1])\n",
    "#         result_dict_full = {}\n",
    "#         for att1 in tests:\n",
    "#             X_name=\"Asians\"\n",
    "#             X = []\n",
    "#             A_name=att1\n",
    "#             A = []\n",
    "#             B_name=att1\n",
    "#             B = []\n",
    "#             for category in dict_full[dates[i]].keys():\n",
    "#                 if category in X_name:\n",
    "#                     for word in dict_full[dates[i]][category].keys():   \n",
    "#                         x = literal_eval(dict_full[dates[i]][category][word])\n",
    "#                         x = np.array(x)\n",
    "#                         X.append(x)\n",
    "#             for attribute in dict_full[dates[i]].keys():\n",
    "#                 if attribute in A_name:\n",
    "#                     for word in dict_full[dates[i]][attribute].keys():\n",
    "#                         a = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                         a = np.array(a)\n",
    "#                         A.append(a)    \n",
    "#             for attribute in dict_full[dates[i+1]].keys():\n",
    "#                 if attribute in B_name:\n",
    "#                     for word in dict_full[dates[i+1]][attribute].keys():\n",
    "#                         b = literal_eval(dict_full[dates[i+1]][attribute][word])\n",
    "#                         b = np.array(b)\n",
    "#                         B.append(b)\n",
    "#             result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                              permt=2, perm_n=1000)\n",
    "#             result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#             result_dict_full[att1] = result_dict\n",
    "            \n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# sweat_consecutivedays = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_consecutivedays = sweat_consecutivedays.reset_index()\n",
    "# sweat_consecutivedays.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_consecutivedays.to_csv('sweat_consecutivedays.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on days of subsequent years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for date in dates:\n",
    "#     if date not in results.keys():\n",
    "#         result_dict_full = {}\n",
    "#         if date[:4] == '2019':\n",
    "#             t2 = re.sub('2019-', '2020-', date)\n",
    "#             if t2 in dict_full.keys():\n",
    "#                 if t2 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t2] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t2].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "#                         for attribute in dict_full[t2].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t2][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t2][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t2]\n",
    "#                         result_dict_full[t2][att1] = result_dict\n",
    "            \n",
    "#             t3 = re.sub('2019-', '2021-', date)\n",
    "#             if t3 in dict_full.keys():\n",
    "#                 if t3 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t3].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "#                         for attribute in dict_full[t3].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t3][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t3][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t3]\n",
    "#                         result_dict_full[t3][att1] = result_dict\n",
    "#         elif date[:4] == '2020':\n",
    "#             t2 = False\n",
    "#             t3 = re.sub('2020-', '2021-', date)\n",
    "#             if t3 in dict_full.keys():\n",
    "#                 if t3 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t3].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "\n",
    "#                         for attribute in dict_full[t3].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t3][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t3][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextyear\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t3]\n",
    "#                         result_dict_full[t3][att1] = result_dict\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#             results[date] = result_dict_full\n",
    "\n",
    "# sweat_consecutiveyears = pd.DataFrame.from_dict({(i,j,k): results[i][j][k]\n",
    "#                                                  for i in results.keys()\n",
    "#                                                  for j in results[i].keys()\n",
    "#                                                  for k in results[i][j].keys()},\n",
    "#                                                 orient='index')\n",
    "# sweat_consecutiveyears = sweat_consecutiveyears.reset_index()\n",
    "# sweat_consecutiveyears.columns = ['date', 'comparison_date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_consecutiveyears.to_csv('sweat_consecutiveyears.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asian corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for date, data in dict_asian.items():\n",
    "#     print(date)\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in data.keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in data[category].keys():   \n",
    "#                     x = literal_eval(data[category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     a = literal_eval(data[attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     b = literal_eval(data[attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000)\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "        \n",
    "#     results[date] = result_dict_full\n",
    "\n",
    "# with open(\"s_weat_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(results, outfile)\n",
    "\n",
    "# sweat_asian = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian = sweat_asian.reset_index()\n",
    "# sweat_asian.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_asian.to_csv('sweat_asian.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_asian.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     if dates[i] not in results.keys():\n",
    "#         print(dates[i])\n",
    "#         print(dates[i+1])\n",
    "#         result_dict_full = {}\n",
    "#         for att1 in tests:\n",
    "#             X_name=\"Asians\"\n",
    "#             X = []\n",
    "#             A_name=att1\n",
    "#             A = []\n",
    "#             B_name=att1\n",
    "#             B = []\n",
    "#             for category in dict_asian[dates[i]].keys():\n",
    "#                 if category in X_name:\n",
    "#                     for word in dict_asian[dates[i]][category].keys():   \n",
    "#                         x = literal_eval(dict_asian[dates[i]][category][word])\n",
    "#                         x = np.array(x)\n",
    "#                         X.append(x)\n",
    "#             for attribute in dict_asian[dates[i]].keys():\n",
    "#                 if attribute in A_name:\n",
    "#                     for word in dict_asian[dates[i]][attribute].keys():\n",
    "#                         a = literal_eval(dict_asian[dates[i]][attribute][word])\n",
    "#                         a = np.array(a)\n",
    "#                         A.append(a)    \n",
    "#             for attribute in dict_asian[dates[i+1]].keys():\n",
    "#                 if attribute in B_name:\n",
    "#                     for word in dict_asian[dates[i+1]][attribute].keys():\n",
    "#                         b = literal_eval(dict_asian[dates[i+1]][attribute][word])\n",
    "#                         b = np.array(b)\n",
    "#                         B.append(b)\n",
    "#             result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                              permt=2, perm_n=1000)\n",
    "#             result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#             result_dict_full[att1] = result_dict\n",
    "            \n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# sweat_asian_consecutivedays = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian_consecutivedays = sweat_asian_consecutivedays.reset_index()\n",
    "# sweat_asian_consecutivedays.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_asian_consecutivedays.to_csv('sweat_asian_consecutivedays.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on days of subsequent years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_asian.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for date in dates:\n",
    "#     if date not in results.keys():\n",
    "#         result_dict_full = {}\n",
    "#         if date[:4] == '2019':\n",
    "#             t2 = re.sub('2019-', '2020-', date)\n",
    "#             if t2 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "#                     for attribute in dict_asian[t2].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t2][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t2][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t2]\n",
    "#                     result_dict_full[t2] = {}\n",
    "#                     result_dict_full[t2][att1] = result_dict\n",
    "#             t3 = re.sub('2019-', '2021-', date)\n",
    "#             if t3 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "\n",
    "#                     for attribute in dict_asian[t3].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t3][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t3][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t3]\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                     result_dict_full[t3][att1] = result_dict\n",
    "#         elif date[:4] == '2020':\n",
    "#             t2 = False\n",
    "#             t3 = re.sub('2020-', '2021-', date)\n",
    "#             if t3 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "\n",
    "#                     for attribute in dict_asian[t3].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t3][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t3][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextyear\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t3]\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                     result_dict_full[t3][att1] = result_dict\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#             results[date] = result_dict_full\n",
    "\n",
    "# sweat_asian_consecutiveyears = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian_consecutiveyears = sweat_asian_consecutiveyears.reset_index()\n",
    "# sweat_asian_consecutiveyears.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_asian_consecutiveyears.to_csv('sweat_asian_consecutiveyears.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with NYT df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nyt = pd.read_csv('df_nyt.csv')\n",
    "# df_nyt = df_nyt.sort_values(by=['date'], ignore_index=True)\n",
    "# df_asian = pd.read_csv('s_weat_asian.csv')\n",
    "# df_asian = df_asian.sort_values(by=['date'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(attribute_weat_asian_df, df_nyt, on=\"date\")\n",
    "# df.to_csv('attribute_weat_asian_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
