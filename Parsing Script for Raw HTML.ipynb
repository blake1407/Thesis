{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys, os\n",
    "import emoji\n",
    "import time\n",
    "import os\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stole from Richa's code, remove emojis from text\n",
    "def deEmojify(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "def remove_extras(x: str):\n",
    "    '''\n",
    "    This function removes the extras character leftover from\n",
    "    the html parser.\n",
    "    \n",
    "    Args:\n",
    "        x (str): any string.\n",
    "    Returns:\n",
    "        str: corrected string.\n",
    "    '''\n",
    "    #remove html tidbits \n",
    "    #remove emojis\n",
    "    x = deEmojify(x)\n",
    "    x = emoji.replace_emoji(x, replace='')\n",
    "    x = x.replace(\"xad\", \"\")\n",
    "    x = x.replace(\"u200b\", \"\")\n",
    "    x = x.replace(\"'\", \"\")\n",
    "    x = x.replace(\"}}]\", \"\")\n",
    "    x = x.replace(\"}}\", \"\")\n",
    "    x = x.replace(\"\\\\t\", \" \")\n",
    "    x = x.replace(\" | \", \". \")\n",
    "    x = x.replace('\\'', \"'\")\n",
    "    x = x.replace(\" \\'\", \"'\")\n",
    "    x = x.replace(\":\\'\", \"\")\n",
    "    x = x.replace(\"> \", \"\")\n",
    "    x = x.replace(\"[\", \"\")\n",
    "    x = x.replace(\"]\", \"\")\n",
    "    x = x.replace(\"**\", \"\")\n",
    "    x = x.replace(\"*\", \"\")\n",
    "    x = x.replace(\"---\", \" \")\n",
    "    x = x.replace(\"\\\\n\", \". \")\n",
    "    x = x.replace('\\\\', \"\")\n",
    "    x = x.replace('~~', \"\")\n",
    "    x = x.replace(\"_\", \"\")\n",
    "    x = x.replace(\"....\", \" \")\n",
    "    x = x.replace(\"...\", \". \")\n",
    "    x = x.replace(\"..\", \". \")\n",
    "    x = x.replace(\". . \", \". \")\n",
    "    x = x.replace(\".  . \", \". \")\n",
    "    x = x.replace(\"^#1\", \",\").replace(\"^#2\", \",\").replace(\"^#3\", \",\").replace(\"^#4\", \",\").replace(\"^#5\", \",\")\n",
    "    x = x.replace('\\n', ' ')\n",
    "    #remove user handles\n",
    "    # x = re.sub('@[^\\s]+','',x)\n",
    "    #regex to remove URLs\n",
    "    x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
    "    #remove unk tokens\n",
    "    # x = re.sub('unk', '', x)\n",
    "    x = x.replace(\"()\", \"\")\n",
    "    #removing anything that's not alphabets\n",
    "    # x = re.sub('[^A-Z a-z]+', '', x)\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "#Check if the corpus actually contains these keywords\n",
    "def contains_keyword(text): \n",
    "    keywords = [\n",
    "        r'Gaza\\w*',\n",
    "        r'Israel\\w*',\n",
    "        r'West.*Bank|Bank.*West',\n",
    "        r'Tel.*Aviv',\n",
    "        r'Tel.*Aviv-Yafo',\n",
    "        r'Bedouin\\w*',\n",
    "        r'Palestin\\w*',\n",
    "        r'Middle East\\w*',\n",
    "        r'Middle Eastern',\n",
    "        r'Jew\\w*',\n",
    "        r'rabbi',\n",
    "        r'Muslim\\w*',\n",
    "        r'Islam\\w*',\n",
    "        r'Jihad',\n",
    "        r'Antisemiti\\w*',\n",
    "        r'Zionis\\w*',\n",
    "        r'IDF',\n",
    "        r'IOF',\n",
    "        r'Hamas',\n",
    "        r'Massacre',\n",
    "        r'Genocide',\n",
    "        r'Ceasefire',\n",
    "        r'Terroris\\w*',\n",
    "        r'Netanyahu',\n",
    "        r'Histadrut',\n",
    "        r'Haniyeh',\n",
    "        r'Yahya',\n",
    "        r'Sinwar',\n",
    "        r'Fatah',\n",
    "        r'Mohammed'\n",
    "        r'Deif'\n",
    "    ]\n",
    "    for keyword in keywords:\n",
    "        if re.search(keyword, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_usernames(text):\n",
    "    pattern = r'@\\w+'\n",
    "    usernames = re.findall(pattern, text)\n",
    "    return usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beinlibertarian', 'briebriejoy', 'NPR', 'foxnews', 'LPNational', 'HouseJudiciary', 'gmail', 'BadFaithPod', 'lpnational', 'washingtonpost', 'AP', 'nbcnews', 'TheAtlantic', 'theintercept', 'BreitbartNews']\n",
      "Number of followers to process: 15\n"
     ]
    }
   ],
   "source": [
    "#replace as needed\n",
    "folder_name = \"Supplementary Materials\"\n",
    "followers_path = os.path.join(folder_name, \"followers_US.txt\")\n",
    "completed_path = os.path.join(folder_name, \"completed_accounts\")\n",
    "\n",
    "followers_US = []\n",
    "# with open(r'''X\\followers_US.txt''', 'r') as f: # PC\n",
    "with open(followers_path,'r') as f: #List of accounts from raw followers list\n",
    "    for line in f:\n",
    "        if \"@\" in line:\n",
    "            follower = line.strip()[1:]\n",
    "            if follower not in followers_US:\n",
    "                # print(follower)\n",
    "                followers_US.append(follower)\n",
    "f.close()\n",
    "\n",
    "with open(completed_path, 'r') as f: #Get list of accounts that are completed\n",
    "    for line in f:\n",
    "        if line.startswith('total'):\n",
    "            completed_list = ast.literal_eval(line[8:])\n",
    "\n",
    "#Filter out the ones that are completed\n",
    "followers_US_set = set(followers_US)\n",
    "followers_US = [f for f in followers_US_set if f not in completed_list]\n",
    "\n",
    "print(followers_US)\n",
    "\n",
    "print(f\"Number of followers to process: {len(followers_US)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "#After conflict\n",
    "until = \"2024-09-22\"\n",
    "since = \"2023-10-07\"\n",
    "\n",
    "#Before conflict\n",
    "\n",
    "#Creating file paths\n",
    "\n",
    "raw_data_folder = \"Raw Data\"\n",
    "parsed_folder = \"Parsed Data\"\n",
    "raw_json = os.path.join(raw_data_folder, f\"2 - US_tweets_{until}_{since}.json\")\n",
    "cleaned_json = os.path.join(raw_data_folder, f\"2 - US_tweets_cleaned_{until}_{since}.json\")\n",
    "output_path = os.path.join(parsed_folder, f\"2 - Parsed_{until}_{since}.csv\")\n",
    "output_cleaned_path = os.path.join(parsed_folder, f\"2 - Parse_cleaned_{until}_{since}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'list' object is not callable\n",
      "<class 'TypeError'> 3807324497.py 13\n",
      "Number of discard for this run (from duplication and failure to contain keywords): 0\n"
     ]
    }
   ],
   "source": [
    "all_tweets = []\n",
    "account_processed = []\n",
    "account_pre_process = []\n",
    "account_for_retry = []\n",
    "account_irrelevant = []\n",
    "discard = 0\n",
    "\n",
    "try: \n",
    "    with open(raw_json, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Prepare CSV filename\n",
    "    usernames = list(data.keys())\n",
    "    \n",
    "    # print(len(usernames))\n",
    "    for u in usernames:\n",
    "        if u not in account_pre_process:\n",
    "            account_pre_process.append(u) #List of usernames before getting discarded\n",
    "\n",
    "    # Prepare CSV file and write headers\n",
    "    # csv_filename = r'''X\\US_parsed.csv''' #PC\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Poster', 'Date', 'Tweet', 'No of Likes', 'No of Retweets', 'No of Replies', 'No of Views'])\n",
    "        \n",
    "        for name in followers_US: \n",
    "            try:\n",
    "                if len(data[name]) == 0: #Account with no data\n",
    "                    account_for_retry.append(name)\n",
    "                    print(f\"Retry button appeared: {name}\")\n",
    "\n",
    "                for tweet_html in data[name]: \n",
    "                    print(f\"Processing user: {name}\")\n",
    "                    all_username = []\n",
    "                    all_time = []\n",
    "                    all_likes = []\n",
    "                    all_comments = []\n",
    "                    all_retweets = []\n",
    "                    all_views = []\n",
    "\n",
    "                    soup = BeautifulSoup(tweet_html, 'html.parser')\n",
    "\n",
    "                    #Check if there's an error with the html file\n",
    "                    span_text = \"Something went wrong. Try reloading.\"\n",
    "                    span = soup.find('span', string=span_text)\n",
    "                    if span:\n",
    "                        # Check if the span is within a div\n",
    "                        parent_div = span.find_parent('div')\n",
    "                        if parent_div:\n",
    "                            account_for_retry.append(name)\n",
    "                            print(f\"Span text '{span_text}' found. Skipping to the next account!\")\n",
    "                            continue\n",
    "                            # print(f\"Parent div classes: {parent_div.get('class', [])}\")\n",
    "                        else:\n",
    "                            print(f\"Span with text '{span_text}' found, but not within a div.\")\n",
    "\n",
    "                    # # Find all usernames\n",
    "                    # usernames = soup.find_all('a', attrs = {'aria-hidden': 'true'})\n",
    "                    # for username in usernames:\n",
    "                    #     href_value = username.get('href', None)\n",
    "                    #     if href_value and href_value not in all_username:\n",
    "                    #         all_username.append(href_value[1:])\n",
    "                    #     else:\n",
    "                    #         print('No name found')\n",
    "\n",
    "                    # Extract all tweet texts\n",
    "                    tweets = soup.find_all(attrs={\"data-testid\": \"tweetText\"})\n",
    "                    tweet_texts = [remove_extras(tweet.get_text(strip=False)) for tweet in tweets]\n",
    "\n",
    "                    \n",
    "                    # Select the time value\n",
    "                    time_element = soup.find_all('time')\n",
    "                    for t in time_element:\n",
    "                        datetime_value = t['datetime']\n",
    "                        dt = datetime.fromisoformat(datetime_value.replace(\"Z\", \"+00:00\")) \n",
    "                        readable_time = dt.strftime(\"%B %d, %Y %I:%M %p\")\n",
    "                        all_time.append(readable_time)\n",
    "\n",
    "                    # Find all <div> elements with aria-label for likes, comments, and retweets\n",
    "                    div_elements = soup.find_all('div', attrs={\"aria-label\": True})\n",
    "\n",
    "                    for div_element in div_elements:\n",
    "                        aria_label_value = div_element['aria-label']\n",
    "                        try:\n",
    "                            # if 'likes' in aria_label_value or ('bookmarks' in aria_label_value and 'views' in aria_label_value): \n",
    "                            #changed from or to and\n",
    "                            if 'likes' in aria_label_value and 'replies' in aria_label_value and 'reposts' in aria_label_value and 'views' in aria_label_value: \n",
    "                                values = aria_label_value.split(', ')\n",
    "\n",
    "                                try:\n",
    "                                    replies = values[0].split(' ')[0]\n",
    "                                    all_comments.append(replies)\n",
    "                                except IndexError:\n",
    "                                    replies = 'N/A'\n",
    "                                try:\n",
    "                                    retweets = values[1].split(' ')[0]\n",
    "                                    all_retweets.append(retweets)\n",
    "                                except IndexError:\n",
    "                                    retweets = 'N/A'\n",
    "                                try:\n",
    "                                    likes = values[2].split(' ')[0]\n",
    "                                    all_likes.append(likes)\n",
    "                                except IndexError:\n",
    "                                    likes = 'N/A'\n",
    "\n",
    "                                for v in values:\n",
    "                                    if 'views' in v:\n",
    "                                        views = v.split(' ')[0]\n",
    "                                        all_views.append(views)\n",
    "                                        # print(views)\n",
    "\n",
    "                        except: \n",
    "                            print(\"Failed to find aria labels!\")\n",
    "\n",
    "                    # Write to CSV\n",
    "                    for user, time, tweet, likes, retweets, comments, views in zip(usernames, all_time, tweet_texts, all_likes, all_retweets, all_comments, all_views):\n",
    "                        line = [user, time, tweet, likes, retweets, comments, views]\n",
    "            \n",
    "                        if tweet not in all_tweets and contains_keyword(tweet):\n",
    "                        # if tweet not in all_tweets:\n",
    "                            #Check whether there is a duplicate and that the tweet actually contains intended keywords\n",
    "                            writer.writerow(line)\n",
    "                            all_tweets.append(tweet)\n",
    "                            if user not in account_processed:\n",
    "                                account_processed.append(user)\n",
    "                        else:\n",
    "                            discard+=1 \n",
    "                            # print(\"Duplicated tweet found.\")\n",
    "                    print(f\"Sucessfully processed user {name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing user: {str(e)}\")\n",
    "                exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                print(exc_type, fname, exc_tb.tb_lineno)\n",
    "                continue\n",
    "            finally:\n",
    "                # print(f'Current count: {count}')\n",
    "                count+=1\n",
    "        \n",
    "        #Check for accounts with parsed tweets but discarded because of irrelevance\n",
    "        for a in account_pre_process:\n",
    "            if a not in account_processed and a not in account_for_retry:\n",
    "                account_irrelevant.append(a)\n",
    "        #Check that accounts needed to be re-run is not processed list\n",
    "        account_for_retry_set = set(account_for_retry)\n",
    "        account_processed = [a for a in account_processed if a not in account_for_retry_set]\n",
    "\n",
    "        #Writing run statistics at the end of the csv file\n",
    "        now = datetime.now() #Current time that the data is being processed\n",
    "        readable_now = now.strftime(\"%B %d, %Y %I:%M %p\")\n",
    "\n",
    "        writer.writerow([])\n",
    "        writer.writerow([\"|RUN STATISTICS|\"])\n",
    "        writer.writerow([f\"Time Processed: {readable_now}\", f\"Number of discards: {discard}\", f\"List of accounts processed: {account_processed}\", f\"List of accounts coming in: {account_pre_process}\", f\"List of accounts needed to be rerun: {account_for_retry}\", f\"List of accounts with parsed but only irrelevant tweets: {account_irrelevant}\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "    print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "print(f\"Number of discard for this run (from duplication and failure to contain keywords): {discard}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
