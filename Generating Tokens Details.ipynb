{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_33480/319895550.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2024-11-18 22:17:21.526180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \\nDictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\\n\\nOnce we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: string):\n",
    "    return ' '.join([item for item in text.split() if item not in stopwords.words('english')])\n",
    "\n",
    "def remove_punctuations(text: string):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "def delete_usernames(text: str) -> str:\n",
    "    pattern = r'@\\w+'\n",
    "    return re.sub(pattern, '@user', text)\n",
    "\n",
    "def remove_non_words(text: string):\n",
    "    return re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                          \" \",          # Replace all non-letters with spaces\n",
    "                          str(text))\n",
    "\n",
    "\n",
    "# Function to split \"After Corpus\" into individual sentences and create a new DataFrame\n",
    "def split_sentences(df, status: str):\n",
    "    abbreviations = {\n",
    "        \"U.S.\": \"USA\",\n",
    "        \"U.K.\": \"UK\",\n",
    "        \"e.g.\": \"for example,\",\n",
    "        \"i.e.\": \"such as,\",\n",
    "        \"U.N.\": \"UN\",\n",
    "        \"Gov.\": \"Governor\",\n",
    "        \"etc.\": \"...\"\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the rows of the new DataFrame\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the relevant data\n",
    "        subject_id = row['Subject_ID']\n",
    "        tweet_id = row['Tweet_ID']\n",
    "        corpus = row[f'{status}_Corpus']\n",
    "\n",
    "        # Replace abbreviations with placeholders\n",
    "        for abbr, placeholder in abbreviations.items():\n",
    "            corpus = corpus.replace(abbr, placeholder)\n",
    "        \n",
    "        # Split the corpus into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus)\n",
    "\n",
    "        # Add each sentence as a new row in the list, keeping track of the tweet ID and subject ID\n",
    "        for sentence in sentences:\n",
    "            rows.append({'Subject_ID': subject_id, 'Tweet_ID': tweet_id, 'Sentence': sentence})\n",
    "\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    return new_df\n",
    "\n",
    "# def stem_text(text: string):\n",
    "#     return [ps.stem(word) for word in text]\n",
    "\n",
    "def get_segments_ids(df):\n",
    "    segments_ids = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Create a list of repeated IDs for each sentence\n",
    "        segment_id_list = [idx] * len(row['Indexed_Tokens'])\n",
    "        segments_ids.append(segment_id_list)\n",
    "    df[\"Segments_IDs\"] = segments_ids\n",
    "    return df[\"Segments_IDs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process Text Data\n",
    "clean_data_path = \"Cleaned Data\"\n",
    "after = pd.read_csv(f\"{clean_data_path}/After_NN_Cleaned.csv\")\n",
    "before = pd.read_csv(f\"{clean_data_path}/Before_NN_Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Marked_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>hey remember phoned despot bestie Netanyahu to...</td>\n",
       "      <td>[CLS] hey remember phoned despot bestie Netany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>ace job homeslice</td>\n",
       "      <td>[CLS] ace job homeslice [SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>good thing meddling caused serious consequence...</td>\n",
       "      <td>[CLS] good thing meddling caused serious conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>705</td>\n",
       "      <td>hey remember Jared Kushner brought peace Middl...</td>\n",
       "      <td>[CLS] hey remember Jared Kushner brought peace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>705</td>\n",
       "      <td>neither</td>\n",
       "      <td>[CLS] neither [SEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_ID  Tweet_ID                                           Sentence  \\\n",
       "0          74       609  hey remember phoned despot bestie Netanyahu to...   \n",
       "1          74       609                                  ace job homeslice   \n",
       "2          74       609  good thing meddling caused serious consequence...   \n",
       "3          74       705  hey remember Jared Kushner brought peace Middl...   \n",
       "4          74       705                                            neither   \n",
       "\n",
       "                                     Marked_Sentence  \n",
       "0  [CLS] hey remember phoned despot bestie Netany...  \n",
       "1                      [CLS] ace job homeslice [SEP]  \n",
       "2  [CLS] good thing meddling caused serious conse...  \n",
       "3  [CLS] hey remember Jared Kushner brought peace...  \n",
       "4                                [CLS] neither [SEP]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to split sentences\n",
    "after_sentences = split_sentences(after, \"After\")\n",
    "before_sentences = split_sentences(before, \"Before\")\n",
    "\n",
    "# Apply all cleaning functions to the After Corpus column\n",
    "after_sentences[\"Sentence\"] = (after_sentences[\"Sentence\"]\n",
    "                             .apply(delete_usernames)\n",
    "                             .apply(remove_punctuations)\n",
    "                             .apply(remove_stop_words)\n",
    "                             .apply(remove_non_words))\n",
    "\n",
    "before_sentences[\"Sentence\"] = (before_sentences[\"Sentence\"]\n",
    "                             .apply(delete_usernames)\n",
    "                             .apply(remove_punctuations)\n",
    "                             .apply(remove_stop_words)\n",
    "                             .apply(remove_non_words))\n",
    "\n",
    "# Apply sentence markers\n",
    "after_sentences[\"Marked_Sentence\"] = (\"[CLS] \"+ after_sentences[\"Sentence\"] + \" [SEP]\")\n",
    "before_sentences[\"Marked_Sentence\"] = (\"[CLS] \"+ before_sentences[\"Sentence\"] + \" [SEP]\")\n",
    "\n",
    "# Show the first few rows of the resulting DataFrame\n",
    "after_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Marked_Sentence</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Indexed_Tokens</th>\n",
       "      <th>Segments_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>353</td>\n",
       "      <td>holy fucking shit criticizing Israel antisemitic</td>\n",
       "      <td>[CLS] holy fucking shit criticizing Israel ant...</td>\n",
       "      <td>[[CLS], holy, fucking, shit, criticizing, isra...</td>\n",
       "      <td>[101, 4151, 8239, 4485, 21289, 3956, 3424, 336...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>353</td>\n",
       "      <td>IS antisemitic putting holocaustdenying lunati...</td>\n",
       "      <td>[CLS] IS antisemitic putting holocaustdenying ...</td>\n",
       "      <td>[[CLS], is, anti, ##se, ##mit, ##ic, putting, ...</td>\n",
       "      <td>[101, 2003, 3424, 3366, 22930, 2594, 5128, 115...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>353</td>\n",
       "      <td>sit fuck shut fuck spare us fucking fake outrage</td>\n",
       "      <td>[CLS] sit fuck shut fuck spare us fucking fake...</td>\n",
       "      <td>[[CLS], sit, fuck, shut, fuck, spare, us, fuck...</td>\n",
       "      <td>[101, 4133, 6616, 3844, 6616, 8622, 2149, 8239...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>538</td>\n",
       "      <td>congratulations literally zero Republicans sto...</td>\n",
       "      <td>[CLS] congratulations literally zero Republica...</td>\n",
       "      <td>[[CLS], congratulations, literally, zero, repu...</td>\n",
       "      <td>[101, 23156, 6719, 5717, 10643, 2768, 7939, 23...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>538</td>\n",
       "      <td>take victory lap spineless fucking cowards</td>\n",
       "      <td>[CLS] take victory lap spineless fucking cowar...</td>\n",
       "      <td>[[CLS], take, victory, lap, spine, ##less, fuc...</td>\n",
       "      <td>[101, 2202, 3377, 5001, 8560, 3238, 8239, 1659...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_ID  Tweet_ID                                           Sentence  \\\n",
       "0          74       353   holy fucking shit criticizing Israel antisemitic   \n",
       "1          74       353  IS antisemitic putting holocaustdenying lunati...   \n",
       "2          74       353   sit fuck shut fuck spare us fucking fake outrage   \n",
       "3          74       538  congratulations literally zero Republicans sto...   \n",
       "4          74       538         take victory lap spineless fucking cowards   \n",
       "\n",
       "                                     Marked_Sentence  \\\n",
       "0  [CLS] holy fucking shit criticizing Israel ant...   \n",
       "1  [CLS] IS antisemitic putting holocaustdenying ...   \n",
       "2  [CLS] sit fuck shut fuck spare us fucking fake...   \n",
       "3  [CLS] congratulations literally zero Republica...   \n",
       "4  [CLS] take victory lap spineless fucking cowar...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [[CLS], holy, fucking, shit, criticizing, isra...   \n",
       "1  [[CLS], is, anti, ##se, ##mit, ##ic, putting, ...   \n",
       "2  [[CLS], sit, fuck, shut, fuck, spare, us, fuck...   \n",
       "3  [[CLS], congratulations, literally, zero, repu...   \n",
       "4  [[CLS], take, victory, lap, spine, ##less, fuc...   \n",
       "\n",
       "                                      Indexed_Tokens  \\\n",
       "0  [101, 4151, 8239, 4485, 21289, 3956, 3424, 336...   \n",
       "1  [101, 2003, 3424, 3366, 22930, 2594, 5128, 115...   \n",
       "2  [101, 4133, 6616, 3844, 6616, 8622, 2149, 8239...   \n",
       "3  [101, 23156, 6719, 5717, 10643, 2768, 7939, 23...   \n",
       "4  [101, 2202, 3377, 5001, 8560, 3238, 8239, 1659...   \n",
       "\n",
       "                                        Segments_IDs  \n",
       "0                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2                  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  \n",
       "3   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  \n",
       "4                     [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "after_sentences[\"Tokens\"] = after_sentences[\"Marked_Sentence\"].apply(tokenizer.tokenize)\n",
    "before_sentences[\"Tokens\"] = before_sentences[\"Marked_Sentence\"].apply(tokenizer.tokenize)\n",
    "\n",
    "#Getting Indexed Tokens using BERT tokenzier\n",
    "after_sentences[\"Indexed_Tokens\"] = after_sentences[\"Tokens\"].apply(lambda x: [tokenizer.convert_tokens_to_ids(token) for token in x])\n",
    "before_sentences[\"Indexed_Tokens\"] = before_sentences[\"Tokens\"].apply(lambda x: [tokenizer.convert_tokens_to_ids(token) for token in x])\n",
    "\n",
    "#Getting Segment IDs based on the number of sentences, all tokens in the same sentence get the same ID.\n",
    "after_sentences[\"Segments_IDs\"] = get_segments_ids(after_sentences)\n",
    "before_sentences[\"Segments_IDs\"] = get_segments_ids(before_sentences)\n",
    "\n",
    "before_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_sentences.to_csv(\"Tokenized Data/After_Tokenized.csv\", index=False)\n",
    "before_sentences.to_csv(\"Tokenized Data/Before_Tokenized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
