{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "from itertools import combinations\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import spatial\n",
    "from math import sqrt\n",
    "\n",
    "\"\"\"\n",
    "Test: Take out 20 randomly (same length in each bucket), \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_std(values, weights):\n",
    "    # values = numpy ndarray with the same shape as weights\n",
    "    # weights = numpy ndarray with the same shape as values\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    # Small sample size bias correction:\n",
    "    variance_ddof1 = variance*len(values)/(len(values)-1)\n",
    "    return sqrt(variance_ddof1)\n",
    "\n",
    "def within_group_cohesion(X):\n",
    "    # X = 2D numpy array of vectors for all words belonging to group X\n",
    "    dist = spatial.distance.pdist(X, 'cosine')\n",
    "    return dist.mean()\n",
    "\n",
    "def sim(x, A, B):\n",
    "    # x = ndarray for each word x in category X\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    x_ = x.reshape(1, -1)\n",
    "    results_A = spatial.distance.cdist(x_, A, 'cosine')\n",
    "    sum_A = (1 - results_A).sum()\n",
    "    results_B = spatial.distance.cdist(x_, B, 'cosine')\n",
    "    sum_B = (1 - results_B).sum()\n",
    "    difference = sum_A/len(A) - sum_B/len(B)\n",
    "    return difference\n",
    "\n",
    "def group_cohesion_test(X, Y, perm_n = 1000, permtype = 1):\n",
    "    # X = 2D numpy array of vectors for all words belonging to group X\n",
    "    # Y = 2D numpy array of vectors for all words belonging to group Y\n",
    "    # perm_n = number of permutations\n",
    "    # permtype = permutation type.\n",
    "    test_statistic = np.average((within_group_cohesion(X), within_group_cohesion(Y)), weights = (len(X), len(Y)))\n",
    "    jointlist = np.concatenate((X,Y))\n",
    "    permutations = np.array([])\n",
    "    if permtype == 1:\n",
    "        count = 0\n",
    "        cutpoint = len(X)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations = np.append(permutations, \n",
    "                                     np.average([within_group_cohesion(set1), within_group_cohesion(set2)], \n",
    "                                                weights = [len(set1), len(set2)]))\n",
    "            count += 1\n",
    "    else:\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(X)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations = np.append(permutations, \n",
    "                                     np.average([within_group_cohesion(set1), within_group_cohesion(set2)], \n",
    "                                                weights = [len(set1), len(set2)]))\n",
    "    P_val = (sum(i <= test_statistic for i in permutations)+1)/(len(permutations)+1)\n",
    "    return P_val\n",
    "\n",
    "def diff_sim(X, A, B, effect=1, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    # effect = boolean for whether standard deviation & effect size need to be calculated\n",
    "    # Y = optional. ndarray for words in category Y\n",
    "    if Y:\n",
    "        sum_X = 0\n",
    "        sum_Y = 0\n",
    "        for x in X:\n",
    "            x = np.array(x)\n",
    "            sum_X += sim(x, A, B)\n",
    "        for y in Y:\n",
    "            y = np.array(y)\n",
    "            sum_Y += sim(y, A, B)\n",
    "        difference = sum_X/len(X) - sum_Y/len(Y)\n",
    "        all_sims = []\n",
    "        for w in (np.concatenate((X,Y))):\n",
    "            all_sims.append(sim(w, A, B))\n",
    "        # For SD calculation, assign weights based on frequency of opposite category\n",
    "        weights = [len(Y) for num in range(len(X))] + [len(X) for num in range(len(Y))]\n",
    "        standard_dev = weighted_std(all_sims, weights)\n",
    "        if standard_dev == 0:\n",
    "            effect_size = 0\n",
    "        else:\n",
    "            effect_size = difference/standard_dev\n",
    "    else:\n",
    "        sum_A = 0\n",
    "        sum_B = 0\n",
    "        all_sims = []\n",
    "        for a in A:\n",
    "            a_ = a.reshape(1, -1)\n",
    "            results = spatial.distance.cdist(a_, X, 'cosine')\n",
    "            sum_X = (1 - results).sum()\n",
    "            val = sum_X/len(X)\n",
    "            sum_A += val\n",
    "            all_sims.append(val)\n",
    "        ave_A = sum_A/len(A)\n",
    "        for b in B:\n",
    "            b_ = b.reshape(1, -1)\n",
    "            results = spatial.distance.cdist(b_, X, 'cosine')\n",
    "            sum_X = (1 - results).sum()\n",
    "            val = sum_X/len(X)\n",
    "            sum_B += val\n",
    "            all_sims.append(val)\n",
    "        ave_B = sum_B/len(B)\n",
    "        difference = ave_A - ave_B\n",
    "        standard_dev = np.std(all_sims, ddof=1)\n",
    "        if standard_dev == 0:\n",
    "            effect_size = 0\n",
    "        else:\n",
    "            effect_size = difference/standard_dev\n",
    "    if effect == 1:\n",
    "        return difference, standard_dev, effect_size\n",
    "    else:\n",
    "        return difference    \n",
    "\n",
    "def permutation_test(X, A, B, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # Y = ndarray for words in category Y\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    if Y:\n",
    "        jointlist = np.array(list(X) + list(Y))\n",
    "        permutations = []\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(X)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations.append(diff_sim(set1, set2, A, B))\n",
    "    else:\n",
    "        jointlist = np.array(list(A) + list(B))\n",
    "        permutations = []\n",
    "        nums = list(range(len(jointlist)))\n",
    "        for comb in combinations(nums, len(A)):\n",
    "            set1 = [item for i, item in enumerate(jointlist) if i in comb]\n",
    "            set2 = [item for i, item in enumerate(jointlist) if i not in comb]\n",
    "            permutations.append(diff_sim(X, set1, set2, effect=0))\n",
    "    return permutations\n",
    "\n",
    "def rand_test(X, A, B, perm_n, Y=False):\n",
    "    # X = ndarray for words in category X\n",
    "    # Y = ndarray for words in category Y\n",
    "    # A = ndarray for words in attribute A\n",
    "    # B = ndarray for words in attribute B\n",
    "    # perm_n = number of permutations\n",
    "    if Y:\n",
    "        jointlist = np.array(list(X) + list(Y))\n",
    "        np.random.shuffle(jointlist)\n",
    "        permutations = []\n",
    "        count = 0\n",
    "        cutpoint = len(X)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations.append(diff_sim(set1, set2, A, B))\n",
    "            count += 1\n",
    "    else:\n",
    "        jointlist = np.array(list(A) + list(B))\n",
    "        np.random.shuffle(jointlist)\n",
    "        permutations = []\n",
    "        count = 0\n",
    "        cutpoint = len(A)\n",
    "        while count < perm_n:\n",
    "            np.random.shuffle(jointlist)\n",
    "            set1 = jointlist[:cutpoint]\n",
    "            set2 = jointlist[cutpoint:]\n",
    "            permutations.append(diff_sim(X, set1, set2, effect=0))\n",
    "            count += 1\n",
    "    return permutations\n",
    "\n",
    "### DOUBLE CATEGORY WEAT\n",
    "\n",
    "def weat(X_name, X, Y_name, Y, A_name, A, B_name, B, \n",
    "         permt=0, perm_n=10000, cohesion_test=False, cohesion_permutations=1000, cohesion_type=2):\n",
    "    # X_name = name of category 1. Will be used in result output.\n",
    "    # X = category 1. Input should be iterable and contain numpy array(s) for words in category 1\n",
    "    # Y_name = name of category 2. Will be used in result output.\n",
    "    # Y = category 2. Input should be iterable and contain numpy array(s) for words in category 2\n",
    "    # A_name = name of attribute 1. Will be used in result output.\n",
    "    # A = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 1\n",
    "    # B_name = name of attribute 2. Will be used in result output.\n",
    "    # B = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 2\n",
    "    # permt = do you want to perform a permutation test? 0 = no, 1 = yes, 2 = yes, with the perm_n specified\n",
    "    # perm_n = number of permutations\n",
    "    # cohesion_test = boolean for testing within-category cohesion\n",
    "    # cohesion_permutations = number of permutations for cohesion test\n",
    "    # cohesion_type = type of cohesion test. 1 = test cohesion of only one group, 2 = test cohesion of both groups\n",
    "    \n",
    "    # Calculate effect size\n",
    "    difference, standard_dev, effect_size = diff_sim(X=X, Y=Y, A=A, B=B, effect=1)\n",
    "    \n",
    "    result_dict = OrderedDict({\"categories\": [X_name, Y_name],\n",
    "                               \"attributes\": [A_name, B_name],\n",
    "                               \"difference\": difference,\n",
    "                               \"standard_dev\": standard_dev,\n",
    "                               \"effect_size\": effect_size})\n",
    "    \n",
    "    # Permutations if permt is not 0\n",
    "    if permt == 1 or permt == 2:\n",
    "        if permt == 1:\n",
    "            permutations = np.array(permutation_test(X=X, Y=Y, A=A, B=B))\n",
    "        elif permt == 2:\n",
    "            permutations = np.array(rand_test(X=X, Y=Y, A=A, B=B, perm_n=perm_n))\n",
    "        perm_mean = np.mean(permutations)\n",
    "        permutations = permutations - perm_mean\n",
    "        sum_c = effect_size - perm_mean\n",
    "        Pleft = (sum(i <= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Pright = (sum(i >= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Ptot = (sum(abs(i) >= abs(sum_c) for i in permutations)+1)/(len(permutations)+1)\n",
    "        se = np.std(permutations)\n",
    "        result_dict[\"Pleft\"] = Pleft\n",
    "        result_dict[\"Pright\"] = Pright\n",
    "        result_dict[\"Ptot\"] = Ptot\n",
    "        result_dict[\"se\"] = se\n",
    "    \n",
    "    # Cohesion test if cohesion_test is true\n",
    "    if cohesion_test == True:\n",
    "        cohesion_categories = group_cohesion_test(X=X, Y=Y, perm_n=cohesion_permutations, permtype=cohesion_type)\n",
    "        cohesion_attributes = group_cohesion_test(X=A, Y=B, perm_n=cohesion_permutations, permtype=cohesion_type)\n",
    "        result_dict[\"cohesion_categories\"] = cohesion_categories\n",
    "        result_dict[\"cohesion_attributes\"] = cohesion_attributes\n",
    "        \n",
    "    return result_dict    \n",
    "\n",
    "### SINGLE CATEGORY WEAT\n",
    "\n",
    "def s_weat(X_name, X, A_name, A, B_name, B, permt = 0, perm_n = 10000):\n",
    "    # X_name = name of category 1. Will be used in result output.\n",
    "    # X = category 1. Input should be iterable and contain numpy array(s) for words in category 1\n",
    "    # A_name = name of attribute 1. Will be used in result output.\n",
    "    # A = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 1\n",
    "    # B_name = name of attribute 2. Will be used in result output.\n",
    "    # B = attribute 1. Input should be iterable and contain numpy array(s) for words in attribute 2\n",
    "    # permt = do you want to perform a permutation test? 0 = no, 1 = yes, 2 = yes, with the perm_n specified\n",
    "    # perm_n = number of permutations\n",
    "\n",
    "    difference, standard_dev, effect_size = diff_sim(X=X, A=A, B=B)\n",
    "    \n",
    "    result_dict = OrderedDict({\"category\": [X_name],\n",
    "                               \"attributes\": [A_name, B_name],\n",
    "                               \"difference\": difference,\n",
    "                               \"standard_dev\": standard_dev,\n",
    "                               \"effect_size\": effect_size})\n",
    "    if permt == 1 or permt == 2:\n",
    "        if permt == 1:\n",
    "            permutations = np.array(permutation_test(X, A, B))\n",
    "        elif permt == 2:\n",
    "            permutations = np.array(rand_test(X, A, B, perm_n = perm_n))\n",
    "        perm_mean = np.mean(permutations)\n",
    "        permutations = permutations - perm_mean\n",
    "        sum_c = difference - perm_mean\n",
    "        Pleft = (sum(i <= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Pright = (sum(i >= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "        Ptot = (sum(abs(i) >= abs(sum_c) for i in permutations)+1)/(len(permutations)+1)\n",
    "        result_dict[\"Pleft\"] = Pleft\n",
    "        result_dict[\"Pright\"] = Pright\n",
    "        result_dict[\"Ptot\"] = Ptot\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_asian = pd.read_csv('embeddings_asian.csv')\n",
    "# dict_asian = df_asian.groupby('day').apply(lambda a: dict(a.groupby('category').apply(lambda x: dict(zip(x['word'], x['vectors'])))))\n",
    "# dict_asian = dict_asian.to_dict()\n",
    "\n",
    "# df_full = pd.read_csv('embeddings_full.csv')\n",
    "# df_full = df_full.sort_values(by=\"day\")\n",
    "# dict_full = df_full.groupby('day').apply(lambda a: dict(a.groupby('category').apply(lambda x: dict(zip(x['word'], x['vectors'])))))\n",
    "# dict_full = dict_full.to_dict()\n",
    "\n",
    "df_prepost = pd.read_csv('embeddings_prepost.csv')\n",
    "df_prepost = df_prepost.sort_values(by=\"day\")\n",
    "dict_prepost = df_prepost.groupby('day').apply(lambda a: dict(a.groupby('category').apply(lambda x: dict(zip(x['word'], x['vectors'])))))\n",
    "dict_prepost = dict_prepost.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charlesworth_df = pd.read_csv(\"../../Charlesworth/Study 3/Kurdi, Mann, Charlesworth, & Banaji (2018) Vectors.csv\")\n",
    "# charlesworth_df.head()\n",
    "charlesworth_df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['Asians', 'Whites']\n",
    "X = []\n",
    "X_raw = charlesworth_df[charlesworth_df.category == \"Asians\"].vector.tolist()\n",
    "for array in X_raw: \n",
    "    X.append(np.array(literal_eval(array)))\n",
    "Y = []\n",
    "Y_raw = charlesworth_df[charlesworth_df.category == \"Whites\"].vector.tolist()\n",
    "for array in Y_raw:\n",
    "    Y.append(np.array(literal_eval(array)))\n",
    "A = []\n",
    "A_raw = charlesworth_df[charlesworth_df.category == \"Warm\"].vector.tolist()\n",
    "for array in A_raw:\n",
    "    A.append(np.array(literal_eval(array)))\n",
    "B = []\n",
    "B_raw = charlesworth_df[charlesworth_df.category == \"Cold\"].vector.tolist()\n",
    "for array in B_raw:\n",
    "    B.append(np.array(literal_eval(array)))\n",
    "    \n",
    "weat(X_name=\"Asians\", X=X, \n",
    "     Y_name=\"Whites\", Y=Y, \n",
    "     A_name=\"Warm\", A=A, \n",
    "     B_name=\"Cold\", B=B, \n",
    "     permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['Asians', 'Whites']\n",
    "X = []\n",
    "X_raw = charlesworth_df[charlesworth_df.category == \"Asians\"].vector.tolist()\n",
    "for array in X_raw: \n",
    "    X.append(np.array(literal_eval(array)))\n",
    "Y = []\n",
    "Y_raw = charlesworth_df[charlesworth_df.category == \"Whites\"].vector.tolist()\n",
    "for array in Y_raw:\n",
    "    Y.append(np.array(literal_eval(array)))\n",
    "A = []\n",
    "A_raw = charlesworth_df[charlesworth_df.category == \"Competence\"].vector.tolist()\n",
    "for array in A_raw:\n",
    "    A.append(np.array(literal_eval(array)))\n",
    "B = []\n",
    "B_raw = charlesworth_df[charlesworth_df.category == \"Incompetence\"].vector.tolist()\n",
    "for array in B_raw:\n",
    "    B.append(np.array(literal_eval(array)))\n",
    "    \n",
    "weat(X_name=\"Asians\", X=X, \n",
    "     Y_name=\"Whites\", Y=Y, \n",
    "     A_name=\"Competence\", A=A, \n",
    "     B_name=\"Incompetence\", B=B, \n",
    "     permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "dates = []\n",
    "for date in dict_full.keys():\n",
    "    if date not in dates:\n",
    "        dates.append(date)\n",
    "dates.sort()\n",
    "tests = [(\"Warm\", \"Cold\"),(\"Competent\", \"Incompetent\")]\n",
    "\n",
    "for i in np.arange(len(dates)-1):\n",
    "    result_dict_full = {}\n",
    "    for att1, att2 in tests:\n",
    "        X_name=\"Asians\"\n",
    "        X = []\n",
    "        Y_name=\"Whites\"\n",
    "        Y = []\n",
    "        A_name=att1\n",
    "        A = []\n",
    "        B_name=att2\n",
    "        B = []\n",
    "        for category in dict_full[dates[i]].keys():\n",
    "            if category in X_name:\n",
    "                for word in dict_full[dates[i]][category].keys():   \n",
    "                    x = literal_eval(dict_full[dates[i]][category][word])\n",
    "                    x = np.array(x)\n",
    "                    X.append(x)\n",
    "        for category in dict_full[dates[i]].keys():\n",
    "            if category in Y_name:\n",
    "                for word in dict_full[dates[i]][category].keys():\n",
    "                    y = literal_eval(dict_full[dates[i]][category][word])\n",
    "                    y = np.array(y)\n",
    "                    Y.append(y)\n",
    "        for attribute in dict_full[dates[i]].keys():\n",
    "            if attribute in A_name:\n",
    "                for word in dict_full[dates[i]][attribute].keys():\n",
    "                    a = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "                    a = np.array(a)\n",
    "                    A.append(a)    \n",
    "        for attribute in dict_full[dates[i]].keys():\n",
    "            if attribute in B_name:\n",
    "                for word in dict_full[dates[i]][attribute].keys():\n",
    "                    b = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "                    b = np.array(b)\n",
    "                    B.append(b)\n",
    "        result_dict = weat(X_name=\"Asians\", X=X, Y_name=\"Whites\", Y=Y, A_name=att1, A=A, B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "        result_dict['date'] = dates[i]\n",
    "        result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "    \n",
    "    if dates[i] not in results.keys():\n",
    "        results[dates[i]] = result_dict_full\n",
    "\n",
    "weat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "                                  for i in results.keys()\n",
    "                                  for j in results[i].keys()},\n",
    "                                 orient='index')\n",
    "weat_df = weat_df.reset_index()\n",
    "weat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'se', 'date']\n",
    "weat_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double category WEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Post corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "dates = []\n",
    "for date in dict_prepost.keys():\n",
    "    if date not in dates:\n",
    "        dates.append(date)\n",
    "dates.sort()\n",
    "tests = [(\"Warm\", \"Cold\"),(\"Competent\", \"Incompetent\")]\n",
    "\n",
    "for i in np.arange(len(dates)-1):\n",
    "    print(dates[i])\n",
    "    print(dates[i+1])\n",
    "    result_dict_prepost = {}\n",
    "    for att1, att2 in tests:\n",
    "        X_name=\"Asians\"\n",
    "        X = []\n",
    "        Y_name=\"Asians\"\n",
    "        Y = []\n",
    "        A_name=att1\n",
    "        A = []\n",
    "        B_name=att2\n",
    "        B = []\n",
    "        for category in dict_prepost[dates[i]].keys():\n",
    "            if category in X_name:\n",
    "                for word in dict_prepost[dates[i]][category].keys():   \n",
    "                    x = literal_eval(dict_prepost[dates[i]][category][word])\n",
    "                    x = np.array(x)\n",
    "                    X.append(x)\n",
    "        for category in dict_prepost[dates[i+1]].keys():\n",
    "            if category in Y_name:\n",
    "                for word in dict_prepost[dates[i+1]][category].keys():\n",
    "                    y = literal_eval(dict_prepost[dates[i+1]][category][word])\n",
    "                    y = np.array(y)\n",
    "                    Y.append(y)\n",
    "        for attribute in dict_prepost[dates[i]].keys():\n",
    "            if attribute in A_name:\n",
    "                for word in dict_prepost[dates[i]][attribute].keys():\n",
    "                    a = literal_eval(dict_prepost[dates[i]][attribute][word])\n",
    "                    a = np.array(a)\n",
    "                    A.append(a)    \n",
    "        for attribute in dict_prepost[dates[i]].keys():\n",
    "            if attribute in B_name:\n",
    "                for word in dict_prepost[dates[i]][attribute].keys():\n",
    "                    b = literal_eval(dict_prepost[dates[i]][attribute][word])\n",
    "                    b = np.array(b)\n",
    "                    B.append(b)\n",
    "        result_dict = weat(X_name=\"Asians_pre\", X=X, Y_name=\"Asian_post\", Y=Y, A_name=att1, A=A, B_name=att2, B=B, \n",
    "                           permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "        result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "        result_dict_prepost[f\"{att1}_{att2}\"] = result_dict\n",
    "    \n",
    "    if dates[i] not in results.keys():\n",
    "        results[dates[i]] = result_dict_prepost\n",
    "\n",
    "weat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "                                  for i in results.keys()\n",
    "                                  for j in results[i].keys()},\n",
    "                                 orient='index')\n",
    "weat_df = weat_df.reset_index()\n",
    "weat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'se', 'dates']\n",
    "weat_df.head()\n",
    "\n",
    "# weat_df.to_csv('weat_prepost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "# tests = [(\"Warm\", \"Cold\"),(\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     print(dates[i])\n",
    "#     print(dates[i+1])\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         Y_name=\"Asians\"\n",
    "#         Y = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in dict_full[dates[i]].keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in dict_full[dates[i]][category].keys():   \n",
    "#                     x = literal_eval(dict_full[dates[i]][category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for category in dict_full[dates[i+1]].keys():\n",
    "#             if category in Y_name:\n",
    "#                 for word in dict_full[dates[i+1]][category].keys():\n",
    "#                     y = literal_eval(dict_full[dates[i+1]][category][word])\n",
    "#                     y = np.array(y)\n",
    "#                     Y.append(y)\n",
    "#         for attribute in dict_full[dates[i]].keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in dict_full[dates[i]][attribute].keys():\n",
    "#                     a = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)    \n",
    "#         for attribute in dict_full[dates[i]].keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in dict_full[dates[i]][attribute].keys():\n",
    "#                     b = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = weat(X_name=\"Asians\", X=X, Y_name=\"Asian_nextday\", Y=Y, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "#         result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "    \n",
    "#     if dates[i] not in results.keys():\n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# weat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                   for i in results.keys()\n",
    "#                                   for j in results[i].keys()},\n",
    "#                                  orient='index')\n",
    "# weat_df = weat_df.reset_index()\n",
    "# weat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'se', 'dates']\n",
    "# weat_df.head()\n",
    "# weat_df.to_csv('weat_consecutivedays.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "# tests = [(\"Warm\", \"Cold\"),(\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     print(dates[i])\n",
    "#     print(dates[i+1])\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         Y_name=\"Asians\"\n",
    "#         Y = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in dict_full[dates[i]].keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in dict_full[dates[i]][category].keys():   \n",
    "#                     x = literal_eval(dict_full[dates[i]][category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for category in dict_full[dates[i+1]].keys():\n",
    "#             if category in Y_name:\n",
    "#                 for word in dict_full[dates[i+1]][category].keys():\n",
    "#                     y = literal_eval(dict_full[dates[i+1]][category][word])\n",
    "#                     y = np.array(y)\n",
    "#                     Y.append(y)\n",
    "#         for attribute in dict_full[dates[i]].keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in dict_full[dates[i]][attribute].keys():\n",
    "#                     a = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)    \n",
    "#         for attribute in dict_full[dates[i]].keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in dict_full[dates[i]][attribute].keys():\n",
    "#                     b = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = weat(X_name=\"Asians\", X=X, Y_name=\"Asian_nextday\", Y=Y, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "#         result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "    \n",
    "#     if dates[i] not in results.keys():\n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# weat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                   for i in results.keys()\n",
    "#                                   for j in results[i].keys()},\n",
    "#                                  orient='index')\n",
    "# weat_df = weat_df.reset_index()\n",
    "# weat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'se', 'dates']\n",
    "# weat_df.head()\n",
    "# weat_df.to_csv('weat_consecutivedays.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asian corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_asian.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "# tests = [(\"Warm\", \"Cold\"),(\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     print(dates[i])\n",
    "#     print(dates[i+1])\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         Y_name=\"Asians\"\n",
    "#         Y = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in dict_asian[dates[i]].keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in dict_asian[dates[i]][category].keys():   \n",
    "#                     x = literal_eval(dict_asian[dates[i]][category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for category in dict_asian[dates[i+1]].keys():\n",
    "#             if category in Y_name:\n",
    "#                 for word in dict_asian[dates[i+1]][category].keys():\n",
    "#                     y = literal_eval(dict_asian[dates[i+1]][category][word])\n",
    "#                     y = np.array(y)\n",
    "#                     Y.append(y)\n",
    "#         for attribute in dict_asian[dates[i]].keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in dict_asian[dates[i]][attribute].keys():\n",
    "#                     a = literal_eval(dict_asian[dates[i]][attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)    \n",
    "#         for attribute in dict_asian[dates[i]].keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in dict_asian[dates[i]][attribute].keys():\n",
    "#                     b = literal_eval(dict_asian[dates[i]][attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = weat(X_name=\"Asians\", X=X, Y_name=\"Asian_nextday\", Y=Y, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000, cohesion_test=False, cohesion_permutations=100, cohesion_type=0)\n",
    "#         result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "    \n",
    "#     if dates[i] not in results.keys():\n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# weat_asian_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                         for i in results.keys()\n",
    "#                                         for j in results[i].keys()},\n",
    "#                                        orient='index')\n",
    "# weat_asian_df = weat_asian_df.reset_index()\n",
    "# weat_asian_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'se', 'dates']\n",
    "# weat_asian_df.to_csv('weat_asian.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single category WEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Post corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for date, data in dict_prepost.items():\n",
    "#     print(date)\n",
    "#     result_dict_prepost = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in data.keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in data[category].keys():   \n",
    "#                     x = literal_eval(data[category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     a = literal_eval(data[attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     b = literal_eval(data[attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000)\n",
    "#         result_dict_prepost[f\"{att1}_{att2}\"] = result_dict\n",
    "        \n",
    "#     results[date] = result_dict_prepost\n",
    "\n",
    "# sweat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "\n",
    "# sweat_df = sweat_df.reset_index()\n",
    "# sweat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "# sweat_df.head()\n",
    "\n",
    "# sweat_df.to_csv('sweat_prepost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random.randint(1, 1000))\n",
    "results = {}\n",
    "dates = ['pre', 'post']\n",
    "\n",
    "tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "for i in np.arange(len(dates)-1):\n",
    "    if dates[i] not in results.keys():\n",
    "        print(dates[i])\n",
    "        print(dates[i+1])\n",
    "        result_dict_prepost = {}\n",
    "        for att1 in tests:\n",
    "            X_name=\"Asians\"\n",
    "            X = []\n",
    "            A_name=att1\n",
    "            A = []\n",
    "            B_name=att1\n",
    "            B = []\n",
    "            for category in dict_prepost[dates[i]].keys():\n",
    "                if category in X_name:\n",
    "                    for word in dict_prepost[dates[i]][category].keys():   \n",
    "                        x = literal_eval(dict_prepost[dates[i]][category][word])\n",
    "                        x = np.array(x)\n",
    "                        X.append(x)\n",
    "            for attribute in dict_prepost[dates[i]].keys():\n",
    "                if attribute in A_name:\n",
    "                    for word in dict_prepost[dates[i]][attribute].keys():\n",
    "                        a = literal_eval(dict_prepost[dates[i]][attribute][word])\n",
    "                        a = np.array(a)\n",
    "                        A.append(a)    \n",
    "            for attribute in dict_prepost[dates[i+1]].keys():\n",
    "                if attribute in B_name:\n",
    "                    for word in dict_prepost[dates[i+1]][attribute].keys():\n",
    "                        b = literal_eval(dict_prepost[dates[i+1]][attribute][word])\n",
    "                        b = np.array(b)\n",
    "                        B.append(b)\n",
    "            result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_post\", B=B, \n",
    "                             permt=2, perm_n=1000)\n",
    "            result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "            result_dict_prepost[att1] = result_dict\n",
    "            \n",
    "        results[dates[i]] = result_dict_prepost\n",
    "\n",
    "sweat_prepost_consecutive = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "                                          for i in results.keys()\n",
    "                                          for j in results[i].keys()},\n",
    "                                         orient='index')\n",
    "sweat_prepost_consecutive = sweat_prepost_consecutive.reset_index()\n",
    "sweat_prepost_consecutive.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "sweat_prepost_consecutive.to_csv('sweat_prepost_consecutive.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for date, data in dict_full.items():\n",
    "#     print(date)\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in data.keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in data[category].keys():   \n",
    "#                     x = literal_eval(data[category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     a = literal_eval(data[attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     b = literal_eval(data[attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000)\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "        \n",
    "#     results[date] = result_dict_full\n",
    "\n",
    "# sweat_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_df.head()\n",
    "# sweat_df = sweat_df.reset_index()\n",
    "# sweat_df.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_df.to_csv('sweat.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     if dates[i] not in results.keys():\n",
    "#         print(dates[i])\n",
    "#         print(dates[i+1])\n",
    "#         result_dict_full = {}\n",
    "#         for att1 in tests:\n",
    "#             X_name=\"Asians\"\n",
    "#             X = []\n",
    "#             A_name=att1\n",
    "#             A = []\n",
    "#             B_name=att1\n",
    "#             B = []\n",
    "#             for category in dict_full[dates[i]].keys():\n",
    "#                 if category in X_name:\n",
    "#                     for word in dict_full[dates[i]][category].keys():   \n",
    "#                         x = literal_eval(dict_full[dates[i]][category][word])\n",
    "#                         x = np.array(x)\n",
    "#                         X.append(x)\n",
    "#             for attribute in dict_full[dates[i]].keys():\n",
    "#                 if attribute in A_name:\n",
    "#                     for word in dict_full[dates[i]][attribute].keys():\n",
    "#                         a = literal_eval(dict_full[dates[i]][attribute][word])\n",
    "#                         a = np.array(a)\n",
    "#                         A.append(a)    \n",
    "#             for attribute in dict_full[dates[i+1]].keys():\n",
    "#                 if attribute in B_name:\n",
    "#                     for word in dict_full[dates[i+1]][attribute].keys():\n",
    "#                         b = literal_eval(dict_full[dates[i+1]][attribute][word])\n",
    "#                         b = np.array(b)\n",
    "#                         B.append(b)\n",
    "#             result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                              permt=2, perm_n=1000)\n",
    "#             result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#             result_dict_full[att1] = result_dict\n",
    "            \n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# sweat_consecutivedays = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_consecutivedays = sweat_consecutivedays.reset_index()\n",
    "# sweat_consecutivedays.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_consecutivedays.to_csv('sweat_consecutivedays.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on days of subsequent years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_full.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for date in dates:\n",
    "#     if date not in results.keys():\n",
    "#         result_dict_full = {}\n",
    "#         if date[:4] == '2019':\n",
    "#             t2 = re.sub('2019-', '2020-', date)\n",
    "#             if t2 in dict_full.keys():\n",
    "#                 if t2 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t2] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t2].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "#                         for attribute in dict_full[t2].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t2][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t2][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t2]\n",
    "#                         result_dict_full[t2][att1] = result_dict\n",
    "            \n",
    "#             t3 = re.sub('2019-', '2021-', date)\n",
    "#             if t3 in dict_full.keys():\n",
    "#                 if t3 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t3].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "#                         for attribute in dict_full[t3].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t3][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t3][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t3]\n",
    "#                         result_dict_full[t3][att1] = result_dict\n",
    "#         elif date[:4] == '2020':\n",
    "#             t2 = False\n",
    "#             t3 = re.sub('2020-', '2021-', date)\n",
    "#             if t3 in dict_full.keys():\n",
    "#                 if t3 not in result_dict_full.keys():\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                 for att1 in tests:\n",
    "#                     if att1 not in result_dict_full[t3].keys():\n",
    "#                         X_name=\"Asians\"\n",
    "#                         X = []\n",
    "#                         A_name=att1\n",
    "#                         A = []\n",
    "#                         B_name=att1\n",
    "#                         B = []\n",
    "#                         for category in dict_full[date].keys():\n",
    "#                             if category in X_name:\n",
    "#                                 for word in dict_full[date][category].keys():   \n",
    "#                                     x = literal_eval(dict_full[date][category][word])\n",
    "#                                     x = np.array(x)\n",
    "#                                     X.append(x)\n",
    "#                         for attribute in dict_full[date].keys():\n",
    "#                             if attribute in A_name:\n",
    "#                                 for word in dict_full[date][attribute].keys():\n",
    "#                                     a = literal_eval(dict_full[date][attribute][word])\n",
    "#                                     a = np.array(a)\n",
    "#                                     A.append(a)  \n",
    "\n",
    "#                         for attribute in dict_full[t3].keys():\n",
    "#                             if attribute in B_name:\n",
    "#                                 for word in dict_full[t3][attribute].keys():\n",
    "#                                     b = literal_eval(dict_full[t3][attribute][word])\n",
    "#                                     b = np.array(b)\n",
    "#                                     B.append(b)\n",
    "#                         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextyear\", B=B, \n",
    "#                                          permt=2, perm_n=1000)\n",
    "#                         result_dict['dates'] = [date, t3]\n",
    "#                         result_dict_full[t3][att1] = result_dict\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#             results[date] = result_dict_full\n",
    "\n",
    "# sweat_consecutiveyears = pd.DataFrame.from_dict({(i,j,k): results[i][j][k]\n",
    "#                                                  for i in results.keys()\n",
    "#                                                  for j in results[i].keys()\n",
    "#                                                  for k in results[i][j].keys()},\n",
    "#                                                 orient='index')\n",
    "# sweat_consecutiveyears = sweat_consecutiveyears.reset_index()\n",
    "# sweat_consecutiveyears.columns = ['date', 'comparison_date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_consecutiveyears.to_csv('sweat_consecutiveyears.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asian corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare opposing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "\n",
    "# tests = [(\"Warm\", \"Cold\"), (\"Competent\", \"Incompetent\")]\n",
    "\n",
    "# for date, data in dict_asian.items():\n",
    "#     print(date)\n",
    "#     result_dict_full = {}\n",
    "#     for att1, att2 in tests:\n",
    "#         X_name=\"Asians\"\n",
    "#         X = []\n",
    "#         A_name=att1\n",
    "#         A = []\n",
    "#         B_name=att2\n",
    "#         B = []\n",
    "#         for category in data.keys():\n",
    "#             if category in X_name:\n",
    "#                 for word in data[category].keys():   \n",
    "#                     x = literal_eval(data[category][word])\n",
    "#                     x = np.array(x)\n",
    "#                     X.append(x)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in A_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     a = literal_eval(data[attribute][word])\n",
    "#                     a = np.array(a)\n",
    "#                     A.append(a)\n",
    "#         for attribute in data.keys():\n",
    "#             if attribute in B_name:\n",
    "#                 for word in data[attribute].keys():\n",
    "#                     b = literal_eval(data[attribute][word])\n",
    "#                     b = np.array(b)\n",
    "#                     B.append(b)\n",
    "#         result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=att2, B=B, \n",
    "#                            permt=2, perm_n=1000)\n",
    "#         result_dict_full[f\"{att1}_{att2}\"] = result_dict\n",
    "        \n",
    "#     results[date] = result_dict_full\n",
    "\n",
    "# with open(\"s_weat_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(results, outfile)\n",
    "\n",
    "# sweat_asian = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian = sweat_asian.reset_index()\n",
    "# sweat_asian.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot']\n",
    "\n",
    "# sweat_asian.to_csv('sweat_asian.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_asian.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for i in np.arange(len(dates)-1):\n",
    "#     if dates[i] not in results.keys():\n",
    "#         print(dates[i])\n",
    "#         print(dates[i+1])\n",
    "#         result_dict_full = {}\n",
    "#         for att1 in tests:\n",
    "#             X_name=\"Asians\"\n",
    "#             X = []\n",
    "#             A_name=att1\n",
    "#             A = []\n",
    "#             B_name=att1\n",
    "#             B = []\n",
    "#             for category in dict_asian[dates[i]].keys():\n",
    "#                 if category in X_name:\n",
    "#                     for word in dict_asian[dates[i]][category].keys():   \n",
    "#                         x = literal_eval(dict_asian[dates[i]][category][word])\n",
    "#                         x = np.array(x)\n",
    "#                         X.append(x)\n",
    "#             for attribute in dict_asian[dates[i]].keys():\n",
    "#                 if attribute in A_name:\n",
    "#                     for word in dict_asian[dates[i]][attribute].keys():\n",
    "#                         a = literal_eval(dict_asian[dates[i]][attribute][word])\n",
    "#                         a = np.array(a)\n",
    "#                         A.append(a)    \n",
    "#             for attribute in dict_asian[dates[i+1]].keys():\n",
    "#                 if attribute in B_name:\n",
    "#                     for word in dict_asian[dates[i+1]][attribute].keys():\n",
    "#                         b = literal_eval(dict_asian[dates[i+1]][attribute][word])\n",
    "#                         b = np.array(b)\n",
    "#                         B.append(b)\n",
    "#             result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                              permt=2, perm_n=1000)\n",
    "#             result_dict['dates'] = [dates[i], dates[i+1]]\n",
    "#             result_dict_full[att1] = result_dict\n",
    "            \n",
    "#         results[dates[i]] = result_dict_full\n",
    "\n",
    "# sweat_asian_consecutivedays = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian_consecutivedays = sweat_asian_consecutivedays.reset_index()\n",
    "# sweat_asian_consecutivedays.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_asian_consecutivedays.to_csv('sweat_asian_consecutivedays.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare same attribute on days of subsequent years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(random.randint(1, 1000))\n",
    "# results = {}\n",
    "# dates = []\n",
    "# for date in dict_asian.keys():\n",
    "#     if date not in dates:\n",
    "#         dates.append(date)\n",
    "# dates.sort()\n",
    "\n",
    "# tests = [\"Warm\", \"Cold\", \"Competent\", \"Incompetent\", \"Foreign\", \"Diseased\"]\n",
    "\n",
    "# for date in dates:\n",
    "#     if date not in results.keys():\n",
    "#         result_dict_full = {}\n",
    "#         if date[:4] == '2019':\n",
    "#             t2 = re.sub('2019-', '2020-', date)\n",
    "#             if t2 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "#                     for attribute in dict_asian[t2].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t2][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t2][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t2]\n",
    "#                     result_dict_full[t2] = {}\n",
    "#                     result_dict_full[t2][att1] = result_dict\n",
    "#             t3 = re.sub('2019-', '2021-', date)\n",
    "#             if t3 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "\n",
    "#                     for attribute in dict_asian[t3].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t3][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t3][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextday\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t3]\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                     result_dict_full[t3][att1] = result_dict\n",
    "#         elif date[:4] == '2020':\n",
    "#             t2 = False\n",
    "#             t3 = re.sub('2020-', '2021-', date)\n",
    "#             if t3 in dict_asian.keys():\n",
    "#                 for att1 in tests:\n",
    "#                     X_name=\"Asians\"\n",
    "#                     X = []\n",
    "#                     A_name=att1\n",
    "#                     A = []\n",
    "#                     B_name=att1\n",
    "#                     B = []\n",
    "#                     for category in dict_asian[date].keys():\n",
    "#                         if category in X_name:\n",
    "#                             for word in dict_asian[date][category].keys():   \n",
    "#                                 x = literal_eval(dict_asian[date][category][word])\n",
    "#                                 x = np.array(x)\n",
    "#                                 X.append(x)\n",
    "#                     for attribute in dict_asian[date].keys():\n",
    "#                         if attribute in A_name:\n",
    "#                             for word in dict_asian[date][attribute].keys():\n",
    "#                                 a = literal_eval(dict_asian[date][attribute][word])\n",
    "#                                 a = np.array(a)\n",
    "#                                 A.append(a)  \n",
    "\n",
    "#                     for attribute in dict_asian[t3].keys():\n",
    "#                         if attribute in B_name:\n",
    "#                             for word in dict_asian[t3][attribute].keys():\n",
    "#                                 b = literal_eval(dict_asian[t3][attribute][word])\n",
    "#                                 b = np.array(b)\n",
    "#                                 B.append(b)\n",
    "#                     result_dict = s_weat(X_name=\"Asians\", X=X, A_name=att1, A=A, B_name=f\"{att1}_nextyear\", B=B, \n",
    "#                                      permt=2, perm_n=1000)\n",
    "#                     result_dict['dates'] = [date, t3]\n",
    "#                     result_dict_full[t3] = {}\n",
    "#                     result_dict_full[t3][att1] = result_dict\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#             results[date] = result_dict_full\n",
    "\n",
    "# sweat_asian_consecutiveyears = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "#                                           for i in results.keys()\n",
    "#                                           for j in results[i].keys()},\n",
    "#                                          orient='index')\n",
    "# sweat_asian_consecutiveyears = sweat_asian_consecutiveyears.reset_index()\n",
    "# sweat_asian_consecutiveyears.columns = ['date', 'axis', 'category', 'attribute', 'difference', 'standard_dev', 'effect_size', 'Pleft', 'Pright', 'Ptot', 'dates']\n",
    "\n",
    "# sweat_asian_consecutiveyears.to_csv('sweat_asian_consecutiveyears.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with NYT df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nyt = pd.read_csv('df_nyt.csv')\n",
    "# df_nyt = df_nyt.sort_values(by=['date'], ignore_index=True)\n",
    "# df_asian = pd.read_csv('s_weat_asian.csv')\n",
    "# df_asian = df_asian.sort_values(by=['date'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(attribute_weat_asian_df, df_nyt, on=\"date\")\n",
    "# df.to_csv('attribute_weat_asian_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
