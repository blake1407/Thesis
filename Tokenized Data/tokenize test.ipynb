{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_1196/3126634965.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens tensor shape: torch.Size([2, 6])\n",
      "Segments tensor shape: torch.Size([2, 6])\n",
      "Attention mask shape: torch.Size([2, 6])\n",
      "\n",
      "Output information:\n",
      "\n",
      "Last hidden state:\n",
      "Shape: torch.Size([2, 6, 768])\n",
      "Type: torch.float32\n",
      "Device: cpu\n",
      "\n",
      "Number of hidden state layers: 13\n",
      "\n",
      "First hidden state layer:\n",
      "Shape: torch.Size([2, 6, 768])\n",
      "Type: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_bert_sentences(sentences_df):\n",
    "    \"\"\"\n",
    "    Process sentences through BERT with proper tensor conversion.\n",
    "    \n",
    "    Args:\n",
    "        sentences_df (pd.DataFrame): DataFrame containing 'Indexed_Tokens' and 'Segments_IDs' columns\n",
    "    \"\"\"\n",
    "    # Convert DataFrame series to numpy arrays first\n",
    "    tokens_array = np.array(sentences_df['Indexed_Tokens'].tolist())\n",
    "    segments_array = np.array(sentences_df['Segments_IDs'].tolist())\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor(tokens_array)\n",
    "    segments_tensor = torch.tensor(segments_array)\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (tokens_tensor != 0).long()\n",
    "    \n",
    "    # Print shapes for debugging\n",
    "    print(f\"Tokens tensor shape: {tokens_tensor.shape}\")\n",
    "    print(f\"Segments tensor shape: {segments_tensor.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states=True,\n",
    "                                    return_dict=True)\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    segments_tensor = segments_tensor.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    # Get outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokens_tensor,\n",
    "            token_type_ids=segments_tensor,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Helper function to print tensor info\n",
    "def print_tensor_info(tensor, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Shape: {tensor.shape}\")\n",
    "    print(f\"Type: {tensor.dtype}\")\n",
    "    print(f\"Device: {tensor.device}\")\n",
    "\n",
    "# Example usage with detailed debugging\n",
    "def run_example():\n",
    "    # Create sample data - make sure all sequences are the same length\n",
    "    example_data = {\n",
    "        'Indexed_Tokens': [\n",
    "            [101, 2054, 2003, 102, 0, 0],  # Padded to length 6\n",
    "            [101, 2040, 2001, 102, 0, 0]   # Padded to length 6\n",
    "        ],\n",
    "        'Segments_IDs': [\n",
    "            [0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0]\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(example_data)\n",
    "    \n",
    "    try:\n",
    "        # Process sentences\n",
    "        outputs = process_bert_sentences(df)\n",
    "        \n",
    "        # Print detailed information about the outputs\n",
    "        print(\"\\nOutput information:\")\n",
    "        print_tensor_info(outputs.last_hidden_state, \"Last hidden state\")\n",
    "        print(f\"\\nNumber of hidden state layers: {len(outputs.hidden_states)}\")\n",
    "        print_tensor_info(outputs.hidden_states[0], \"First hidden state layer\")\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nInput data shape:\")\n",
    "        print(f\"Number of rows in DataFrame: {len(df)}\")\n",
    "        print(f\"Sample Indexed_Tokens shape: {np.array(df['Indexed_Tokens'].iloc[0]).shape}\")\n",
    "        print(f\"Sample Segments_IDs shape: {np.array(df['Segments_IDs'].iloc[0]).shape}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    outputs = run_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
