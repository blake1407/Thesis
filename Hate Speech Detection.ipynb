{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You may want to change the runtime to GPU for faster training!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 13240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/olid-training-v1.0.tsv',delimiter=\"\\t\")\n",
    "\n",
    "print(f'Number of training samples: {len(df)}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_train(file_name):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    fin = open(file_name)\n",
    "    title = fin.readline()\n",
    "    set_a = ['NOT' , 'OFF']\n",
    "    set_b = ['NULL', 'TIN', 'UNT']\n",
    "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
    "    while True:\n",
    "        line = fin.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        items = line.split('\\t')\n",
    "        text = items[1]\n",
    "        label_a = set_a.index(items[2].strip())\n",
    "        label_b = set_b.index(items[3].strip())\n",
    "        label_c = set_c.index(items[4].strip())\n",
    "\n",
    "        if len(text) > 0:\n",
    "            texts.append(text)\n",
    "            labels.append([label_a, label_b, label_c])\n",
    "            \n",
    "    return {'texts':texts, 'labels':labels}\n",
    "\n",
    "def reader_test(test_textlist, test_labellist):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    text_dict = {}\n",
    "    \n",
    "    # build text_dict\n",
    "    for file_text in test_textlist:\n",
    "        fin = open(file_text)\n",
    "        title = fin.readline()\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            items = line.split('\\t')\n",
    "            if items[0] not in text_dict:\n",
    "                text_dict[items[0]] = items[1]\n",
    "        fin.close()\n",
    "    label_dict_list = []\n",
    "    \n",
    "    # build label_dict\n",
    "    for i, file_label in enumerate(test_labellist):\n",
    "        label_dict_list.append({})\n",
    "        fin = open(file_label)\n",
    "        title = fin.readline()\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            items = line.split(',')\n",
    "            label_dict_list[i][items[0]] = items[1]\n",
    "        fin.close()    \n",
    "    \n",
    "    set_a = ['NOT' , 'OFF']\n",
    "    set_b = ['NULL', 'TIN', 'UNT']\n",
    "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
    "    \n",
    "    for idx, text in text_dict.items():\n",
    "        if len(text) > 0:\n",
    "            texts.append(text)\n",
    "            if idx in label_dict_list[0]:\n",
    "                label_a = label_dict_list[0][idx]\n",
    "            else:\n",
    "                label_a = 'OFF'\n",
    "            if idx in label_dict_list[1]:\n",
    "                label_b = label_dict_list[1][idx]\n",
    "            else:\n",
    "                label_b = 'NULL'\n",
    "            if idx in label_dict_list[2]:\n",
    "                label_c = label_dict_list[2][idx]\n",
    "            else:\n",
    "                label_c = 'NULL'\n",
    "            \n",
    "            label_a = set_a.index(label_a.strip())\n",
    "            label_b = set_b.index(label_b.strip())\n",
    "            label_c = set_c.index(label_c.strip())\n",
    "        \n",
    "            labels.append([label_a, label_b, label_c])\n",
    "            \n",
    "    return {'texts':texts, 'labels':labels}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OlidDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = input_set['texts']\n",
    "        self.labels = input_set['labels']\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        texts = []\n",
    "        labels_a = []\n",
    "        labels_b = []\n",
    "        labels_c = []\n",
    "\n",
    "        for b in batch:\n",
    "            texts.append(b['text'])\n",
    "            labels_a.append(b['label_a'])\n",
    "            labels_b.append(b['label_b'])\n",
    "            labels_c.append(b['label_c'])\n",
    "\n",
    "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
    "        # We also pad shorter sentences to a length of 128 tokens\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        labels = {}\n",
    "        encodings['label_a'] =  torch.tensor(labels_a)\n",
    "        encodings['label_b'] =  torch.tensor(labels_b)\n",
    "        encodings['label_c'] =  torch.tensor(labels_c)\n",
    "        \n",
    "        return encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        item = {'text': self.texts[idx],\n",
    "                'label_a': self.labels[idx][0],\n",
    "                'label_b': self.labels[idx][1],\n",
    "                'label_c': self.labels[idx][2]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489cb3bc217b4a759f5d5e5d72d05a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109d754b3c30434fa172b63193165084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194cf7dc460342e2afb0a8ef00e36117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbcf258bab944b788df060ada250926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# we can check the parameters of this tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = reader_train('./data/olid-training-v1.0.tsv')\n",
    "testset = reader_test(['./data/testset-levela.tsv','./data/testset-levelb.tsv','./data/testset-levelc.tsv'], \n",
    "                      ['./data/labels-levela.csv','./data/labels-levelb.csv','./data/labels-levelc.csv'])\n",
    "\n",
    "train_dataset = OlidDataset(tokenizer, trainset)\n",
    "test_dataset = OlidDataset(tokenizer, testset)\n",
    "\n",
    "#returns first item as dictionnary\n",
    "#print(train_dataset[0])\n",
    "\n",
    "# put all train set into one batch for the collate_fn function\n",
    "# batch = [sample for sample in train_dataset]\n",
    "\n",
    "# encodings = train_dataset.collate_fn(batch[:10])\n",
    "\n",
    "# for key, value in encodings.items():\n",
    "#   print(f\"{key}: {value.numpy().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5c8d675335495ab2e1f0851db6cf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 108310272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#180 M\n",
    "print(f\"Model size: {model.num_parameters()}\")\n",
    "\n",
    "#model summary\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_hate_speech(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # BERT Model\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # Task A\n",
    "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
    "                                                torch.nn.Linear(config.hidden_size, 2))\n",
    "      \n",
    "        # the sigmoid activation here is not strictly necessary.\n",
    "        # However, its presence here doesn't make a big difference to the model's performance\n",
    "        # Why do you think that is?\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Task B\n",
    "        # TBA\n",
    "        \n",
    "        # Task C\n",
    "        # TBA\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None):\n",
    " \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Logits A\n",
    "        logits_a = self.projection_a(outputs[1])\n",
    "        out =  self.activation(logits_a)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_hate_speech(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = {}\n",
    "        labels['label_a'] = inputs.pop('label_a')\n",
    "        labels['label_b'] = inputs.pop('label_b')\n",
    "        labels['label_c'] = inputs.pop('label_c')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # TASK A\n",
    "        loss_task_a = nn.CrossEntropyLoss()\n",
    "        labels_a = labels['label_a']\n",
    "        loss_a = loss_task_a(outputs.view(-1, 2), labels_a.view(-1))\n",
    "\n",
    "        loss = loss_a\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hate_speech():\n",
    "\n",
    "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
    "    model = BERT_hate_speech.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./experiment/hate_speech',\n",
    "        learning_rate = 0.0001,\n",
    "        logging_steps= 100,\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs = 3,\n",
    "        remove_unused_columns=False # This argument prevents the collator to drop data from our batch when customizing the data collator\n",
    "    )\n",
    "    trainer = Trainer_hate_speech(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=train_dataset,                   \n",
    "        data_collator=train_dataset.collate_fn\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model('./models/ht_bert_finetuned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BERT_hate_speech were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['projection_a.1.bias', 'projection_a.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3b3e731e384b5ba68b6efae3dc2ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5826, 'learning_rate': 9.194847020933978e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5557, 'learning_rate': 8.389694041867955e-05, 'epoch': 0.48}\n"
     ]
    }
   ],
   "source": [
    "main_hate_speech()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
