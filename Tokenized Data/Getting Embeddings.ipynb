{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(input_df, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract BERT embeddings for tokens in the input dataframe.\n",
    "    Returns a DataFrame with token-level embeddings and metadata.\n",
    "    \"\"\"\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each row in the input DataFrame\n",
    "    for _, row in input_df.iterrows():\n",
    "        subject_id = row['Subject_ID']\n",
    "        tweet_id = row['Tweet_ID']\n",
    "        tokens = eval(row['Tokens'])  # Convert string representation to list\n",
    "        token_ids = eval(row['Indexed_Tokens'])\n",
    "        segment_ids = eval(row['Segments_IDs'])\n",
    "\n",
    "        # Split long token sequences into chunks of max_length\n",
    "        for i in range(0, len(token_ids), max_length):\n",
    "            chunk_token_ids = token_ids[i:i+max_length]\n",
    "            chunk_segment_ids = segment_ids[i:i+max_length]\n",
    "            chunk_tokens = tokens[i:i+max_length]\n",
    "        \n",
    "            # Convert inputs to tensors\n",
    "            tokens_tensor = torch.tensor([chunk_token_ids])\n",
    "            segments_tensor = torch.tensor([chunk_segment_ids])\n",
    "            \n",
    "            # Get BERT embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor, segments_tensor)\n",
    "                hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "                \n",
    "            # Process each token in the chunk\n",
    "            for j, (token, embedding) in enumerate(zip(chunk_tokens, hidden_states)):\n",
    "                if token in ['[CLS]', '[SEP]']:\n",
    "                    continue\n",
    "                \n",
    "                # Convert embedding to numpy and store as list\n",
    "                token_embedding = embedding.numpy().tolist()\n",
    "                \n",
    "                # Store all information\n",
    "                result = {\n",
    "                    'Subject_ID': subject_id,\n",
    "                    'Tweet_ID': tweet_id,\n",
    "                    'Token': token,\n",
    "                    'Token_ID': chunk_token_ids[j],\n",
    "                    'Segment_ID': chunk_segment_ids[j],\n",
    "                    'Position': i + j,  # Position in the full text\n",
    "                    'Embedding': token_embedding\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the resulting DataFrame:\n",
      "   Subject_ID  Tweet_ID        Token  Token_ID  Segment_ID  Position  \\\n",
      "0          74       353         holy      4151           0         1   \n",
      "1          74       353      fucking      8239           0         2   \n",
      "2          74       353         shit      4485           0         3   \n",
      "3          74       353  criticizing     21289           0         4   \n",
      "4          74       353       israel      3956           0         5   \n",
      "\n",
      "                                           Embedding  \n",
      "0  [0.5452250838279724, 1.0058563947677612, 0.125...  \n",
      "1  [0.46159079670906067, 0.9647833108901978, 0.13...  \n",
      "2  [0.5526859164237976, 0.8520041108131409, 0.025...  \n",
      "3  [0.16068339347839355, 0.7583393454551697, -0.0...  \n",
      "4  [0.26864200830459595, 0.864945113658905, 0.004...  \n",
      "\n",
      "DataFrame shape: (1692, 7)\n",
      "Number of embedding dimensions: 0\n"
     ]
    }
   ],
   "source": [
    "status = \"Before\"\n",
    "# Read the input CSV\n",
    "input_df = pd.read_csv(f'{status}_Tokenized.csv')\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_df = get_bert_embeddings(input_df)\n",
    "\n",
    "# Save to CSV\n",
    "embeddings_df.to_csv(f'{status}_token_embeddings.csv', index=False)\n",
    "\n",
    "# Print sample of results\n",
    "print(\"\\nSample of the resulting DataFrame:\")\n",
    "print(embeddings_df.head())\n",
    "\n",
    "# Print shape information\n",
    "print(\"\\nDataFrame shape:\", embeddings_df.shape)\n",
    "print(\"Number of embedding dimensions:\", len([col for col in embeddings_df.columns if 'embedding_' in col]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
