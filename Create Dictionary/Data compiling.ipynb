{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "a621e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import contextlib\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from functools import total_ordering\n",
    "from itertools import chain, islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader import CorpusReader\n",
    "from nltk.internals import deprecated\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import binary_search_file as _binary_search_file\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "import warnings\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "f8c39401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d8bca",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "5d6c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    new = []\n",
    "    # Handle both string and list inputs\n",
    "    if isinstance(word, str):\n",
    "        words_to_check = [word]\n",
    "    elif isinstance(word, list):\n",
    "        words_to_check = word\n",
    "    else:\n",
    "        return []  # Return empty list if word is neither string nor list\n",
    "    \n",
    "    for text in words_to_check:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            # Hypernyms\n",
    "            hypernyms = syn.hypernyms()\n",
    "            if len(hypernyms) > 0:\n",
    "                for hypernym in hypernyms:\n",
    "                    if hypernym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hypernym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Hyponyms\n",
    "            hyponyms = syn.hyponyms()\n",
    "            if len(hyponyms) > 0:\n",
    "                for hyponym in hyponyms:\n",
    "                    if hyponym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hyponym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Holonyms\n",
    "            member_holonyms = syn.member_holonyms()\n",
    "            if len(member_holonyms) > 0:\n",
    "                for holonym in member_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            substance_holonyms = syn.substance_holonyms()\n",
    "            if len(substance_holonyms) > 0:\n",
    "                for holonym in substance_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            part_holonyms = syn.part_holonyms()\n",
    "            if len(part_holonyms) > 0:\n",
    "                for holonym in part_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Meronyms\n",
    "            member_meronyms = syn.member_meronyms()\n",
    "            if len(member_meronyms) > 0:\n",
    "                for meronym in member_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            substance_meronyms = syn.substance_meronyms()\n",
    "            if len(substance_meronyms) > 0:\n",
    "                for meronym in substance_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            part_meronyms = syn.part_meronyms()\n",
    "            if len(part_meronyms) > 0:\n",
    "                for meronym in part_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Also see\n",
    "            also_sees = syn.also_sees()\n",
    "            if len(also_sees) > 0:\n",
    "                for seealso in also_sees:\n",
    "                    if seealso.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = seealso.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Similar to\n",
    "            similar_tos = syn.similar_tos()\n",
    "            if len(similar_tos) > 0:\n",
    "                for similar in similar_tos:\n",
    "                    if similar.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = similar.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Attributes\n",
    "            attributes = syn.attributes()\n",
    "            if len(attributes) > 0:\n",
    "                for attribute in attributes:\n",
    "                    if attribute.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = attribute.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Synonyms\n",
    "            if syn.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                word = syn.name().split(\".\")[0]\n",
    "                if word not in new:\n",
    "                    new.append(word)\n",
    "                # Derivatives\n",
    "                lemmas = wordnet.lemmas(syn.name().split(\".\")[0], syn.name().split(\".\")[1])\n",
    "                if len(lemmas) > 0:\n",
    "                    for lemma in lemmas:\n",
    "                        if lemma.syntactic_marker():\n",
    "                            new.append(lemma.name())\n",
    "                        else:\n",
    "                            pass\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return list(set(final)) #Get only unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "7662d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonyms(word):\n",
    "    new = []\n",
    "    # Handle both string and list inputs\n",
    "    if isinstance(word, pd.Series):\n",
    "        # Flatten the Series of lists into a single list\n",
    "        words_to_check = [item for sublist in word.tolist() for item in sublist]\n",
    "    if isinstance(word, str):\n",
    "        words_to_check = [word]\n",
    "    elif isinstance(word, list):\n",
    "        words_to_check = word\n",
    "    \n",
    "    for text in words_to_check:\n",
    "        # Get all synsets for the word\n",
    "        for syn in wordnet.synsets(text):\n",
    "            # Get all lemmas for the synset\n",
    "            for lemma in syn.lemmas():\n",
    "                # Get antonyms for each lemma\n",
    "                antonyms = lemma.antonyms()\n",
    "                if antonyms:  # Check if there are any antonyms\n",
    "                    for antonym in antonyms:\n",
    "                        word = antonym.name()\n",
    "                        new.append(word)\n",
    "    \n",
    "    # Lemmatize and deduplicate\n",
    "    final = []\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in final:\n",
    "            final.append(lemma)\n",
    "    \n",
    "    return list(set(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "f4eb4933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Incompetence' 'Cold' 'Warm' 'Jews' 'Competence' 'Christians']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Word</th>\n",
       "      <th>Synonyms</th>\n",
       "      <th>Antonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incompetence</td>\n",
       "      <td>unskilled</td>\n",
       "      <td>[humble, unskilled, bad, hopeless, unprofessio...</td>\n",
       "      <td>[skilled]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cold</td>\n",
       "      <td>untrustworthy</td>\n",
       "      <td>[unreliable, fly-by-night, slippery, devious, ...</td>\n",
       "      <td>[trustworthy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Warm</td>\n",
       "      <td>warm</td>\n",
       "      <td>[loving, enthusiastic, warming, warm, near, em...</td>\n",
       "      <td>[cool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Warm</td>\n",
       "      <td>friendly</td>\n",
       "      <td>[social, warm, sociable, couthie, affable, com...</td>\n",
       "      <td>[unfriendly, hostile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jews</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>[Jewish, jewish]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category           Word  \\\n",
       "0  Incompetence      unskilled   \n",
       "1          Cold  untrustworthy   \n",
       "2          Warm           warm   \n",
       "3          Warm       friendly   \n",
       "4          Jews         Jewish   \n",
       "\n",
       "                                            Synonyms               Antonyms  \n",
       "0  [humble, unskilled, bad, hopeless, unprofessio...              [skilled]  \n",
       "1  [unreliable, fly-by-night, slippery, devious, ...          [trustworthy]  \n",
       "2  [loving, enthusiastic, warming, warm, near, em...                 [cool]  \n",
       "3  [social, warm, sociable, couthie, affable, com...  [unfriendly, hostile]  \n",
       "4                                   [Jewish, jewish]                     []  "
      ]
     },
     "execution_count": 1176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Raw_vocab_dictionary.csv\")\n",
    "\n",
    "# Check categories\n",
    "print(df['Category'].unique())\n",
    "\n",
    "#Synonym and Antonyms of each word\n",
    "df[\"Synonyms\"] = df[\"Word\"].apply(get_synonyms)\n",
    "df[\"Antonyms\"] = df[\"Word\"].apply(get_antonyms)\n",
    "\n",
    "word_dict = defaultdict(list)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    comp_antonyms = []\n",
    "    if row[\"Category\"] == \"Cold\":\n",
    "        cold_antonyms = get_antonyms(row[\"Synonyms\"])\n",
    "        word_dict[\"Cold\"].extend(row[\"Synonyms\"])\n",
    "    if row[\"Category\"] == \"Warm\":\n",
    "        warm_antonyms = get_antonyms(row[\"Synonyms\"])\n",
    "        word_dict[\"Warm\"].extend(row[\"Synonyms\"])\n",
    "    if row[\"Category\"] == \"Competence\":\n",
    "        comp_antonyms = get_antonyms(row[\"Synonyms\"])\n",
    "        word_dict[\"Competence\"].extend(row[\"Synonyms\"])\n",
    "    if row[\"Category\"] == \"Incompetence\":\n",
    "        incomp_antonyms = get_antonyms(row[\"Synonyms\"])\n",
    "        word_dict[\"Incompetence\"].extend(row[\"Synonyms\"])\n",
    "    if row[\"Category\"] == \"Jews\":\n",
    "        word_dict[\"Jews\"].extend(row[\"Synonyms\"])\n",
    "    if row[\"Category\"] == \"Christians\":\n",
    "        word_dict[\"Christians\"].extend(row[\"Synonyms\"])\n",
    "\n",
    "    word_dict[\"Warm\"].extend(cold_antonyms)\n",
    "    word_dict[\"Cold\"].extend(warm_antonyms)\n",
    "    word_dict[\"Competence\"].extend(incomp_antonyms)\n",
    "    word_dict[\"Incompetence\"].extend(comp_antonyms)\n",
    "\n",
    "\n",
    "word_dict[\"Warm\"] = list(set(word_dict[\"Warm\"]))\n",
    "word_dict[\"Cold\"] = list(set(word_dict[\"Cold\"]))\n",
    "word_dict[\"Competence\"] = list(set(word_dict[\"Competence\"]))\n",
    "word_dict[\"Incompetence\"] = list(set(word_dict[\"Incompetence\"]))\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "4ca9eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Christians': ['Christians',\n",
      "                'Protestant',\n",
      "                'complaining',\n",
      "                'protestant',\n",
      "                'Catholic',\n",
      "                'catholic',\n",
      "                'broad-minded',\n",
      "                'christlike',\n",
      "                'christianly',\n",
      "                'Christian',\n",
      "                'christian'],\n",
      " 'Cold': ['unpatriotic',\n",
      "          'recreant',\n",
      "          'fly-by-night',\n",
      "          'unforgiving',\n",
      "          'refrigerant',\n",
      "          'unpleasant',\n",
      "          'faithless',\n",
      "          'unfriendly',\n",
      "          'selfish',\n",
      "          'acold',\n",
      "          'deceitful',\n",
      "          'passionless',\n",
      "          'discourage',\n",
      "          'unvoiced',\n",
      "          'ignoble',\n",
      "          'neutral',\n",
      "          'cerebral',\n",
      "          'merciless',\n",
      "          'undependable',\n",
      "          'undesirable',\n",
      "          'corrupt',\n",
      "          'loud',\n",
      "          'inhumane',\n",
      "          'normal',\n",
      "          'dishonorable',\n",
      "          'base',\n",
      "          'weak',\n",
      "          'uncordial',\n",
      "          'dishonest',\n",
      "          'impotent',\n",
      "          'arctic',\n",
      "          'evilness',\n",
      "          'bad',\n",
      "          'inhospitable',\n",
      "          'slippery',\n",
      "          'algid',\n",
      "          'bitter',\n",
      "          'contemptible',\n",
      "          'unlovable',\n",
      "          'unpeaceful',\n",
      "          'playful',\n",
      "          'picaresque',\n",
      "          'ill-natured',\n",
      "          'impolite',\n",
      "          'frosty',\n",
      "          'unenthusiastic',\n",
      "          'unreliable',\n",
      "          'unrespectable',\n",
      "          'unsealed',\n",
      "          'comfortable',\n",
      "          'deceptive',\n",
      "          'malicious',\n",
      "          'irresponsible',\n",
      "          'offensive',\n",
      "          'far',\n",
      "          'unsocial',\n",
      "          'head-on',\n",
      "          'cold',\n",
      "          'ungracious',\n",
      "          'preserved',\n",
      "          'beggarly',\n",
      "          'lineal',\n",
      "          'stingy',\n",
      "          'mutinous',\n",
      "          'frivolous',\n",
      "          'ambidextrous',\n",
      "          'irreconcilable',\n",
      "          'hostile',\n",
      "          'opponent',\n",
      "          'unsteady',\n",
      "          'untrustworthy',\n",
      "          'ice-cold',\n",
      "          'evil',\n",
      "          'shivery',\n",
      "          'unconscious',\n",
      "          'condemnable',\n",
      "          'cool',\n",
      "          'wrong',\n",
      "          'egoistic',\n",
      "          'devious',\n",
      "          'hard',\n",
      "          'emotionless',\n",
      "          'untruthful',\n",
      "          'nominal',\n",
      "          'disingenuous',\n",
      "          'salty',\n",
      "          'refrigerated',\n",
      "          'frore',\n",
      "          'average',\n",
      "          'false',\n",
      "          'disagreeable',\n",
      "          'malign',\n",
      "          'hardened',\n",
      "          'unoriginal',\n",
      "          'insincere',\n",
      "          'bastardly',\n",
      "          'unintended',\n",
      "          'uncivil',\n",
      "          'unloving',\n",
      "          'unneighborly',\n",
      "          'improperly',\n",
      "          'discourteous',\n",
      "          'unsure',\n",
      "          'hateful',\n",
      "          'beguiling',\n",
      "          'aggressive',\n",
      "          'disloyal',\n",
      "          'vicious',\n",
      "          'frozen',\n",
      "          'counterfeit',\n",
      "          'indecent',\n",
      "          'incoherent',\n",
      "          'stale',\n",
      "          'violent',\n",
      "          'forte',\n",
      "          'mean',\n",
      "          'dirty',\n",
      "          'nasty',\n",
      "          'perfect',\n",
      "          'barbarous',\n",
      "          'insurgent',\n",
      "          'uncertain',\n",
      "          'silence',\n",
      "          'corruptible',\n",
      "          'poor',\n",
      "          'unkind',\n",
      "          'discouraging',\n",
      "          'unsound',\n",
      "          'uncharitable',\n",
      "          'unsteadily',\n",
      "          'dull',\n",
      "          'antagonistic',\n",
      "          'malignant',\n",
      "          'unheated',\n",
      "          'negative',\n",
      "          'self-serving',\n",
      "          'skilled',\n",
      "          'crooked',\n",
      "          'inconsistent',\n",
      "          'bleak',\n",
      "          'dead',\n",
      "          'egotistic',\n",
      "          'unexcitable',\n",
      "          'unemotional',\n",
      "          'rebellious',\n",
      "          'frigorific',\n",
      "          'unfaithful',\n",
      "          'imprecise',\n",
      "          'chilly',\n",
      "          'unfastidious',\n",
      "          'unreal',\n",
      "          'unreproducible',\n",
      "          'wicked',\n",
      "          'rascally',\n",
      "          'insubstantial',\n",
      "          'devoice',\n",
      "          'at_loggerheads',\n",
      "          'beetle-browed',\n",
      "          'intense',\n",
      "          'discordant',\n",
      "          'un-american',\n",
      "          'crisp',\n",
      "          'heatless',\n",
      "          'hate',\n",
      "          'unsupportive',\n",
      "          'unsociable',\n",
      "          'uncongenial',\n",
      "          'stone-cold',\n",
      "          'poisonous',\n",
      "          'frigid',\n",
      "          'inconsiderate',\n",
      "          'ill',\n",
      "          'badness',\n",
      "          'thieving',\n",
      "          'abominable'],\n",
      " 'Competence': ['able',\n",
      "                'politic',\n",
      "                'streetwise',\n",
      "                'workmanlike',\n",
      "                'felicitous',\n",
      "                'proud',\n",
      "                'adequate',\n",
      "                'stylish',\n",
      "                'dependent',\n",
      "                'advisable',\n",
      "                'warm',\n",
      "                'nice',\n",
      "                'reassured',\n",
      "                'well-qualified',\n",
      "                'businesslike',\n",
      "                'cocksure',\n",
      "                'adequate_to',\n",
      "                'skillful',\n",
      "                'accomplished',\n",
      "                'educated',\n",
      "                'smart',\n",
      "                'susceptible',\n",
      "                'determinate',\n",
      "                'cost-efficient',\n",
      "                'self-assured',\n",
      "                'adept',\n",
      "                'assured',\n",
      "                'independent',\n",
      "                'astute',\n",
      "                'prehensile',\n",
      "                'articulate',\n",
      "                'restricted',\n",
      "                'immutable',\n",
      "                'definite',\n",
      "                'resourceful',\n",
      "                'automatic',\n",
      "                'expeditious',\n",
      "                'powerful',\n",
      "                'delicate',\n",
      "                'precise',\n",
      "                'unambiguous',\n",
      "                'chic',\n",
      "                'professional',\n",
      "                'ball-hawking',\n",
      "                'seal',\n",
      "                'eligible',\n",
      "                'economic',\n",
      "                'dependable',\n",
      "                'registered',\n",
      "                'confident',\n",
      "                'convinced',\n",
      "                'sophisticated',\n",
      "                'invariable',\n",
      "                'limited',\n",
      "                'unregretful',\n",
      "                'proficient',\n",
      "                'disingenuous',\n",
      "                'arch',\n",
      "                'healthy',\n",
      "                'experienced',\n",
      "                'streamlined',\n",
      "                'hedged',\n",
      "                'apt',\n",
      "                'unequivocal',\n",
      "                'qualified',\n",
      "                'hopeful',\n",
      "                'good',\n",
      "                'brainy',\n",
      "                'versatile',\n",
      "                'agile',\n",
      "                'artful',\n",
      "                'intelligent',\n",
      "                'mean',\n",
      "                'dirty',\n",
      "                'graceful',\n",
      "                'wise',\n",
      "                'scintillating',\n",
      "                'capable',\n",
      "                'conditional',\n",
      "                'certain',\n",
      "                'prudent',\n",
      "                'reasonable',\n",
      "                'quick',\n",
      "                'fresh',\n",
      "                'searching',\n",
      "                'technical',\n",
      "                'well-advised',\n",
      "                'goodness',\n",
      "                'efficient',\n",
      "                'hot',\n",
      "                'consummate',\n",
      "                'sure',\n",
      "                'skilled',\n",
      "                'strong',\n",
      "                'unfairly',\n",
      "                'high-octane',\n",
      "                'sealed',\n",
      "                'fast',\n",
      "                'adroit',\n",
      "                'precocious',\n",
      "                'sure-handed',\n",
      "                'reliable',\n",
      "                'informed',\n",
      "                'rational',\n",
      "                'effective',\n",
      "                'bright',\n",
      "                'refined',\n",
      "                'fit',\n",
      "                'intense',\n",
      "                'accelerate',\n",
      "                'certified',\n",
      "                'competent',\n",
      "                'heat',\n",
      "                'modified',\n",
      "                'predictable',\n",
      "                'cagey',\n",
      "                'unchangeable',\n",
      "                'unclean',\n",
      "                'natural',\n",
      "                'forward'],\n",
      " 'Incompetence': ['unnatural',\n",
      "                  'back',\n",
      "                  'uneconomical',\n",
      "                  'dependent',\n",
      "                  'unworkmanlike',\n",
      "                  'unregistered',\n",
      "                  'asinine',\n",
      "                  'inadequate',\n",
      "                  'impolitic',\n",
      "                  'maladroit',\n",
      "                  'variable',\n",
      "                  'styleless',\n",
      "                  'hopeless',\n",
      "                  'sharp',\n",
      "                  'indeterminate',\n",
      "                  'unhealthy',\n",
      "                  'helpless',\n",
      "                  'weak',\n",
      "                  'decertify',\n",
      "                  'nonprehensile',\n",
      "                  'up_in_the_air',\n",
      "                  'bad',\n",
      "                  'retarded',\n",
      "                  'uneducated',\n",
      "                  'unable',\n",
      "                  'ill-conceived',\n",
      "                  'dimmed',\n",
      "                  'independent',\n",
      "                  'diffident',\n",
      "                  'ambiguous',\n",
      "                  'unreliable',\n",
      "                  'ill-advised',\n",
      "                  'incapable',\n",
      "                  'bungling',\n",
      "                  'unsealed',\n",
      "                  'dumb',\n",
      "                  'cold',\n",
      "                  'artless',\n",
      "                  'humble',\n",
      "                  'preserved',\n",
      "                  'inefficient',\n",
      "                  'harebrained',\n",
      "                  'incompetent',\n",
      "                  'ignorant',\n",
      "                  'fond',\n",
      "                  'stupid',\n",
      "                  'feckless',\n",
      "                  'uninformed',\n",
      "                  'cool',\n",
      "                  'uncertified',\n",
      "                  'crude',\n",
      "                  'rugged',\n",
      "                  'ambivalent',\n",
      "                  'unlimited',\n",
      "                  'unprofessional',\n",
      "                  'bungled',\n",
      "                  'absurd',\n",
      "                  'salty',\n",
      "                  'slow',\n",
      "                  'supernatural',\n",
      "                  'unrestricted',\n",
      "                  'awkward',\n",
      "                  'botchy',\n",
      "                  'unskilled',\n",
      "                  'unsusceptible',\n",
      "                  'powerless',\n",
      "                  'lubberly',\n",
      "                  'nontechnical',\n",
      "                  'unfit',\n",
      "                  'imprudent',\n",
      "                  'cockamamie',\n",
      "                  'foolish',\n",
      "                  'worry',\n",
      "                  'derestrict',\n",
      "                  'unsure',\n",
      "                  'outright',\n",
      "                  'irrational',\n",
      "                  'inarticulate',\n",
      "                  'naive',\n",
      "                  'categoric',\n",
      "                  'semiskilled',\n",
      "                  'unintelligent',\n",
      "                  'brainless',\n",
      "                  'unmodified',\n",
      "                  'stale',\n",
      "                  'ineligible',\n",
      "                  'unconvinced',\n",
      "                  'ineffective',\n",
      "                  'disagree',\n",
      "                  'contingent',\n",
      "                  'unwise',\n",
      "                  'nasty',\n",
      "                  'uncertain',\n",
      "                  'mild',\n",
      "                  'flat',\n",
      "                  'unpredictable',\n",
      "                  'unconditional',\n",
      "                  'local',\n",
      "                  'dense',\n",
      "                  'dull',\n",
      "                  'artificial',\n",
      "                  'quack',\n",
      "                  'changeable',\n",
      "                  'aft',\n",
      "                  'clean',\n",
      "                  'unentitled',\n",
      "                  'inept',\n",
      "                  'inexperienced',\n",
      "                  'indefinite',\n",
      "                  'imprecise',\n",
      "                  'manual',\n",
      "                  'anserine',\n",
      "                  'reverse',\n",
      "                  'groping',\n",
      "                  'rattlebrained',\n",
      "                  'infelicitous',\n",
      "                  'unfashionable',\n",
      "                  'amateurish',\n",
      "                  'chancy',\n",
      "                  'speechless',\n",
      "                  'disqualify',\n",
      "                  'unreasonable',\n",
      "                  'doubtful',\n",
      "                  'unqualified',\n",
      "                  'out_of_practice',\n",
      "                  'inadvisable',\n",
      "                  'backward'],\n",
      " 'Jews': ['Jewish', 'jewish', 'Jew', 'Jews', 'jewish'],\n",
      " 'Warm': ['right-handed',\n",
      "          'accordant',\n",
      "          'unfreeze',\n",
      "          'serious',\n",
      "          'inoffensive',\n",
      "          'creditworthy',\n",
      "          'nice',\n",
      "          'warm',\n",
      "          'consistent',\n",
      "          'fiducial',\n",
      "          'lovable',\n",
      "          'near',\n",
      "          'excitable',\n",
      "          'lukewarm',\n",
      "          'faithful',\n",
      "          'couthie',\n",
      "          'live',\n",
      "          'kind',\n",
      "          'affectionate',\n",
      "          'uncomfortable',\n",
      "          'warmly',\n",
      "          'hotness',\n",
      "          'altruistic',\n",
      "          'well',\n",
      "          'virtuous',\n",
      "          'dainty',\n",
      "          'positive',\n",
      "          'real',\n",
      "          'dear',\n",
      "          'peaceful',\n",
      "          'estimable',\n",
      "          'rightfulness',\n",
      "          'loyal',\n",
      "          'boil',\n",
      "          'enthusiastic',\n",
      "          'warming',\n",
      "          'cozy',\n",
      "          'equable',\n",
      "          'intended',\n",
      "          'emotional',\n",
      "          'desirable',\n",
      "          'generous',\n",
      "          'incorrupt',\n",
      "          'companionate',\n",
      "          'agonist',\n",
      "          'ardent',\n",
      "          'honorable',\n",
      "          'respectable',\n",
      "          'reconcilable',\n",
      "          'courteous',\n",
      "          'unmalicious',\n",
      "          'precise',\n",
      "          'patriotic',\n",
      "          'certificatory',\n",
      "          'trustworthy',\n",
      "          'forgiving',\n",
      "          'affable',\n",
      "          'right',\n",
      "          'straight',\n",
      "          'dependable',\n",
      "          'heart-whole',\n",
      "          'left-handed',\n",
      "          'steady',\n",
      "          'earnest',\n",
      "          'abnormal',\n",
      "          'charitable',\n",
      "          'collateral',\n",
      "          'sociable',\n",
      "          'warmed',\n",
      "          'neighborly',\n",
      "          'accessory',\n",
      "          'bona_fide',\n",
      "          'alive',\n",
      "          'unskilled',\n",
      "          'chummy',\n",
      "          'loving',\n",
      "          'imperfect',\n",
      "          'good',\n",
      "          'supportive',\n",
      "          'passionate',\n",
      "          'well-intentioned',\n",
      "          'rich',\n",
      "          'unfrozen',\n",
      "          'amiable',\n",
      "          'genuine',\n",
      "          'tested',\n",
      "          'cordial',\n",
      "          'unselfish',\n",
      "          'congenial',\n",
      "          'encouraging',\n",
      "          'demonstrative_of',\n",
      "          'mild',\n",
      "          'kindhearted',\n",
      "          'certain',\n",
      "          'benign',\n",
      "          'quick',\n",
      "          'fresh',\n",
      "          'paranormal',\n",
      "          'goodness',\n",
      "          'merciful',\n",
      "          'responsible',\n",
      "          'amicable',\n",
      "          'hot',\n",
      "          'sure',\n",
      "          'strong',\n",
      "          'rich_people',\n",
      "          'fastidious',\n",
      "          'clean',\n",
      "          'considerate',\n",
      "          'undeviating',\n",
      "          'polite',\n",
      "          'correctly',\n",
      "          'savory',\n",
      "          'nonslippery',\n",
      "          'benignant',\n",
      "          'pleasant',\n",
      "          'reliable',\n",
      "          'gentle',\n",
      "          'decent',\n",
      "          'lively',\n",
      "          'humane',\n",
      "          'original',\n",
      "          'sound',\n",
      "          'honest',\n",
      "          'sincere',\n",
      "          'authentic',\n",
      "          'social',\n",
      "          'noble',\n",
      "          'comradely',\n",
      "          'true',\n",
      "          'unblock',\n",
      "          'synergistic',\n",
      "          'euphemistic',\n",
      "          'correct',\n",
      "          'ample',\n",
      "          'hospitable',\n",
      "          'soft',\n",
      "          'friendly',\n",
      "          'nonviolent',\n",
      "          'agreeable',\n",
      "          'living',\n",
      "          'heat',\n",
      "          'accessary',\n",
      "          'ingenuous',\n",
      "          'hearty',\n",
      "          'gracious',\n",
      "          'good-natured',\n",
      "          'conciliatory',\n",
      "          'unaggressive',\n",
      "          'conscious',\n",
      "          'defensive']}\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"Processed_vocab_dictionary.csv\", index=False)\n",
    "with open('Stereotype_Dictionary.json', 'w') as fp:\n",
    "    json.dump(word_dict, fp)\n",
    "\n",
    "pprint.pprint(dict(word_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4bb1f",
   "metadata": {},
   "source": [
    "# For GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69e07f",
   "metadata": {},
   "source": [
    "## Pre-post corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "7e4f536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "# result.sort()\n",
    "\n",
    "# result_dict = {}\n",
    "\n",
    "# result_dict[\"pre\"] = {}\n",
    "# result_dict[\"post\"] = {}\n",
    "\n",
    "# response_pre = []\n",
    "# response_deduped_pre = []\n",
    "\n",
    "# for r in result[:324]:\n",
    "#     print(r)\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_pre.append(word)\n",
    "                \n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_pre):\n",
    "#     if token not in response_deduped_pre:\n",
    "#         response_deduped_pre.append(token)\n",
    "            \n",
    "# result_dict[\"pre\"][\"corpus\"] = response_pre\n",
    "# result_dict[\"pre\"][\"length\"] = len(response_deduped_pre)\n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "24132af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"corpus_prepost.json\")\n",
    "# result_dict = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# response_post = []\n",
    "# response_deduped_post = []\n",
    "\n",
    "# for r in tqdm(result[324:]):\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_post.append(word)\n",
    "\n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_post):\n",
    "#     if token not in response_deduped_post:\n",
    "#         response_deduped_post.append(token)\n",
    "            \n",
    "# result_dict[\"post\"][\"corpus\"] = response_post\n",
    "# result_dict[\"post\"][\"length\"] = len(response_deduped_post) \n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65f185",
   "metadata": {},
   "source": [
    "## Daily full corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "b3ee7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "\n",
    "# result_dict = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict[key] = {}\n",
    "#     result_dict[key][\"corpus\"] = response\n",
    "#     result_dict[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_full.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be491ffb",
   "metadata": {},
   "source": [
    "## Daily Asian corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "ec448747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/Asian/*.csv')\n",
    "\n",
    "# result_dict_asian = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_asian[key] = {}\n",
    "#     result_dict_asian[key][\"corpus\"] = response\n",
    "#     result_dict_asian[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_asian, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f319dd",
   "metadata": {},
   "source": [
    "## Daily COVID corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "340a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/COVID/*.csv')\n",
    "\n",
    "# result_dict_covid = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_covid[key] = {}\n",
    "#     result_dict_covid[key][\"corpus\"] = response\n",
    "#     result_dict_covid[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_covid.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_covid, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46d12c",
   "metadata": {},
   "source": [
    "## Config dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "e3dad618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_directory = {}\n",
    "\n",
    "# config = {\"device\": \"cpu\",\n",
    "#           \"window_size\": 15,\n",
    "#           \"num_partitions\": 15,\n",
    "#           \"x_max\": 10,\n",
    "#           \"alpha\": 0.75,\n",
    "#           \"batch_size\": 32,\n",
    "#           \"num_epochs\": 10,\n",
    "#           \"embedding_size\": 50}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory = {}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_full.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory_asian = {}\n",
    "\n",
    "# for key in result_dict_asian.keys():\n",
    "#     config_directory_asian[key] = {}\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory_asian[key] = config\n",
    "\n",
    "# with open(\"config_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory_asian, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60fa0",
   "metadata": {},
   "source": [
    "# Stereotypes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "a739640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stereotypes_df = pd.read_csv(\"Kurdi et al./Kurdi, Mann, Charlesworth, & Banaji (2018) Vectors.csv\")\n",
    "# stereotypes_df = stereotypes_df.groupby('category')['word'].apply(list).to_dict()\n",
    "\n",
    "# stereotypes = {}\n",
    "# keywords = ['Cold', 'Warm', 'Competence', 'Incompetence']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         word_list = stereotypes_df[key]\n",
    "#         word_list = get_synonyms(word_list)\n",
    "        \n",
    "#         stereotypes[key] = []\n",
    "#         for word in word_list:\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n",
    "\n",
    "# stereotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "bbdb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stereotypes dict\n",
    "\n",
    "# stereotypes = {\"Cold\": ['cold', 'deceitful', 'dishonest', 'disloyal', 'hateful', 'hostile', 'mean', 'selfish', \n",
    "#                         'unfriendly', 'untrustworthy', 'vicious', 'unsociable', 'unprincipled', 'disagreeable', \n",
    "#                         'egoistic', 'egotistic', 'unkindly', 'unloving', 'inhumane', 'crooked', 'dishonorable', \n",
    "#                         'insincere', 'deceptive', 'thieving', 'corrupt', 'abominable', 'inhospitable', 'ignoble', \n",
    "#                         'stingy', 'contemptible', 'inconsiderate', 'self-serving', 'uncongenial', 'uncordial', \n",
    "#                         'unneighborly', 'devious', 'evil', 'condemnable', 'malicious', 'unsocial', 'antisocial', \n",
    "#                         'ungregarious', 'harsh', 'ill-natured', 'unkind'],\n",
    "               \n",
    "#                \"Warm\": ['warm', 'agreeable', 'dependable', 'reliable', 'friendly', 'good-natured', 'kind', 'nice', \n",
    "#                         'sincere', 'honest', 'supportive', 'trustworthy', 'helpful', 'sociable', 'guileless', \n",
    "#                         'enthusiastic', 'consistent', 'authentic', 'amicable', 'congenial', 'gracious', 'hospitable', \n",
    "#                         'affable', 'neighborly', 'pleasant', 'amiable', 'considerate', 'charitable', 'gentle', \n",
    "#                         'kindhearted', 'forgiving', 'good', 'respectable', 'decent', 'polite', 'courteous', 'genuine', \n",
    "#                         'earnest', 'honorable', 'unpretentious', 'truthful', 'encouraging', 'accommodating', \n",
    "#                         'cooperative', 'extroverted'],\n",
    "               \n",
    "#                \"Competent\": ['able', 'capable', 'competent', 'confident', 'efficient', 'intelligent', 'proficient', \n",
    "#                              'qualified', 'skilled', 'skillful', 'smart', 'motivated', 'persistent', 'resourceful', \n",
    "#                              'effective', 'self-assured', 'certain', 'businesslike', 'cost-efficient', 'expeditious', \n",
    "#                              'streamlined', 'precocious', 'agile', 'brainy', 'bright', 'quick', 'sophisticated', \n",
    "#                              'reasonable', 'rational', 'adept', 'technical', 'well-qualified', 'experienced', \n",
    "#                              'accomplished', 'delicate', 'sure-handed', 'versatile', 'precise', 'astute', 'streetwise', \n",
    "#                              'fastidious', 'driven', 'unforgettable', 'stubborn', 'dogged'],\n",
    "               \n",
    "#                \"Incompetent\": ['dumb', 'foolish', 'helpless', 'ignorant', 'incompetent', 'inefficient', 'inept', \n",
    "#                                'clumsy', 'uncertain', 'unintelligent', 'unqualified', 'unskilled', 'disorganized', \n",
    "#                                'stupid', 'dense', 'inarticulate', 'asinine', 'unwise', 'powerless', 'hopeless', \n",
    "#                                'dependent', 'uneducated', 'uninformed', 'feckless', 'ineffective', 'bungling', 'bad', \n",
    "#                                'inadequate', 'incapable', 'awkward', 'maladroit', 'gawky', 'unpredictable', 'unreliable', \n",
    "#                                'retarded', 'brainless', 'ineligible', 'quack', 'inexperienced', 'weak', 'unprofessional', \n",
    "#                                'amateurish', 'unsystematic', 'chaotic', 'unmethodical'],\n",
    "               \n",
    "#                \"Foreign\": ['foreign', 'alien', 'immigrant', 'extraneous', 'un-american', 'unpatriotic'],\n",
    "               \n",
    "#                \"Diseased\": ['diseased', 'dirty', 'poisonous', 'contagious', 'ill']}\n",
    "\n",
    "# keywords = ['Asians', 'Whites', 'Jews']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         if key not in stereotypes.keys():\n",
    "#             stereotypes[key] = []\n",
    "#         for word in stereotypes_df[key]:\n",
    "#             word = word.lower()\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n",
    "        \n",
    "# with open(\"stereotypes.json\", \"w\") as outfile:\n",
    "#     json.dump(stereotypes, outfile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "df387585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Maybe just group positive-negative - use textblob to get polarity\n",
    "# full = []\n",
    "\n",
    "# for stereotype in stereotypes.keys():\n",
    "#     for word in stereotypes[stereotype]:\n",
    "#         if word not in full:\n",
    "#             full.append(word)\n",
    "\n",
    "# positive = get_positive(full)\n",
    "# negative = get_negative(full)\n",
    "\n",
    "# revised['negative'] = negative\n",
    "# revised['positive'] = positive\n",
    "\n",
    "# # stereotype_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in revised.items() ]))\n",
    "# # stereotype_df.to_csv(\"stereotypes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151a4f9",
   "metadata": {},
   "source": [
    "# NYT keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "03c01d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"nyt.json\")\n",
    "# results = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# keywords_china = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', 'National Bureau of Statistics (China)', \n",
    "#             'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', 'Mao Zedong', \n",
    "#             'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', \n",
    "#             'Museum of Chinese in America', 'Far East, South and Southeast Asia and Pacific Areas', 'East Asia', \n",
    "#             'Central Asia']\n",
    "\n",
    "# keywords_asia = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Taiwan', 'Taipei (Taiwan)', 'Taoyuan (Taiwan)', 'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', \n",
    "#             'National Bureau of Statistics (China)', 'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Chinese Nationalist Party (Taiwan)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', \n",
    "#             'Mao Zedong', 'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', 'Museum of Chinese in America', \n",
    "#             'Asian-Americans', 'Indian-Americans', 'Vietnamese-Americans', 'Asian-Americans (TV Program)', 'Korean-Americans', \n",
    "#             'Bangladeshi-Americans', 'Far East, South and Southeast Asia and Pacific Areas', 'Southeast Asia', 'East Asia', \n",
    "#             'Central Asia', 'Japan', 'Nara (Japan)', 'Kanazawa (Japan)', 'South Korea', 'Seoul (South Korea)', \n",
    "#             'Jeju Island (South Korea)', 'Daegu (South Korea)', 'Thailand', 'Bangkok (Thailand)', 'Chiang Mai (Thailand)', \n",
    "#             'Lopburi (Thailand)', 'Phuket (Thailand)', 'Singapore', 'Indonesia', 'Bali (Indonesia)', 'Tomohon (Indonesia)', \n",
    "#             'Sulawesi (Indonesia)', 'Java (Indonesia)', 'Surabaya (Indonesia)', 'Maluku Islands (Indonesia)', 'Cambodia', \n",
    "#             'Sihanoukville (Cambodia)', 'Myanmar', 'Yangon (Myanmar)', 'Mandalay (Myanmar)', 'Philippines', 'Manila (Philippines)', \n",
    "#             'Vietnam', 'Ho Chi Minh City (Vietnam)', 'Cam Ranh Bay (Vietnam)', 'North Korea', 'Kaesong (North Korea)', \n",
    "#             'Laos', 'Malaysia', 'Macau', 'Mongolia', 'Nepal', 'Kathmandu (Nepal)', 'Sri Lanka', 'Bangladesh', 'Karachi (Pakistan)', \n",
    "#             'Bhutan','India', 'Bharatiya Janata Party', 'New Delhi (India)', 'Kerala (India)', 'Rajasthan (India)', \n",
    "#             'Uttar Pradesh State (India)', 'Delhi (India)', 'Mumbai (India)', 'Kashmir and Jammu (India)', 'Jaipur (India)', \n",
    "#             'Kashmir Valley (Kashmir and Jammu)',  'Odisha (India)', 'Karnataka (India)', 'Maharashtra (India)', \n",
    "#             'Bay of Bengal', 'Gujarat State (India)', 'Kolkata (India)', 'Bihar (India)', 'Srinagar (Jammu and Kashmir)',\n",
    "#             'AHMEDABAD (INDIA)', 'Himalayas', 'Ladakh (India)', 'Noida (India)', 'Darjeeling (India)', 'Serum Institute of India', \n",
    "#             'Punjab (India)', 'Andhra Pradesh (India)', 'Tamil Nadu (India)', 'Tripura (India)', 'Agartala (India)', \n",
    "#             'West Bengal (India)', 'Dharamsala (India)', 'Nashik (India)', 'Bhopal (India)', 'Goa (India)', 'Pune (India)', \n",
    "#             'Public Health Foundation of India']\n",
    "        \n",
    "# keywords_political = ['Trump, Donald J', 'Conservative Political Action Conference', 'Republican Party', \n",
    "#                       'Republican National Committee', 'Republican National Convention', 'Democratic Party', \n",
    "#                       'Democratic National Committee', 'Democratic National Convention']\n",
    "\n",
    "# nyt_df = {}\n",
    "# for result in results.keys():\n",
    "#     if result not in nyt_df.keys():\n",
    "#         nyt_df[result] = {}\n",
    "#         nyt_df[result]['total'] = len(results[result])\n",
    "#         chinese = 0\n",
    "#         asian = 0\n",
    "#         political = 0\n",
    "#         if len(results[result]) > 0:\n",
    "#             for article in results[result]:\n",
    "#                 chinese_article = 0\n",
    "#                 asian_article = 0\n",
    "#                 political_article = 0\n",
    "#                 for keyword in article['keywords']:\n",
    "#                     if keyword['value'] in keywords_china:\n",
    "#                         chinese_article += 1\n",
    "#                     if keyword['value'] in keywords_asia:\n",
    "#                         asian_article += 1\n",
    "#                     if keyword['value'] in keywords_political:\n",
    "#                         political_article += 1\n",
    "#                 if chinese_article > 0:\n",
    "#                     chinese += 1\n",
    "#                 if asian_article > 0:\n",
    "#                     asian += 1\n",
    "#                 if political_article > 0:\n",
    "#                     political += 1\n",
    "#         nyt_df[result]['chinese'] = chinese\n",
    "#         nyt_df[result]['asian'] = asian\n",
    "#         nyt_df[result]['political'] = political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "d3b4db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nyt = pd.DataFrame.from_dict({(i): nyt_df[i] \n",
    "#                                  for i in nyt_df.keys()},\n",
    "#                                 orient='index')\n",
    "# df_nyt.reset_index(inplace=True)\n",
    "# df_nyt = df_nyt.rename(columns = {'index':'date'})\n",
    "# df_nyt = df_nyt.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "\n",
    "# df_nyt.head()\n",
    "\n",
    "# df_nyt.to_csv(\"df_nyt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bee064",
   "metadata": {},
   "source": [
    "# For LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "8c4298af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all files in the BERT folder\n",
    "# results = glob('../raw/Full/*.csv')\n",
    "# results.sort()\n",
    "\n",
    "# f = open(\"corpus_full.json\")\n",
    "# corpus_full = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# full_raw = {}\n",
    "\n",
    "# for r in results:\n",
    "#     day = r[-14:-4]\n",
    "#     print(day)\n",
    "#     if day not in full_raw.keys():\n",
    "#         full_raw[day] = {}\n",
    "#     full_raw[day]['sentence'] = []\n",
    "#     full_raw[day]['user'] = []\n",
    "    \n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df = df[df.lemma_length > 0]\n",
    "    \n",
    "#     corpus = df.text.tolist()\n",
    "#     users = df.author_id.tolist()\n",
    "    \n",
    "#     for user, sentence in zip(users, corpus):\n",
    "#         sentence = preprocess(sentence)\n",
    "#         full_raw[day]['sentence'].append(sentence)\n",
    "#         full_raw[day]['user'].append(user)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "0ca2c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df = pd.DataFrame.from_dict({(i): full_raw[i]\n",
    "#                                       for i in full_raw.keys()}, \n",
    "#                                      orient=\"index\")\n",
    "\n",
    "# full_raw_df = full_raw_df.explode(['sentence', 'user'])\n",
    "# full_raw_df = full_raw_df.reset_index()\n",
    "# full_raw_df.columns = [\"day\", \"sentence\", \"user\"]\n",
    "\n",
    "# full_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "8609c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df.to_csv(\"LIWC_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58ca2",
   "metadata": {},
   "source": [
    "# After LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "a5b3f58d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LIWC_df_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1192], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LIWC_df_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLIWC_df_results.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m LIWC_df_results\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LIWC_df_results.csv'"
     ]
    }
   ],
   "source": [
    "LIWC_df_results = pd.read_csv(\"LIWC_df_results.csv\", lineterminator='\\n')\n",
    "\n",
    "LIWC_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a55426",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"^[0-9]+-[0-9]+-[0-9]+$\")\n",
    "index = [bool(pattern.match(string)) for string in LIWC_df_results.day.tolist()]\n",
    "\n",
    "set(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df_results = LIWC_df_results[index]\n",
    "LIWC_df_results.reset_index(drop=True)\n",
    "\n",
    "LIWC_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df_results.to_csv(\"LIWC_df_results_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
