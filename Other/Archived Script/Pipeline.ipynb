{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_10684/4272412708.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import os\n",
    "   \n",
    "import pandas as pd \n",
    "import spacy \n",
    "# spacy.cli.download(\"pt_core_news_sm\")\n",
    "# spacy.cli.download(\"es_core_news_sm\")\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#Sentence Tokenization using sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#Detect language using detect_langs\n",
    "import langdetect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "#Detect language using Lingua\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments yielded for the corpus (that are not urls or deleted): 161.\n",
      "Number of removed/deleted comments (has been filetered from corpus): 8.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "First, I'm gonna get the entire corpus from the \"Reddit Post Parsed\" folder.\n",
    "\"\"\"\n",
    "#replace as needed\n",
    "folder_name = \"Election Day 2020\"\n",
    "file_path = os.path.join(folder_name, \"log.csv\")\n",
    "\n",
    "all_post_titles = []\n",
    "expected_no_comments = 0\n",
    "corpus = \"\"\n",
    "comment_urls = []\n",
    "all_links = []\n",
    "og_posts = []\n",
    "comments_only = []\n",
    "total_comments_for_each_post = []\n",
    "\n",
    "with open(file_path, mode = 'r') as file:\n",
    "    link_column = []\n",
    "    title_column = []\n",
    "    comments_column = []\n",
    "    all_no_comments = []\n",
    "    total_comments_column =[]\n",
    "    csvFile = csv.reader(file)\n",
    "    for line in csvFile:\n",
    "\n",
    "        title_column.append(line[2])\n",
    "        all_post_titles = title_column[1:]\n",
    "\n",
    "        comments_column.append(line[9])\n",
    "        all_no_comments = comments_column[1:]\n",
    "\n",
    "        link_column.append(line[3])\n",
    "        all_links = link_column[1:]\n",
    "\n",
    "        total_comments_column.append(line[8])\n",
    "        total_comments_for_each_post = total_comments_column[1:]\n",
    "        \n",
    "    for number in all_no_comments:\n",
    "        expected_no_comments += int(number)\n",
    "\n",
    "@dataclass \n",
    "class Post:\n",
    "    post: str\n",
    "    likes: int\n",
    "    is_og: bool\n",
    "    id: int\n",
    "    length: int\n",
    "    sentiment: str\n",
    "    adj: str\n",
    "    adj_count: int\n",
    "    profanity_score: float\n",
    "    toxicity_label: str\n",
    "    toxicity_score: float\n",
    "    total_comments: int\n",
    "\n",
    "post_data = []\n",
    "\n",
    "#loop to open all post titles in create one big corpus of all comments\n",
    "def create_corpus(titles: list) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in a list of posts titles in the \n",
    "    folder \"Reddit Post Parsed\" and loops through each \n",
    "    csv file to filter for proper comments, that are not urls\n",
    "    and deleted to return the corpus.\n",
    "\n",
    "    Comments that are just links will be \n",
    "    appended to the list \"comment_urls\"!\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "    global comment_urls\n",
    "    global folder_name\n",
    "\n",
    "    count_proper_comments = 0\n",
    "    no_deleted_comments = 0\n",
    "    empty = \"\"\n",
    "    list_of_comments = []\n",
    "    id = 1\n",
    "    \n",
    "    base_folder = folder_name\n",
    "    i = 0\n",
    "    for title in titles:\n",
    "        count = 0\n",
    "        title_csv = os.path.join(base_folder, title + \"'s post.csv\")\n",
    "        if not os.path.isfile(title_csv):\n",
    "            print(f\"File '{title_csv}' not found.\")\n",
    "            continue\n",
    "        with open(title_csv, mode='r', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                likes = row[7]\n",
    "                post = empty.join(row[9:]).strip()\n",
    "                list_of_comments.append(post)\n",
    "                if count == 1:\n",
    "                    og_posts.append(post)\n",
    "                    post_data.append(Post(post, likes, True, id, len(post.split()), \"\", \"\", 0, 0,\"\", 0, total_comments_for_each_post[i]))\n",
    "                if count > 1:\n",
    "                    comments_only.append(post)\n",
    "                    post_data.append(Post(post, likes, False, id, len(post.split()), \"\", \"\", 0, 0,\"\", 0, 0))\n",
    "                count +=1\n",
    "        id += 1\n",
    "        i += 1\n",
    "\n",
    "    # print(list_of_comments)\n",
    "    # print(f'OG Posts: {og_posts}')\n",
    "\n",
    "    for comment in list_of_comments:\n",
    "        if comment.strip() != \"Body\":\n",
    "            if comment.strip() == '\"deleted\"' or comment.strip() == '\"removed\"':\n",
    "                no_deleted_comments +=1\n",
    "                comment = \"\"\n",
    "            if comment.strip().startswith('\"https:'):\n",
    "                comment_urls.append(comment.replace('\"', \"\").strip())\n",
    "            else:\n",
    "                count_proper_comments += 1 \n",
    "                corpus = corpus + \" \" + comment.replace(\"**\", \"\").replace(\"#\", \"\").strip()[1:-1] \n",
    "    print(f'Number of comments yielded for the corpus (that are not urls or deleted): {count_proper_comments}.') \n",
    "    print(f'Number of removed/deleted comments (has been filetered from corpus): {no_deleted_comments}.\\n')                  \n",
    "                \n",
    "create_corpus(all_post_titles)\n",
    "# print(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_10684/3832054853.py:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of total sentence tokens: 378.\n",
      "Amount of total word token: 3256.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class entities:\n",
    "    name: str\n",
    "    label: str\n",
    "\n",
    "#strip out the pronouns, conjunctions, etc.!\n",
    "# f = open('stop words.txt', 'r')\n",
    "# stopwords = f.read()\n",
    "# stopwords = stopwords.split('\\n')\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "# Load the spaCy English & Portuguese models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "pt_nlp = spacy.load('pt_core_news_sm')\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "#separate into tokenized sentences\n",
    "tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n",
    "sentences_token = tokenizer.tokenize(corpus)\n",
    "sentences = []\n",
    "for sentence in sentences_token:\n",
    "    if sentence.strip() not in stopwords:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "#separate corpus in words\n",
    "words_token = word_tokenize(corpus)\n",
    "words = []\n",
    "#remove any conjunctions, articles, particles, etc.\n",
    "for word in words_token:\n",
    "    if word.lower().strip() not in stopwords:\n",
    "        words.append(word)\n",
    "\n",
    "def checkW(x: int):\n",
    "    return (x/len(words))*100\n",
    "\n",
    "def checkS(x: int):\n",
    "    return (x/len(sentences))*100\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(f'Amount of total sentence tokens: {len(sentences)}.')\n",
    "print(f'Amount of total word token: {len(words)}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3: Lingua sentence by sentence\n",
    "\n",
    "#import English, Portuguese, Spanish detector\n",
    "languages = [Language.ENGLISH, Language.PORTUGUESE, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentences = []\n",
    "\n",
    "discarded_l = 0\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(sentence, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(sentence, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(sentence, Language.SPANISH)\n",
    "        if en_l > 0.8:\n",
    "            english_sentences.append(sentence)\n",
    "        elif pt_l > 0.8:\n",
    "            portuguese_sentences.append(sentence)\n",
    "        elif es_l > 0.8:\n",
    "            spanish_sentences.append(sentence)\n",
    "        else:\n",
    "            mixed_sentences.append(sentence)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discarded_l +=1\n",
    "        continue\n",
    "\n",
    "# print(\"3. Lingua sentence by sentence\")\n",
    "# print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "# print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "# print(f'Spanish sentences: {len(spanish_sentences)} - {checkS(len(spanish_sentences)):.2f}%.')\n",
    "# print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "# print(f'Discarded: {discarded_l} - {checkS(discarded_l):.2f}%.')\n",
    "# print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentences)+ len(mixed_sentences)):.2f}%.\\n')\n",
    "\n",
    "# METHOD 4: Lingua word by word\n",
    "\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "\n",
    "discard_w = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(word, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(word, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(word, Language.SPANISH)\n",
    "        if en_l > 0.5:\n",
    "            en_w.append(word)\n",
    "        elif pt_l > 0.5:\n",
    "            pt_w.append(word)\n",
    "        elif es_l > 0.5:\n",
    "            es_w.append(word)\n",
    "        else:\n",
    "            mixed_w.append(word)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discard_w +=1\n",
    "        continue\n",
    "\n",
    "# print(\"Lingua word by word\")\n",
    "# print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "# print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "# print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "# print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "# print(f'Discarded: {discard_w} - {checkW(discard_w):.2f}%.')\n",
    "# print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')\n",
    "\n",
    "# print(f'English sentences: {english_sentences}')\n",
    "# print(f'Portuguese sentences: {portuguese_sentences}')\n",
    "# print(f'Spanish sentences: {spanish_sentences}')\n",
    "# print(f'Mixed sentences: {mixed_sentences}')\n",
    "\n",
    "# print(f'English words: {en_w}')\n",
    "# print(f'Portuguese words: {pt_w}')\n",
    "# print(f'Spanish words: {es_w}')\n",
    "# print(f'Mixed words: {mixed_w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = english_sentences + mixed_sentences\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "\n",
    "def filter(words: list):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word.lower().strip() not in stopwords and len(word) != 1 and word.lower() not in result:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "english_words = filter(en_w)\n",
    "portuguese_words = filter(pt_w)\n",
    "spanish_words = filter(es_w)\n",
    "mixed_words = filter(mixed_w)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['covid19', 'Lindsay Graham', 'Biden', 'Trumps', 'Emmet Sullivan', 'John Kruzel', 'Bill Pascrell Jr.', 'Haul Louis DeJoy', 'Louis DeJoy', 'Jo Jorgensen', 'Facebook', 'Trump', 'Lyndon B. Johnson', 'Mark Twain', 'STEM degrees', 'Vaccine', 'Joe Biden', 'Beaus', 'Joe', 'Beau', 'Lindsey Graham', 'Parks', 'Rec', 'Fox', 'Covid', 'Don Jr', 'Hillary', 'Carter', 'Jimmy Carter', 'Bezos', 'Buffett', 'Susan Collins', 'LeBron James', 'Michael Bloomberg', 'Michael Bloomberg LeBron James', 'Lawrence Mower', 'Langston Taylor', 'Donald Trump', 'LeBron', 'Dixville Notch', 'Ilhan Omar', 'Fuck Facebook', '\\u200b.', '\\u200b. Change']\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class entities:\n",
    "    name: str\n",
    "    label: str\n",
    "\n",
    "# #strip out the pronouns, conjunctions, etc.!\n",
    "# f = open('stop words.txt', 'r')\n",
    "# stopwords = f.read()\n",
    "# stopwords = stopwords.split('\\n')\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "# Load the spaCy English & Portuguese models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "pt_nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "PeopleS = []\n",
    "PeopleW = []\n",
    "\n",
    "en_doc = en_nlp(\" \".join(total_sentences))\n",
    "pt_doc = pt_nlp(\" \".join(spanish_sentences) + \" \".join(portuguese_sentences))\n",
    "\n",
    "for ent in en_doc.ents:\n",
    "    # The output displayed the names of the entities and their predicted labels.\n",
    "    if (ent.text not in PeopleS) and ent.label_ == 'PERSON' and (not ent.text.startswith('http')):\n",
    "        # PeopleS.append(entities(ent.text, ent.label_))\n",
    "        PeopleS.append(ent.text)\n",
    "\n",
    "print(PeopleS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(re.escape(people) for people in PeopleS)\n",
    "\n",
    "# Use re.sub to remove the phrases from the corpus\n",
    "cleaned_corpus = re.sub(pattern, '', corpus)\n",
    "\n",
    "corpus = en_nlp(cleaned_corpus)\n",
    "noun_chunks = []\n",
    "for chunk in corpus.noun_chunks:\n",
    "    noun_chunks.append(chunk.text)\n",
    "\n",
    "\n",
    "unwanted_patterns = r'\\b(my|them|me|everyone|our|even|him|her|us|itself|people|a|an|the|he|she|it|i|you|we|they|his|her|hers|its|their|theirs|this|that|these|those|there|where|who|whom|which|what|when|why|how|am|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|shall|should|can|could|may|might|must|ought|and|but|or|nor|for|yet|so|because|as|if|once|since|unless|until|while|although|though|after|before|until|by|on|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|not|only|own|same|so|than|too|very|s|t|can|will|just|don|should|now)\\b|[.,!?;:[]()]'\n",
    "\n",
    "filtered_words = []\n",
    "for noun in noun_chunks:\n",
    "    filtered_phrase = ' '.join(word for word in re.split(r'\\s+', noun) if not re.fullmatch(unwanted_patterns, word, re.I))\n",
    "    if filtered_phrase:  # Ensure it's not empty\n",
    "        filtered_words.append(filtered_phrase)\n",
    "\n",
    "leftover = []\n",
    "for word in filtered_words:\n",
    "    en_word = en_nlp(word)\n",
    "    for ent in en_word.ents:\n",
    "        #Should I add in 'ORG' tags? 'Trump' is flagged as ORG, but so does 'Congress', 'Rulers', 'FAQ'\n",
    "        if (ent.text not in leftover) and (ent.label_ == 'PERSON' or ent.label_ == 'ORG') and (not ent.text.startswith('http')):\n",
    "            # print(ent.text + \" \" + ent.label_)\n",
    "            leftover.append(ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of influencers: ['covid19', 'Lindsay Graham', 'Biden', 'Trumps', 'Emmet Sullivan', 'John Kruzel', 'Bill Pascrell Jr.', 'Haul Louis DeJoy', 'Louis DeJoy', 'Jo Jorgensen', 'Facebook', 'Trump', 'Lyndon B. Johnson', 'Mark Twain', 'STEM degrees', 'Vaccine', 'Joe Biden', 'Beaus', 'Joe', 'Beau', 'Lindsey Graham', 'Parks', 'Rec', 'Fox', 'Covid', 'Don Jr', 'Hillary', 'Carter', 'Jimmy Carter', 'Bezos', 'Buffett', 'Susan Collins', 'LeBron James', 'Michael Bloomberg', 'Michael Bloomberg LeBron James', 'Lawrence Mower', 'Langston Taylor', 'Donald Trump', 'LeBron', 'Dixville Notch', 'Ilhan Omar', 'Fuck Facebook', '\\u200b.', '\\u200b. Change', 'I´m', 'WI MI', 'AZ', 'Associated Press', 'USPS', 'United States Postal Service', 'Columbia', 'ET', 'Hill', 'congress', 'PCM', 'sun', 'Liberal Pages', 'Fascist Book Election Night -', 'Shit', 'GOP', 'r/republican', 'Google', 'Amazon', 'Nah', 'devs', 'AOC', 'Senate', 'Nana', 'insult', 'Healthcare', 'Salt', 'NY', 'PS', 'BLM', 'free healthcare', 'Deutsche Bank', 'lesson', 'Clean America', 'Republican Party', 'Supreme Court', 'white supremacists', 'Complicit', 'Los Angeles Lakers', 'lingering court', 'Tampa Bay Times', 'Miami Herald', 'ProPublica', 'Sunshine State', 'Schitts Creek']\n"
     ]
    }
   ],
   "source": [
    "print(f'List of influencers: {PeopleS + leftover}')\n",
    "\n",
    "filtered_words = [word for word in filtered_words if word not in leftover]\n",
    "\n",
    "# Print the updated list\n",
    "# print(f'Final leftover words: {filtered_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Define a function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.softmax(logits, dim=1).detach().numpy()[0]\n",
    "\n",
    "    # Determine sentiment label\n",
    "    label_mapping = {\n",
    "        0: \"Very negative\",\n",
    "        1: \"Negative\",\n",
    "        2: \"Neutral\",\n",
    "        3: \"Positive\",\n",
    "        4: \"Very positive\"\n",
    "    }\n",
    "    sentiment_label = label_mapping[int(probs.argmax())]\n",
    "\n",
    "    return sentiment_label, probs\n",
    "\n",
    "negative_sentences = []\n",
    "positive_sentences = []\n",
    "\n",
    "for post in post_data:\n",
    "    # Perform sentiment analysis\n",
    "    text = post.post\n",
    "    sentiment, probabilities = analyze_sentiment(text)\n",
    "    if text.startswith('\"http') or text == '\"[deleted]\"' or text == '\"deleted\"':\n",
    "        post.sentiment = 'Undefined'\n",
    "    else:\n",
    "        post.sentiment = sentiment\n",
    "\n",
    "# non_url_posts = [post for post in post_data if not post.post.startswith('\"http')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(en_nlp.vocab)\n",
    "\n",
    "# patterns = [\n",
    "#     [{'POS':'ADJ'}, {'POS':'NOUN'}], [{'POS':'AUX'}, {'POS':'ADJ'}], [{'POS':'ADJ'}]\n",
    "#     ]\n",
    "patterns = [\n",
    "    [{'POS':'ADJ'}]\n",
    "    ]\n",
    "matcher.add(\"demo\", patterns)\n",
    "\n",
    "for post in post_data:\n",
    "    doc = en_nlp(post.post)\n",
    "    x = \"\"\n",
    "    count = 0\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = en_nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        if 'http' not in span.text and span.text not in x:\n",
    "            x = x + span.text + \", \"\n",
    "            count += 1\n",
    "        # print(x + \" \" + str(count))\n",
    "    # Remove the last comma and space\n",
    "    post.adj = x[:-2]  # Remove the last comma and space\n",
    "    post.adj_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for curse words\n",
    "from profanity_check import predict, predict_prob\n",
    "\n",
    "for post in post_data:\n",
    "    score = predict_prob([post.post])\n",
    "    for s in score:\n",
    "        post.profanity_score = s\n",
    "# print(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'non-toxic', 'score': 0.822258472442627}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (310 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#Check for toxicity(insults, hatespeech, etc.)\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline\n",
    "\n",
    "model_path = \"JungleLee/bert-toxic-comment-classification\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "print(pipeline(\"Whatever happens I love you all and the sun will come up tomorrow. Now lets see those memes!.\"))\n",
    "\n",
    "for post in post_data:\n",
    "    # Perform sentiment analysis\n",
    "    text = post.post\n",
    "    result = pipeline(text)\n",
    "    for r in result: \n",
    "        label, score = r['label'], r['score']\n",
    "        post.toxicity_score = score\n",
    "        post.toxicity_label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the CSV file\n",
    "csv_file_path = folder_name + \" Post-NLP.csv\"\n",
    "\n",
    "# Define the fieldnames for the CSV file\n",
    "fieldnames = [\"Post\",\"Total Comments\", \"Is OG\", \"ID\", \"Likes\", \"Length\", \"Sentiment\", \"Profanity Score\", \"Toxicity Label\", \"Toxicity Score\", \"ADJ Count\", \"ADJ\"]\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write each post as a row in the CSV file\n",
    "    for post in post_data:\n",
    "        writer.writerow({\n",
    "            \"Post\": post.post,\n",
    "            \"Total Comments\": post.total_comments,\n",
    "            \"Is OG\": post.is_og,\n",
    "            \"ID\": post.id,\n",
    "            \"Likes\": post.likes,\n",
    "            \"Length\": post.length,\n",
    "            \"Sentiment\": post.sentiment,\n",
    "            \"Profanity Score\": post.profanity_score,\n",
    "            \"Toxicity Label\": post.toxicity_label,\n",
    "            \"Toxicity Score\": post.toxicity_score,\n",
    "            \"ADJ Count\": post.adj_count,\n",
    "            \"ADJ\": post.adj\n",
    "        })\n",
    "\n",
    "print(\"CSV file has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nspan = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(span)\\nnegative_adj = []\\nfor token in doc:\\n    if token.pos_ == 'ADJ':\\n        print(token.text, token.pos_, token.dep_, token.head.text)\\n        negative_adj.append(token.text)\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "negative_adj = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "        print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "        negative_adj.append(token.text)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
