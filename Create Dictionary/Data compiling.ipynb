{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a621e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import contextlib\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from functools import total_ordering\n",
    "from itertools import chain, islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader import CorpusReader\n",
    "from nltk.internals import deprecated\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import binary_search_file as _binary_search_file\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8c39401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d8bca",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d6c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    new = []\n",
    "    # for text in word_list:\n",
    "    # new.append(word)\n",
    "    for syn in wordnet.synsets(word):\n",
    "        # Hypernyms\n",
    "        hypernyms = syn.hypernyms()\n",
    "        if len(hypernyms) > 0:\n",
    "            for hypernym in hypernyms:\n",
    "                if hypernym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = hypernym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Hyponyms\n",
    "        hyponyms = syn.hyponyms()\n",
    "        if len(hyponyms) > 0:\n",
    "            for hyponym in hyponyms:\n",
    "                if hyponym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = hyponym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Holonyms\n",
    "        member_holonyms = syn.member_holonyms()\n",
    "        if len(member_holonyms) > 0:\n",
    "            for holonym in member_holonyms:\n",
    "                if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = holonym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        substance_holonyms = syn.substance_holonyms()\n",
    "        if len(substance_holonyms) > 0:\n",
    "            for holonym in substance_holonyms:\n",
    "                if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = holonym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        part_holonyms = syn.part_holonyms()\n",
    "        if len(part_holonyms) > 0:\n",
    "            for holonym in part_holonyms:\n",
    "                if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = holonym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Meronyms\n",
    "        member_meronyms = syn.member_meronyms()\n",
    "        if len(member_meronyms) > 0:\n",
    "            for meronym in member_meronyms:\n",
    "                if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = meronym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        substance_meronyms = syn.substance_meronyms()\n",
    "        if len(substance_meronyms) > 0:\n",
    "            for meronym in substance_meronyms:\n",
    "                if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = meronym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        part_meronyms = syn.part_meronyms()\n",
    "        if len(part_meronyms) > 0:\n",
    "            for meronym in part_meronyms:\n",
    "                if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = meronym.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Also see\n",
    "        also_sees = syn.also_sees()\n",
    "        if len(also_sees) > 0:\n",
    "            for seealso in also_sees:\n",
    "                if seealso.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = seealso.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Similar to\n",
    "        similar_tos = syn.similar_tos()\n",
    "        if len(similar_tos) > 0:\n",
    "            for similar in similar_tos:\n",
    "                if similar.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = similar.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Attributes\n",
    "        attributes = syn.attributes()\n",
    "        if len(attributes) > 0:\n",
    "            for attribute in attributes:\n",
    "                if attribute.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                    word = attribute.name().split(\".\")[0]\n",
    "                    new.append(word)\n",
    "        # Synonyms\n",
    "        if syn.pos() in [\"a\", \"s\", \"r\"]:\n",
    "            word = syn.name().split(\".\")[0]\n",
    "            if word not in new:\n",
    "                new.append(word)\n",
    "            # Derivatives\n",
    "            lemmas = wordnet.lemmas(syn.name().split(\".\")[0], syn.name().split(\".\")[1])\n",
    "            if len(lemmas) > 0:\n",
    "                for lemma in lemmas:\n",
    "                    if lemma.syntactic_marker():\n",
    "                        new.append(lemma.name())\n",
    "                    else:\n",
    "                        pass\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb4933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Word</th>\n",
       "      <th>Extended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>highvalence_samearousal</td>\n",
       "      <td>create</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>highvalence</td>\n",
       "      <td>companionship</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lowvalence</td>\n",
       "      <td>deadly</td>\n",
       "      <td>[fatal, deadly, toxic, unpardonable, noxious, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>highvalence_samearousal</td>\n",
       "      <td>toy</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lowvalence_samearousal</td>\n",
       "      <td>slave</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category           Word  \\\n",
       "0  highvalence_samearousal         create   \n",
       "1              highvalence  companionship   \n",
       "2               lowvalence         deadly   \n",
       "3  highvalence_samearousal            toy   \n",
       "4   lowvalence_samearousal          slave   \n",
       "\n",
       "                                            Extended  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [fatal, deadly, toxic, unpardonable, noxious, ...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Raw_vocab_dictionary.csv\")\n",
    "\n",
    "#For testing\n",
    "# count = 0\n",
    "# for word in df[\"Word\"]:\n",
    "#     print(get_synonyms(word))\n",
    "#     count+=1\n",
    "#     if count ==50:\n",
    "#         break\n",
    "\n",
    "df[\"Synonyms\"] = df[\"Word\"].apply(get_synonyms)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ca9eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Processed_vocab_dictionary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4bb1f",
   "metadata": {},
   "source": [
    "# For GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69e07f",
   "metadata": {},
   "source": [
    "## Pre-post corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e4f536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "# result.sort()\n",
    "\n",
    "# result_dict = {}\n",
    "\n",
    "# result_dict[\"pre\"] = {}\n",
    "# result_dict[\"post\"] = {}\n",
    "\n",
    "# response_pre = []\n",
    "# response_deduped_pre = []\n",
    "\n",
    "# for r in result[:324]:\n",
    "#     print(r)\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_pre.append(word)\n",
    "                \n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_pre):\n",
    "#     if token not in response_deduped_pre:\n",
    "#         response_deduped_pre.append(token)\n",
    "            \n",
    "# result_dict[\"pre\"][\"corpus\"] = response_pre\n",
    "# result_dict[\"pre\"][\"length\"] = len(response_deduped_pre)\n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24132af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"corpus_prepost.json\")\n",
    "# result_dict = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# response_post = []\n",
    "# response_deduped_post = []\n",
    "\n",
    "# for r in tqdm(result[324:]):\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_post.append(word)\n",
    "\n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_post):\n",
    "#     if token not in response_deduped_post:\n",
    "#         response_deduped_post.append(token)\n",
    "            \n",
    "# result_dict[\"post\"][\"corpus\"] = response_post\n",
    "# result_dict[\"post\"][\"length\"] = len(response_deduped_post) \n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65f185",
   "metadata": {},
   "source": [
    "## Daily full corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3ee7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "\n",
    "# result_dict = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict[key] = {}\n",
    "#     result_dict[key][\"corpus\"] = response\n",
    "#     result_dict[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_full.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be491ffb",
   "metadata": {},
   "source": [
    "## Daily Asian corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec448747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/Asian/*.csv')\n",
    "\n",
    "# result_dict_asian = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_asian[key] = {}\n",
    "#     result_dict_asian[key][\"corpus\"] = response\n",
    "#     result_dict_asian[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_asian, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f319dd",
   "metadata": {},
   "source": [
    "## Daily COVID corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "340a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/COVID/*.csv')\n",
    "\n",
    "# result_dict_covid = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_covid[key] = {}\n",
    "#     result_dict_covid[key][\"corpus\"] = response\n",
    "#     result_dict_covid[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_covid.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_covid, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46d12c",
   "metadata": {},
   "source": [
    "## Config dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3dad618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_directory = {}\n",
    "\n",
    "# config = {\"device\": \"cpu\",\n",
    "#           \"window_size\": 15,\n",
    "#           \"num_partitions\": 15,\n",
    "#           \"x_max\": 10,\n",
    "#           \"alpha\": 0.75,\n",
    "#           \"batch_size\": 32,\n",
    "#           \"num_epochs\": 10,\n",
    "#           \"embedding_size\": 50}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory = {}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_full.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory_asian = {}\n",
    "\n",
    "# for key in result_dict_asian.keys():\n",
    "#     config_directory_asian[key] = {}\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory_asian[key] = config\n",
    "\n",
    "# with open(\"config_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory_asian, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60fa0",
   "metadata": {},
   "source": [
    "# Stereotypes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a739640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stereotypes_df = pd.read_csv(\"Kurdi et al./Kurdi, Mann, Charlesworth, & Banaji (2018) Vectors.csv\")\n",
    "# stereotypes_df = stereotypes_df.groupby('category')['word'].apply(list).to_dict()\n",
    "\n",
    "# stereotypes = {}\n",
    "# keywords = ['Cold', 'Warm', 'Competence', 'Incompetence']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         word_list = stereotypes_df[key]\n",
    "#         word_list = get_synonyms(word_list)\n",
    "        \n",
    "#         stereotypes[key] = []\n",
    "#         for word in word_list:\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n",
    "\n",
    "# stereotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbdb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stereotypes dict\n",
    "\n",
    "# stereotypes = {\"Cold\": ['cold', 'deceitful', 'dishonest', 'disloyal', 'hateful', 'hostile', 'mean', 'selfish', \n",
    "#                         'unfriendly', 'untrustworthy', 'vicious', 'unsociable', 'unprincipled', 'disagreeable', \n",
    "#                         'egoistic', 'egotistic', 'unkindly', 'unloving', 'inhumane', 'crooked', 'dishonorable', \n",
    "#                         'insincere', 'deceptive', 'thieving', 'corrupt', 'abominable', 'inhospitable', 'ignoble', \n",
    "#                         'stingy', 'contemptible', 'inconsiderate', 'self-serving', 'uncongenial', 'uncordial', \n",
    "#                         'unneighborly', 'devious', 'evil', 'condemnable', 'malicious', 'unsocial', 'antisocial', \n",
    "#                         'ungregarious', 'harsh', 'ill-natured', 'unkind'],\n",
    "               \n",
    "#                \"Warm\": ['warm', 'agreeable', 'dependable', 'reliable', 'friendly', 'good-natured', 'kind', 'nice', \n",
    "#                         'sincere', 'honest', 'supportive', 'trustworthy', 'helpful', 'sociable', 'guileless', \n",
    "#                         'enthusiastic', 'consistent', 'authentic', 'amicable', 'congenial', 'gracious', 'hospitable', \n",
    "#                         'affable', 'neighborly', 'pleasant', 'amiable', 'considerate', 'charitable', 'gentle', \n",
    "#                         'kindhearted', 'forgiving', 'good', 'respectable', 'decent', 'polite', 'courteous', 'genuine', \n",
    "#                         'earnest', 'honorable', 'unpretentious', 'truthful', 'encouraging', 'accommodating', \n",
    "#                         'cooperative', 'extroverted'],\n",
    "               \n",
    "#                \"Competent\": ['able', 'capable', 'competent', 'confident', 'efficient', 'intelligent', 'proficient', \n",
    "#                              'qualified', 'skilled', 'skillful', 'smart', 'motivated', 'persistent', 'resourceful', \n",
    "#                              'effective', 'self-assured', 'certain', 'businesslike', 'cost-efficient', 'expeditious', \n",
    "#                              'streamlined', 'precocious', 'agile', 'brainy', 'bright', 'quick', 'sophisticated', \n",
    "#                              'reasonable', 'rational', 'adept', 'technical', 'well-qualified', 'experienced', \n",
    "#                              'accomplished', 'delicate', 'sure-handed', 'versatile', 'precise', 'astute', 'streetwise', \n",
    "#                              'fastidious', 'driven', 'unforgettable', 'stubborn', 'dogged'],\n",
    "               \n",
    "#                \"Incompetent\": ['dumb', 'foolish', 'helpless', 'ignorant', 'incompetent', 'inefficient', 'inept', \n",
    "#                                'clumsy', 'uncertain', 'unintelligent', 'unqualified', 'unskilled', 'disorganized', \n",
    "#                                'stupid', 'dense', 'inarticulate', 'asinine', 'unwise', 'powerless', 'hopeless', \n",
    "#                                'dependent', 'uneducated', 'uninformed', 'feckless', 'ineffective', 'bungling', 'bad', \n",
    "#                                'inadequate', 'incapable', 'awkward', 'maladroit', 'gawky', 'unpredictable', 'unreliable', \n",
    "#                                'retarded', 'brainless', 'ineligible', 'quack', 'inexperienced', 'weak', 'unprofessional', \n",
    "#                                'amateurish', 'unsystematic', 'chaotic', 'unmethodical'],\n",
    "               \n",
    "#                \"Foreign\": ['foreign', 'alien', 'immigrant', 'extraneous', 'un-american', 'unpatriotic'],\n",
    "               \n",
    "#                \"Diseased\": ['diseased', 'dirty', 'poisonous', 'contagious', 'ill']}\n",
    "\n",
    "# keywords = ['Asians', 'Whites', 'Jews']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         if key not in stereotypes.keys():\n",
    "#             stereotypes[key] = []\n",
    "#         for word in stereotypes_df[key]:\n",
    "#             word = word.lower()\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n",
    "        \n",
    "# with open(\"stereotypes.json\", \"w\") as outfile:\n",
    "#     json.dump(stereotypes, outfile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df387585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Maybe just group positive-negative - use textblob to get polarity\n",
    "# full = []\n",
    "\n",
    "# for stereotype in stereotypes.keys():\n",
    "#     for word in stereotypes[stereotype]:\n",
    "#         if word not in full:\n",
    "#             full.append(word)\n",
    "\n",
    "# positive = get_positive(full)\n",
    "# negative = get_negative(full)\n",
    "\n",
    "# revised['negative'] = negative\n",
    "# revised['positive'] = positive\n",
    "\n",
    "# # stereotype_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in revised.items() ]))\n",
    "# # stereotype_df.to_csv(\"stereotypes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151a4f9",
   "metadata": {},
   "source": [
    "# NYT keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03c01d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"nyt.json\")\n",
    "# results = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# keywords_china = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', 'National Bureau of Statistics (China)', \n",
    "#             'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', 'Mao Zedong', \n",
    "#             'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', \n",
    "#             'Museum of Chinese in America', 'Far East, South and Southeast Asia and Pacific Areas', 'East Asia', \n",
    "#             'Central Asia']\n",
    "\n",
    "# keywords_asia = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Taiwan', 'Taipei (Taiwan)', 'Taoyuan (Taiwan)', 'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', \n",
    "#             'National Bureau of Statistics (China)', 'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Chinese Nationalist Party (Taiwan)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', \n",
    "#             'Mao Zedong', 'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', 'Museum of Chinese in America', \n",
    "#             'Asian-Americans', 'Indian-Americans', 'Vietnamese-Americans', 'Asian-Americans (TV Program)', 'Korean-Americans', \n",
    "#             'Bangladeshi-Americans', 'Far East, South and Southeast Asia and Pacific Areas', 'Southeast Asia', 'East Asia', \n",
    "#             'Central Asia', 'Japan', 'Nara (Japan)', 'Kanazawa (Japan)', 'South Korea', 'Seoul (South Korea)', \n",
    "#             'Jeju Island (South Korea)', 'Daegu (South Korea)', 'Thailand', 'Bangkok (Thailand)', 'Chiang Mai (Thailand)', \n",
    "#             'Lopburi (Thailand)', 'Phuket (Thailand)', 'Singapore', 'Indonesia', 'Bali (Indonesia)', 'Tomohon (Indonesia)', \n",
    "#             'Sulawesi (Indonesia)', 'Java (Indonesia)', 'Surabaya (Indonesia)', 'Maluku Islands (Indonesia)', 'Cambodia', \n",
    "#             'Sihanoukville (Cambodia)', 'Myanmar', 'Yangon (Myanmar)', 'Mandalay (Myanmar)', 'Philippines', 'Manila (Philippines)', \n",
    "#             'Vietnam', 'Ho Chi Minh City (Vietnam)', 'Cam Ranh Bay (Vietnam)', 'North Korea', 'Kaesong (North Korea)', \n",
    "#             'Laos', 'Malaysia', 'Macau', 'Mongolia', 'Nepal', 'Kathmandu (Nepal)', 'Sri Lanka', 'Bangladesh', 'Karachi (Pakistan)', \n",
    "#             'Bhutan','India', 'Bharatiya Janata Party', 'New Delhi (India)', 'Kerala (India)', 'Rajasthan (India)', \n",
    "#             'Uttar Pradesh State (India)', 'Delhi (India)', 'Mumbai (India)', 'Kashmir and Jammu (India)', 'Jaipur (India)', \n",
    "#             'Kashmir Valley (Kashmir and Jammu)',  'Odisha (India)', 'Karnataka (India)', 'Maharashtra (India)', \n",
    "#             'Bay of Bengal', 'Gujarat State (India)', 'Kolkata (India)', 'Bihar (India)', 'Srinagar (Jammu and Kashmir)',\n",
    "#             'AHMEDABAD (INDIA)', 'Himalayas', 'Ladakh (India)', 'Noida (India)', 'Darjeeling (India)', 'Serum Institute of India', \n",
    "#             'Punjab (India)', 'Andhra Pradesh (India)', 'Tamil Nadu (India)', 'Tripura (India)', 'Agartala (India)', \n",
    "#             'West Bengal (India)', 'Dharamsala (India)', 'Nashik (India)', 'Bhopal (India)', 'Goa (India)', 'Pune (India)', \n",
    "#             'Public Health Foundation of India']\n",
    "        \n",
    "# keywords_political = ['Trump, Donald J', 'Conservative Political Action Conference', 'Republican Party', \n",
    "#                       'Republican National Committee', 'Republican National Convention', 'Democratic Party', \n",
    "#                       'Democratic National Committee', 'Democratic National Convention']\n",
    "\n",
    "# nyt_df = {}\n",
    "# for result in results.keys():\n",
    "#     if result not in nyt_df.keys():\n",
    "#         nyt_df[result] = {}\n",
    "#         nyt_df[result]['total'] = len(results[result])\n",
    "#         chinese = 0\n",
    "#         asian = 0\n",
    "#         political = 0\n",
    "#         if len(results[result]) > 0:\n",
    "#             for article in results[result]:\n",
    "#                 chinese_article = 0\n",
    "#                 asian_article = 0\n",
    "#                 political_article = 0\n",
    "#                 for keyword in article['keywords']:\n",
    "#                     if keyword['value'] in keywords_china:\n",
    "#                         chinese_article += 1\n",
    "#                     if keyword['value'] in keywords_asia:\n",
    "#                         asian_article += 1\n",
    "#                     if keyword['value'] in keywords_political:\n",
    "#                         political_article += 1\n",
    "#                 if chinese_article > 0:\n",
    "#                     chinese += 1\n",
    "#                 if asian_article > 0:\n",
    "#                     asian += 1\n",
    "#                 if political_article > 0:\n",
    "#                     political += 1\n",
    "#         nyt_df[result]['chinese'] = chinese\n",
    "#         nyt_df[result]['asian'] = asian\n",
    "#         nyt_df[result]['political'] = political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3b4db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nyt = pd.DataFrame.from_dict({(i): nyt_df[i] \n",
    "#                                  for i in nyt_df.keys()},\n",
    "#                                 orient='index')\n",
    "# df_nyt.reset_index(inplace=True)\n",
    "# df_nyt = df_nyt.rename(columns = {'index':'date'})\n",
    "# df_nyt = df_nyt.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "\n",
    "# df_nyt.head()\n",
    "\n",
    "# df_nyt.to_csv(\"df_nyt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bee064",
   "metadata": {},
   "source": [
    "# For LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c4298af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all files in the BERT folder\n",
    "# results = glob('../raw/Full/*.csv')\n",
    "# results.sort()\n",
    "\n",
    "# f = open(\"corpus_full.json\")\n",
    "# corpus_full = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# full_raw = {}\n",
    "\n",
    "# for r in results:\n",
    "#     day = r[-14:-4]\n",
    "#     print(day)\n",
    "#     if day not in full_raw.keys():\n",
    "#         full_raw[day] = {}\n",
    "#     full_raw[day]['sentence'] = []\n",
    "#     full_raw[day]['user'] = []\n",
    "    \n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df = df[df.lemma_length > 0]\n",
    "    \n",
    "#     corpus = df.text.tolist()\n",
    "#     users = df.author_id.tolist()\n",
    "    \n",
    "#     for user, sentence in zip(users, corpus):\n",
    "#         sentence = preprocess(sentence)\n",
    "#         full_raw[day]['sentence'].append(sentence)\n",
    "#         full_raw[day]['user'].append(user)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ca2c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df = pd.DataFrame.from_dict({(i): full_raw[i]\n",
    "#                                       for i in full_raw.keys()}, \n",
    "#                                      orient=\"index\")\n",
    "\n",
    "# full_raw_df = full_raw_df.explode(['sentence', 'user'])\n",
    "# full_raw_df = full_raw_df.reset_index()\n",
    "# full_raw_df.columns = [\"day\", \"sentence\", \"user\"]\n",
    "\n",
    "# full_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8609c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df.to_csv(\"LIWC_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58ca2",
   "metadata": {},
   "source": [
    "# After LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5b3f58d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LIWC_df_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LIWC_df_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLIWC_df_results.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m LIWC_df_results\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LIWC_df_results.csv'"
     ]
    }
   ],
   "source": [
    "LIWC_df_results = pd.read_csv(\"LIWC_df_results.csv\", lineterminator='\\n')\n",
    "\n",
    "LIWC_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a55426",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"^[0-9]+-[0-9]+-[0-9]+$\")\n",
    "index = [bool(pattern.match(string)) for string in LIWC_df_results.day.tolist()]\n",
    "\n",
    "set(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df_results = LIWC_df_results[index]\n",
    "LIWC_df_results.reset_index(drop=True)\n",
    "\n",
    "LIWC_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df_results.to_csv(\"LIWC_df_results_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
