{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \\nDictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\\n\\nOnce we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: string):\n",
    "    return ' '.join([item for item in text.split() if item not in stopwords.words('english')])\n",
    "\n",
    "def remove_punctuations(text: string):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "def stem_text(text: string):\n",
    "    return [ps.stem(word) for word in text]\n",
    "\n",
    "def delete_usernames(text: str) -> str:\n",
    "    pattern = r'@\\w+'\n",
    "    return re.sub(pattern, '@user', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_corpus_list = []\n",
    "after_corpus_list = []\n",
    "\n",
    "tweets = {}\n",
    "\n",
    "with open(\"Pre-processing.csv\", 'r', newline='') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    next(reader) #skip header\n",
    "    for line in reader:\n",
    "        person = line[0].strip()\n",
    "        tweets[person] = {}\n",
    "\n",
    "        b_tweet = delete_usernames(remove_punctuations(remove_stop_words(line[3].strip())))\n",
    "        a_tweet = delete_usernames(remove_punctuations(remove_stop_words(line[4].strip())))\n",
    "\n",
    "        before_corpus_list.append(b_tweet)\n",
    "        after_corpus_list.append(a_tweet)\n",
    "        tweets[person][\"Before\"] = b_tweet.lower()\n",
    "        tweets[person][\"After\"] = a_tweet.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RealSpikeCohen:\n",
      "  Before:\n",
      "    just: [-0.6662592   0.01738216  0.49830064  0.04646187  0.696845  ]...\n",
      "    month: [-0.64293134 -0.33376592  0.37095174 -0.09896091  0.30240285]...\n",
      "    ...\n",
      "  After:\n",
      "    hamas: [-0.58247954  0.08837597  0.62763894 -0.31676415  0.17383316]...\n",
      "    great: [ 0.28736863  0.07461813  0.68630224 -0.4492811  -0.11793958]...\n",
      "    ...\n",
      "\n",
      "ComicDaveSmith:\n",
      "  Before:\n",
      "    if: [-0.00075689  0.15637913  0.17640428 -0.15965573  0.06332569]...\n",
      "    mission: [ 0.5582286  -0.18407351  0.16965123  0.10473238  0.34409454]...\n",
      "    ...\n",
      "  After:\n",
      "    as: [-0.5240849   0.34253213  1.6846156   0.37956646  0.01245819]...\n",
      "    israeli: [-0.27700266  0.26969013  0.09555568 -0.62538004  0.31419805]...\n",
      "    ...\n",
      "\n",
      "Words without output vectors:\n",
      "['2005', '250', '7', 'according', 'actually', 'again', 'asinineisrael', 'assad', 'awful', 'benny', 'brutal', 'calling', 'centerleft', 'chemical', 'chief', 'clearly', 'comparison', 'convincing', 'democraticallyelected', 'determine', 'dictator', 'did', 'different', 'dirty', 'doortodoor', 'eastern', 'efforts', 'elected', 'elections', 'endangered', 'entirely', 'everyone', 'expert', 'fault', 'favor', 'fing', 'gantz', 'gives', 'going', 'government', 'hand', 'heartbreaking', 'hes', 'hold', 'humanitarian', 'idf', 'includes', 'international', 'law', 'leader', 'learn', 'mess', 'middle', 'minority', 'minute', 'moderate', 'moronsonce', 'nearly', 'netanyahu', 'now', 'obituary', 'october', 'oh', 'prevent', 'protect', 'really', 'religious', 'rival', 'ruthless', 'savage', 'soldiers', 'syria', 'territory', 'that', 'them', 'then', 'there', 'undermines', 'unequivocally', 'unity', 'used', 'violates', 'voice', 'wait', 'wapo', 'well', 'with', 'worknetanyahus', 'written']\n",
      "Total words without vectors: 89\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_word_embeddings(sentence):\n",
    "    # Tokenize the sentence\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    # Get word-level embeddings\n",
    "    token_embeddings = output.last_hidden_state[0].numpy()\n",
    "    \n",
    "    # Map tokens back to original words\n",
    "    word_vectors = {}\n",
    "    words_without_vectors = set()\n",
    "    words = sentence.split()\n",
    "    token_ids = encoded_input['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    word_index = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('##'):\n",
    "            if word_index > 0:\n",
    "                word_vectors[words[word_index-1]] = \\\n",
    "                    (word_vectors[words[word_index-1]] * (i-start) + token_embeddings[i]) / (i-start+1)\n",
    "        elif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        else:\n",
    "            if word_index < len(words):\n",
    "                word_vectors[words[word_index]] = token_embeddings[i]\n",
    "                start = i\n",
    "                word_index += 1\n",
    "\n",
    "    words_without_vectors = set(words) - set(word_vectors.keys())\n",
    "    return word_vectors, words_without_vectors\n",
    "\n",
    "# Process tweets and generate word vectors\n",
    "result = defaultdict(dict)\n",
    "words_without_vectors_global = set()\n",
    "\n",
    "for person, person_tweets in tweets.items():\n",
    "    for tweet_type in [\"Before\", \"After\"]:\n",
    "        result[person][tweet_type], words_without_vectors = get_word_embeddings(person_tweets[tweet_type])\n",
    "        words_without_vectors_global.update(words_without_vectors)\n",
    "\n",
    "# Print a sample of the result\n",
    "for person in list(result.keys())[:2]:  # Print for first 2 persons\n",
    "    print(f\"\\n{person}:\")\n",
    "    for tweet_type in [\"Before\", \"After\"]:\n",
    "        print(f\"  {tweet_type}:\")\n",
    "        for word, vector in list(result[person][tweet_type].items())[:2]:  # Print first 2 words\n",
    "            print(f\"    {word}: {vector[:5]}...\")  # Print first 5 values of each vector\n",
    "        if len(result[person][tweet_type]) > 2:\n",
    "            print(\"    ...\")\n",
    "\n",
    "print(\"\\nWords without output vectors:\")\n",
    "print(sorted(list(words_without_vectors_global)))\n",
    "print(f\"Total words without vectors: {len(words_without_vectors_global)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "dumped = json.dumps(result, cls=NumpyEncoder)\n",
    "\n",
    "with open('Tokens.json', 'w') as fp:\n",
    "    json.dump(dumped, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (features):\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0261,  0.2147,  0.1159,  ...,  0.0314,  0.0336, -0.1419],\n",
      "         [ 0.0986, -0.0205,  0.2265,  ...,  0.0828, -0.2993,  0.4767],\n",
      "         [-0.1011,  0.2133, -0.2283,  ...,  0.0797, -0.1762, -0.2632],\n",
      "         ...,\n",
      "         [-0.3448, -0.2996, -0.2430,  ..., -0.3354,  0.2429,  0.3029],\n",
      "         [-0.5279, -0.2429,  0.0758,  ..., -0.1733,  0.0389,  0.0513],\n",
      "         [-0.0425,  0.2355,  0.1219,  ...,  0.0018,  0.0732, -0.1305]]]), pooler_output=tensor([[ 2.1684e-01, -1.6747e-01, -3.9913e-02, -1.6048e-01,  1.2537e-01,\n",
      "         -7.5435e-02,  2.1886e-01, -1.3510e-01,  2.0169e-01, -2.1097e-01,\n",
      "         -9.4783e-02, -5.3385e-02, -2.2015e-01,  1.3985e-02,  1.5082e-01,\n",
      "         -6.2459e-02, -1.4716e-01, -6.7477e-02,  2.4149e-02,  1.7152e-01,\n",
      "         -1.1483e-01, -1.7264e-01,  2.9670e-01, -1.6612e-02, -2.3634e-02,\n",
      "          3.5212e-02, -1.6087e-01, -3.7130e-02,  4.9394e-02, -5.8116e-02,\n",
      "         -5.6990e-02, -1.5559e-01,  1.6120e-01,  5.6175e-02,  1.7266e-01,\n",
      "         -5.9472e-02,  2.1309e-01,  1.2466e-01, -1.2592e-01,  9.7800e-02,\n",
      "         -1.5980e-01, -4.7554e-02,  1.1069e-01,  2.2541e-02, -7.0859e-02,\n",
      "          3.4330e-02, -1.3061e-01,  3.6223e-02,  3.8687e-01, -4.9398e-02,\n",
      "         -6.2531e-03,  1.1077e-01, -6.5434e-02, -1.0200e-02, -5.4485e-03,\n",
      "          1.2476e-01, -5.6953e-03,  6.2194e-02, -2.2040e-01,  2.3035e-01,\n",
      "          4.0335e-02,  4.2737e-01, -2.5728e-01, -4.2542e-01, -1.2839e-02,\n",
      "         -1.3663e-01, -4.3934e-03,  1.1903e-01,  4.6278e-01,  2.3251e-01,\n",
      "          2.7170e-01,  1.0339e-01, -2.4488e-01, -1.2347e-02, -9.7118e-02,\n",
      "         -1.6681e-01,  6.5080e-02, -2.9772e-02,  1.3511e-01,  2.4834e-02,\n",
      "         -2.9946e-02, -3.5176e-01,  2.3219e-01, -9.5714e-02, -2.5918e-01,\n",
      "          2.1088e-01,  1.0469e-01,  6.2564e-02, -6.9195e-02,  2.1743e-01,\n",
      "         -1.8241e-01, -7.7701e-02, -1.3959e-01, -2.1408e-01, -1.0395e-01,\n",
      "         -2.8095e-02,  2.9912e-04,  2.3257e-01, -1.2731e-01, -1.7985e-01,\n",
      "         -1.8892e-01,  8.1731e-02, -3.7741e-03,  6.8211e-03,  4.1665e-03,\n",
      "         -8.9084e-02, -3.5342e-02,  2.2507e-01,  2.8873e-02, -7.7968e-02,\n",
      "         -2.0762e-02, -1.6699e-01, -2.2962e-01, -1.6147e-01,  1.9951e-01,\n",
      "         -2.0291e-02, -3.0334e-01,  2.8734e-01,  2.3261e-01,  7.0353e-02,\n",
      "         -3.4001e-02,  2.0040e-01,  1.5577e-02,  3.0432e-01,  5.7032e-02,\n",
      "         -3.4262e-02,  6.6559e-02, -6.2957e-02,  1.2308e-01, -2.3498e-01,\n",
      "          5.8435e-02, -1.7374e-02,  1.5279e-01, -1.6965e-01,  1.1444e-01,\n",
      "         -1.9653e-02,  1.4226e-01,  8.3226e-02,  1.7470e-01,  1.6492e-01,\n",
      "          2.9032e-03, -2.9698e-01, -1.7966e-02,  3.3161e-01,  2.6762e-03,\n",
      "          3.0237e-01, -1.2641e-01,  1.4140e-01, -9.1238e-02, -4.3958e-02,\n",
      "         -2.0410e-03, -1.0386e-01, -1.9405e-01,  1.6783e-01, -4.5707e-02,\n",
      "         -8.6510e-02,  4.4068e-04,  6.7585e-02,  5.1671e-02, -9.1920e-02,\n",
      "          2.1051e-01, -2.8929e-01,  1.1322e-02, -1.5257e-01,  7.3889e-02,\n",
      "         -7.6998e-02, -1.5864e-01, -5.7711e-02, -1.4706e-01, -2.0447e-02,\n",
      "         -1.4777e-01, -1.7777e-01, -6.2066e-01, -2.8002e-01,  2.0328e-01,\n",
      "          1.6720e-01, -1.3829e-01,  9.3785e-02,  1.9930e-01,  6.8548e-02,\n",
      "         -8.3031e-03,  1.0518e-01, -2.2478e-01,  3.6735e-02, -1.4841e-01,\n",
      "          2.2288e-01, -9.9642e-02, -9.6726e-02,  6.0620e-02,  3.3674e-02,\n",
      "          7.1847e-02, -1.7697e-02,  2.5305e-01,  1.8194e-01, -3.8007e-01,\n",
      "         -1.7328e-01,  1.5387e-01, -1.9752e-01,  5.8195e-02,  1.4141e-01,\n",
      "          1.7599e-01,  9.9589e-02, -1.7539e-01,  3.9723e-02,  5.7362e-02,\n",
      "         -6.0153e-03, -7.4779e-02, -3.9814e-02,  4.2529e-03, -6.1435e-03,\n",
      "         -2.7550e-01, -1.7463e-01, -1.0434e-01, -2.8234e-01, -1.1606e-01,\n",
      "          2.2520e-01, -2.6030e-01, -5.6627e-02, -4.5074e-01,  2.0554e-01,\n",
      "          2.4415e-01, -2.4389e-01,  1.4430e-01,  2.7835e-02, -8.4410e-02,\n",
      "         -5.6010e-04,  4.9588e-01,  7.3265e-02, -1.3965e-01, -3.0254e-01,\n",
      "         -3.4468e-02, -8.2589e-02,  1.2788e-01, -1.6157e-02,  1.7172e-01,\n",
      "          6.1288e-01,  1.5454e-01,  9.4415e-02, -1.1867e-01, -2.8975e-02,\n",
      "         -2.2069e-01, -1.8120e-01,  9.5515e-02, -1.8660e-01,  3.1271e-02,\n",
      "          3.6478e-02,  4.3989e-01,  5.9990e-02, -2.1676e-01,  9.2675e-02,\n",
      "         -1.1439e-01, -2.4943e-01, -2.9657e-01,  1.3715e-01, -5.5204e-02,\n",
      "          8.1869e-02, -2.5677e-03,  4.9730e-03,  2.9959e-01,  1.8850e-01,\n",
      "          9.5239e-02,  1.7245e-01, -2.8302e-01, -1.0291e-01, -1.3128e-01,\n",
      "         -7.4131e-02,  6.4491e-02, -2.6746e-01, -7.7197e-02, -1.6304e-01,\n",
      "         -2.2387e-01, -1.2678e-01,  6.6663e-02, -2.5838e-02, -8.8316e-02,\n",
      "         -2.3827e-01, -6.9970e-03,  1.8078e-01, -6.2181e-02, -3.8105e-01,\n",
      "          1.2478e-02,  2.3644e-01, -2.2813e-01, -6.5327e-02, -4.2092e-02,\n",
      "         -4.9957e-02, -1.6963e-02,  2.8927e-01,  1.2748e-01, -1.6388e-02,\n",
      "         -1.4192e-01, -1.0516e-01, -7.7111e-03,  3.0210e-01,  7.8765e-02,\n",
      "         -2.9639e-01,  1.6735e-01,  1.0467e-02,  2.1078e-01,  7.6494e-02,\n",
      "         -1.1333e-01, -1.7217e-01,  1.1372e-01, -2.2313e-01, -1.2148e-01,\n",
      "         -1.9967e-01, -1.0848e-01, -1.8270e-03,  1.9473e-01,  1.9243e-01,\n",
      "          1.7690e-01, -2.0757e-02,  2.1948e-01,  2.6942e-01, -6.2993e-03,\n",
      "          1.4327e-01,  1.1615e-01,  2.0209e-01,  3.8537e-02, -4.6930e-02,\n",
      "         -7.8352e-02, -1.7991e-01, -1.6937e-01, -4.8627e-02,  2.6268e-02,\n",
      "          1.7385e-02, -1.7453e-01,  2.6845e-02, -5.6886e-02,  2.7073e-01,\n",
      "          2.8761e-01, -1.0229e-01,  1.4652e-01, -8.4829e-02, -1.5249e-01,\n",
      "         -1.6665e-01,  1.6805e-01,  7.7209e-02,  5.8677e-02,  4.0058e-02,\n",
      "         -1.5571e-01, -1.2718e-01,  3.1281e-01, -9.1538e-03,  1.3576e-01,\n",
      "          3.9473e-01,  2.4109e-02, -3.8084e-02, -1.4643e-01,  2.8824e-02,\n",
      "         -3.2364e-02, -1.4315e-01,  3.7451e-02, -4.4503e-02,  1.5163e-01,\n",
      "         -2.8780e-01,  1.6395e-01,  1.3348e-01, -2.2630e-01,  7.0919e-02,\n",
      "         -2.1953e-01,  2.5654e-02, -8.2631e-02, -1.2159e-01, -1.5556e-01,\n",
      "          7.7520e-02, -1.2948e-01, -2.3807e-01,  2.7228e-01, -4.2869e-02,\n",
      "         -7.8907e-03,  8.9008e-02,  3.2701e-01, -1.7110e-01, -1.3331e-01,\n",
      "          2.4334e-02,  2.5773e-02, -3.7812e-01, -1.7119e-01, -7.4227e-02,\n",
      "          1.4139e-01, -1.0524e-01, -2.4698e-01,  1.0323e-01, -1.0664e-01,\n",
      "          9.5311e-02, -3.6480e-01,  1.9375e-01,  2.4034e-01, -1.3027e-01,\n",
      "          2.6346e-01, -2.1573e-02,  7.0739e-03, -4.5205e-01,  1.0983e-01,\n",
      "          7.4580e-02,  1.9370e-01,  5.6970e-02,  3.0360e-01, -1.0367e-01,\n",
      "         -6.9653e-02, -2.0863e-01,  1.1197e-01, -8.3405e-02, -4.3823e-02,\n",
      "         -1.0581e-01, -2.6409e-03,  1.2921e-01,  7.2932e-02, -8.2186e-03,\n",
      "         -1.5625e-01,  2.5517e-01, -3.2143e-01, -1.0550e-01, -8.6524e-02,\n",
      "          1.3654e-01,  2.1519e-01,  3.3233e-02,  1.1481e-01, -2.7192e-01,\n",
      "          1.5108e-01, -1.2987e-01,  5.7868e-02,  4.9990e-02, -5.4458e-02,\n",
      "          2.7341e-02, -1.0211e-01, -7.1317e-02, -1.0423e-01, -4.7862e-02,\n",
      "         -1.6482e-02, -1.6296e-01, -1.1742e-01, -1.3969e-02, -9.0473e-02,\n",
      "          3.6633e-02,  5.2472e-02,  1.0371e-01,  5.5842e-02, -1.8505e-01,\n",
      "         -1.7271e-01, -3.4367e-01,  2.3774e-01, -2.4568e-01, -6.8554e-02,\n",
      "         -1.1043e-01, -2.1823e-01, -3.3877e-01, -1.8825e-01, -1.3074e-01,\n",
      "          2.0080e-01, -4.3499e-02, -1.1832e-03, -1.0494e-02,  9.8839e-02,\n",
      "         -9.8611e-02,  1.0916e-01, -1.2520e-01,  2.2091e-01,  1.8034e-01,\n",
      "          1.3691e-01,  5.8596e-02,  9.1621e-02,  2.5686e-01, -1.6616e-01,\n",
      "         -2.1491e-01,  1.5801e-01, -6.9085e-02, -1.1327e-01,  2.2038e-01,\n",
      "          4.0990e-02, -1.2667e-01, -2.1626e-01,  9.1708e-02, -2.1187e-01,\n",
      "          3.8481e-02,  1.3006e-02,  2.8096e-01,  5.8074e-02,  6.4363e-02,\n",
      "         -1.9326e-01,  5.0339e-02, -1.6850e-01,  2.5039e-01, -1.2964e-01,\n",
      "         -4.5641e-02, -1.9423e-01, -2.0506e-01,  2.7561e-01, -2.1644e-01,\n",
      "          1.3510e-01, -1.5607e-01, -1.9618e-01,  3.6909e-01,  1.6662e-02,\n",
      "         -2.7980e-01,  1.6374e-01,  9.8739e-02, -4.2288e-02, -5.9338e-03,\n",
      "         -2.3921e-01, -9.5845e-02,  7.9162e-02,  4.8087e-02, -1.5758e-01,\n",
      "         -3.2436e-01, -2.1874e-01,  9.2944e-02, -1.7660e-01,  3.2471e-01,\n",
      "          4.1640e-02, -8.4214e-02, -1.1156e-01,  1.7573e-01, -1.4107e-01,\n",
      "          9.6379e-02, -8.0719e-02, -2.1352e-01,  1.9738e-01, -7.8109e-04,\n",
      "         -1.4115e-01, -1.4976e-01, -1.0283e-01,  2.7926e-01,  2.2314e-01,\n",
      "          2.6503e-01, -2.9287e-01,  5.3023e-03, -1.6676e-01, -1.7062e-01,\n",
      "          1.6481e-01, -6.6618e-02,  1.3794e-01,  1.2838e-01,  1.7721e-01,\n",
      "         -3.6212e-01,  1.9790e-03, -4.0427e-02, -1.4486e-01, -9.7480e-02,\n",
      "         -1.8925e-01,  4.2498e-01, -5.7728e-02, -4.3534e-02,  1.7206e-02,\n",
      "          1.3141e-01, -2.7031e-02, -1.0911e-01,  3.1601e-01,  5.3196e-02,\n",
      "          2.1432e-01, -1.2668e-01,  8.3884e-02, -3.1696e-01, -1.4914e-01,\n",
      "         -3.0770e-01,  5.2150e-01, -1.0679e-01,  1.0872e-01, -3.0105e-02,\n",
      "         -1.8292e-01,  2.4955e-01,  4.8210e-02,  6.5361e-02, -8.1284e-02,\n",
      "         -7.3967e-02,  2.1774e-01, -2.5905e-01, -9.9748e-02,  2.2861e-01,\n",
      "          2.0665e-01, -6.5433e-02, -1.4660e-01, -2.7731e-02, -3.7495e-01,\n",
      "         -3.3564e-01,  2.2604e-01,  1.2786e-01, -1.1509e-01, -3.2010e-01,\n",
      "          5.0551e-02, -5.2174e-02, -6.8956e-02, -3.2839e-01, -1.4177e-01,\n",
      "          1.2305e-01,  5.3620e-02, -2.2995e-01, -1.0491e-01, -7.8523e-02,\n",
      "         -5.8274e-02, -3.0538e-01,  2.1489e-01,  1.6900e-01,  1.0598e-01,\n",
      "          1.4899e-03, -8.4836e-02, -2.2781e-01,  6.4068e-02,  6.7883e-02,\n",
      "          5.4033e-01,  7.4442e-02,  2.1099e-01, -2.4285e-01, -9.5264e-02,\n",
      "         -7.1960e-02,  7.2113e-02,  1.9963e-01,  7.2251e-02, -1.8008e-01,\n",
      "          1.1052e-02,  7.1147e-03,  1.3077e-01, -1.1621e-01,  1.3478e-01,\n",
      "          1.8277e-01, -1.2061e-01,  2.5727e-01, -4.8500e-02, -7.5047e-02,\n",
      "         -1.1551e-01,  5.9584e-02,  1.1659e-01, -2.9556e-01,  3.4345e-02,\n",
      "          2.3334e-01, -3.8725e-02, -4.8951e-02,  1.2001e-01, -1.2271e-01,\n",
      "          1.2176e-01,  1.7248e-01,  1.7510e-01, -1.0586e-01,  1.4408e-01,\n",
      "          4.6973e-02, -1.9982e-01, -3.1675e-01,  1.8192e-01, -4.9504e-02,\n",
      "         -1.1049e-01,  5.1719e-02, -3.8881e-01, -3.7701e-01, -2.3664e-01,\n",
      "          6.1054e-02, -1.6371e-01, -1.8980e-02, -1.0349e-02, -1.8236e-02,\n",
      "         -3.1771e-01,  1.7057e-02, -6.7177e-02,  3.0066e-02,  1.2955e-01,\n",
      "          2.5586e-02, -1.3062e-02,  1.5314e-01,  1.2715e-01,  2.5056e-01,\n",
      "         -2.6158e-01, -7.3815e-02, -2.8139e-01, -6.8489e-02, -2.4890e-01,\n",
      "         -1.0459e-01,  2.1866e-01, -1.3651e-02,  2.2082e-01,  1.4452e-01,\n",
      "          1.2156e-01, -4.2407e-03,  2.5653e-01, -1.4200e-02, -1.3602e-01,\n",
      "          2.4606e-02, -2.4028e-01,  9.0390e-02, -2.0443e-01, -2.6975e-01,\n",
      "          2.9034e-01, -8.2160e-02, -4.7940e-02,  2.1328e-01,  4.1330e-02,\n",
      "         -1.3397e-01, -5.8570e-02, -1.4730e-01,  2.0228e-01,  2.1142e-01,\n",
      "          1.9471e-01, -9.2306e-02,  2.7115e-02,  2.2871e-02, -2.3399e-01,\n",
      "         -6.6573e-02, -3.1557e-02, -6.7305e-02, -9.1439e-02, -2.6135e-01,\n",
      "         -7.6052e-02, -1.3300e-01,  2.3206e-01,  2.1293e-01, -4.6348e-02,\n",
      "         -1.7028e-01, -3.1770e-02,  3.3343e-03, -7.7845e-03,  2.5576e-01,\n",
      "          4.3426e-02,  1.1490e-02, -9.8305e-02, -2.2572e-01, -5.6736e-02,\n",
      "          2.4170e-01, -8.3043e-02, -3.2750e-01, -1.8894e-01, -7.3210e-02,\n",
      "          2.4886e-02, -5.6985e-02,  3.2579e-01, -7.0417e-02,  5.7850e-02,\n",
      "         -5.1130e-02, -6.9433e-02, -1.9004e-01,  1.9591e-02, -1.7293e-01,\n",
      "         -1.5911e-01,  9.1796e-02,  3.4482e-01, -8.8048e-03,  9.0983e-02,\n",
      "         -2.2528e-02, -1.4741e-01,  5.4736e-02, -2.4447e-01, -1.3338e-01,\n",
      "         -1.2902e-01, -1.2377e-01, -2.4834e-01, -2.0396e-02, -4.5322e-03,\n",
      "          1.2454e-01, -4.7695e-02,  1.7699e-01, -3.2227e-01,  2.7142e-01,\n",
      "         -1.2625e-01, -2.3601e-01,  9.6091e-02,  3.3450e-01, -1.4860e-01,\n",
      "         -1.3938e-01,  1.1786e-01,  2.0374e-02, -1.1923e-01, -2.8852e-01,\n",
      "         -2.2158e-01,  1.6883e-01,  1.2095e-01,  3.1874e-01, -3.6485e-01,\n",
      "          1.2620e-02, -6.6985e-02, -1.9559e-01]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "Last hidden state shape:\n",
      "torch.Size([1, 21, 768])\n",
      "\n",
      "First few values of the last hidden state:\n",
      "tensor([-0.0261,  0.2147,  0.1159, -0.0422,  0.1558, -0.0406,  0.3217, -0.1028,\n",
      "         0.2336, -0.0715])\n"
     ]
    }
   ],
   "source": [
    "#DIFFERENT BERT MODEL FOR TWEETS\n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# For transformers v4.x+:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "# For transformers v3.x:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)  # Models outputs are now tuples\n",
    "    \n",
    "# Print the output (features)\n",
    "print(\"Model output (features):\")\n",
    "print(features)\n",
    "\n",
    "# If you want to print specific parts of the output, you can do so like this:\n",
    "print(\"\\nLast hidden state shape:\")\n",
    "print(features.last_hidden_state.shape)\n",
    "print(\"\\nFirst few values of the last hidden state:\")\n",
    "print(features.last_hidden_state[0, 0, :10])  # First 10 values of the first token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
