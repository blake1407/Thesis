{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \\nDictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\\n\\nOnce we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import csv\n",
    "import os\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Influencer:\n",
    "    name: str\n",
    "    affiliation: str\n",
    "    no_followers: int = 0\n",
    "    before_corpus: str = \"\"\n",
    "    after_corpus: str = \"\"\n",
    "\n",
    "data = []\n",
    "account_list = []\n",
    "before_corpus = \"\"\n",
    "after_corpus = \"\"\n",
    "\n",
    "#Loading File paths\n",
    "supplementary_folder = \"Supplementary Materials\"\n",
    "raw_data_folder = os.path.join(\"Raw Data\", \"Before\")\n",
    "parsed_folder = os.path.join(\"Parsed Data\", \"Before\")\n",
    "parsed_run1_folder = os.path.join(\"Parsed Data\", \"After\", \"Run 1\")\n",
    "parsed_run2_folder = os.path.join(\"Parsed Data\", \"After\", \"Run 2\")\n",
    "parsed_before_folder = os.path.join(\"Parsed Data\", \"Before\")\n",
    "\n",
    "influencers_path = os.path.join(supplementary_folder, \"Followers List & Categories - Accounts Kept.csv\")\n",
    "\n",
    "\n",
    "#Population the data file with initial data of the available influencers\n",
    "with open(influencers_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) #skip header\n",
    "    for line in reader:\n",
    "        name = line[0]\n",
    "        account_list.append(name[1:]) #Creating a list of influencers account names\n",
    "\n",
    "        affiliation = line[1]\n",
    "        if affiliation == \" Libertarian Party\":\n",
    "            affiliation = affiliation[1:]\n",
    "\n",
    "        followers = line[2]\n",
    "        if affiliation or followers:\n",
    "            data.append(Influencer(name[1:], affiliation, followers))\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    csv_file = os.path.join(parsed_run1_folder, f\"{i+1} - Parsed_2024-09-22_2023-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2].strip()\n",
    "            date = line[1]\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if tweet == i.after_corpus:\n",
    "                        if not tweet.endswith(\".\"):\n",
    "                            tweet = tweet.strip() + \". \"\n",
    "                        i.after_corpus = i.after_corpus + tweet\n",
    "                        after_corpus = after_corpus + tweet\n",
    "for i in range(2):\n",
    "    csv_file = os.path.join(parsed_run2_folder, f\"{i+7} - Parsed_2024-09-22_2023-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2]\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if not tweet.endswith(\".\"):\n",
    "                            tweet = tweet.strip() + \". \"\n",
    "                    if tweet not in i.after_corpus:\n",
    "                        i.after_corpus = i.after_corpus + tweet\n",
    "                        after_corpus = after_corpus + tweet\n",
    "\n",
    "for i in range(2):\n",
    "    csv_file = os.path.join(parsed_before_folder, f\"{i+1} - Parsed_2023-10-07_2022-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2].strip()\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if not tweet.endswith(\".\"):\n",
    "                            tweet = tweet.strip() + \". \"\n",
    "                    if tweet not in i.before_corpus:\n",
    "                        i.before_corpus = i.before_corpus + tweet\n",
    "                        before_corpus = before_corpus + tweet\n",
    "\n",
    "\n",
    "with open(\"Pre-processing.csv\", 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Name', 'Affiliation', 'Number of Followers', \"Before Corpus\", \"After Corpus\"])\n",
    "        for i in data:\n",
    "            if i.before_corpus and i.after_corpus:\n",
    "                writer.writerow([i.name, i.affiliation, i.no_followers, i.before_corpus, i.after_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Post:\n",
    "#     status: str\n",
    "#     text: str\n",
    "#     sentiment: str \n",
    "#     toxicity: str \n",
    "#     adj: str\n",
    "#     adj_count: int\n",
    "\n",
    "# all = []\n",
    "\n",
    "# for post in before: \n",
    "#     all.append(Post('Before', post, '', '', '', 0))\n",
    "# for post in after: \n",
    "#     all.append(Post('After', post, '', '', '', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# # Define a function to perform sentiment analysis\n",
    "# def analyze_sentiment(text):\n",
    "#     # Tokenize input text\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "#     # Perform inference\n",
    "#     outputs = model(**inputs)\n",
    "#     logits = outputs.logits\n",
    "\n",
    "#     # Apply softmax to get probabilities\n",
    "#     probs = torch.softmax(logits, dim=1).detach().numpy()[0]\n",
    "\n",
    "#     # Determine sentiment label\n",
    "#     label_mapping = {\n",
    "#         0: \"Very negative\",\n",
    "#         1: \"Negative\",\n",
    "#         2: \"Neutral\",\n",
    "#         3: \"Positive\",\n",
    "#         4: \"Very positive\"\n",
    "#     }\n",
    "#     sentiment_label = label_mapping[int(probs.argmax())]\n",
    "\n",
    "#     return sentiment_label, probs\n",
    "\n",
    "# negative_sentences = []\n",
    "# positive_sentences = []\n",
    "\n",
    "# for post in all:\n",
    "#     # Perform sentiment analysis\n",
    "#     text = post.text\n",
    "#     sentiment, probabilities = analyze_sentiment(text)\n",
    "#     post.sentiment = sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline\n",
    "\n",
    "# model_path = \"JungleLee/bert-toxic-comment-classification\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "# # print(pipeline(\"Whatever happens I love you all and the sun will come up tomorrow. Now lets see those memes!.\"))\n",
    "\n",
    "# for post in all:\n",
    "#     # Perform sentiment analysis\n",
    "#     text = post.text\n",
    "#     result = pipeline(text)\n",
    "#     for r in result: \n",
    "#         label, score = r['label'], r['score']\n",
    "#         post.toxicity = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy \n",
    "# from spacy.matcher import Matcher\n",
    "\n",
    "# en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# matcher = Matcher(en_nlp.vocab)\n",
    "\n",
    "# # patterns = [\n",
    "# #     [{'POS':'ADJ'}, {'POS':'NOUN'}], [{'POS':'AUX'}, {'POS':'ADJ'}], [{'POS':'ADJ'}]\n",
    "# #     ]\n",
    "# patterns = [\n",
    "#     [{'POS':'ADJ'}]\n",
    "#     ]\n",
    "# matcher.add(\"demo\", patterns)\n",
    "\n",
    "# before_adj = []\n",
    "# after_adj = []\n",
    "\n",
    "# for post in all:\n",
    "#     doc = en_nlp(post.text)\n",
    "#     x = \"\"\n",
    "#     count = 0\n",
    "#     matches = matcher(doc)\n",
    "#     for match_id, start, end in matches:\n",
    "#         string_id = en_nlp.vocab.strings[match_id]  # Get string representation\n",
    "#         span = doc[start:end]  # The matched span\n",
    "#         if 'http' not in span.text and span.text not in x:\n",
    "#             x = x + span.text + \", \"\n",
    "#             count += 1\n",
    "#         # print(x + \" \" + str(count))\n",
    "#     # Remove the last comma and space\n",
    "#     post.adj = x[:-2]  # Remove the last comma and space\n",
    "#     post.adj_count = count\n",
    "    \n",
    "#     if post.status == \"Before\":\n",
    "#         before_adj.append(post.adj)\n",
    "#     if post.status == 'After':\n",
    "#         after_adj.append(post.adj)\n",
    "\n",
    "# print(f'Before: {before_adj}')\n",
    "# print(f'After: {after_adj}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_filename = 'Sentiment_analyzed'\n",
    "# with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "#             writer = csv.writer(csv_file)\n",
    "#             writer.writerow(['Status', 'Text', 'Sentiment', 'Toxicity'])\n",
    "#             for post in all:\n",
    "#                    writer.writerow([post.status, post.text, post.sentiment, post.toxicity, post.adj, post.adj_count])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
