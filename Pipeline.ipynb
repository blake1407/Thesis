{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import os\n",
    "   \n",
    "import pandas as pd \n",
    "import spacy \n",
    "# spacy.cli.download(\"pt_core_news_sm\")\n",
    "# spacy.cli.download(\"es_core_news_sm\")\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#Sentence Tokenization using sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#Detect language using detect_langs\n",
    "import langdetect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "#Detect language using Lingua\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments yielded for the corpus (that are not urls or deleted): 255.\n",
      "Number of removed/deleted comments (has been filetered from corpus): 14.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "First, I'm gonna get the entire corpus from the \"Reddit Post Parsed\" folder.\n",
    "\"\"\"\n",
    "#replace as needed\n",
    "folder_name = \"Brazilian Election 1\"\n",
    "file_path = os.path.join(folder_name, \"log.csv\")\n",
    "\n",
    "all_post_titles = []\n",
    "expected_no_comments = 0\n",
    "corpus = \"\"\n",
    "comment_urls = []\n",
    "all_links = []\n",
    "og_posts = []\n",
    "comments_only = []\n",
    "replies = []\n",
    "\n",
    "with open(file_path, mode = 'r') as file:\n",
    "    link_column = []\n",
    "    title_column = []\n",
    "    comments_column = []\n",
    "    all_no_comments = []\n",
    "    csvFile = csv.reader(file)\n",
    "    for line in csvFile:\n",
    "        title_column.append(line[2])\n",
    "        all_post_titles = title_column[1:]\n",
    "\n",
    "        comments_column.append(line[9])\n",
    "        all_no_comments = comments_column[1:]\n",
    "\n",
    "        link_column.append(line[3])\n",
    "        all_links = link_column[1:]\n",
    "        \n",
    "    for number in all_no_comments:\n",
    "        expected_no_comments += int(number)\n",
    "\n",
    "@dataclass \n",
    "class Post:\n",
    "    post: str\n",
    "    is_og: bool\n",
    "    id: int\n",
    "    length: int\n",
    "    sentiment: str\n",
    "    adj: str\n",
    "    adj_count: int\n",
    "    profanity_score: float\n",
    "\n",
    "post_data = []\n",
    "\n",
    "#loop to open all post titles in create one big corpus of all comments\n",
    "def create_corpus(titles: list) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in a list of posts titles in the \n",
    "    folder \"Reddit Post Parsed\" and loops through each \n",
    "    csv file to filter for proper comments, that are not urls\n",
    "    and deleted to return the corpus.\n",
    "\n",
    "    Comments that are just links will be \n",
    "    appended to the list \"comment_urls\"!\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "    global comment_urls\n",
    "    global folder_name\n",
    "\n",
    "    count_proper_comments = 0\n",
    "    no_deleted_comments = 0\n",
    "    empty = \"\"\n",
    "    list_of_comments = []\n",
    "    id = 1\n",
    "    \n",
    "    base_folder = folder_name\n",
    "    for title in titles:\n",
    "        count = 1\n",
    "        title_csv = os.path.join(base_folder, title + \"'s post.csv\")\n",
    "        if not os.path.isfile(title_csv):\n",
    "            print(f\"File '{title_csv}' not found.\")\n",
    "            continue\n",
    "        with open(title_csv, mode='r', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                post = empty.join(row[9:]).strip()\n",
    "                list_of_comments.append(post)\n",
    "                if count == 2:\n",
    "                    og_posts.append(post)\n",
    "                    post_data.append(Post(post,True,id, len(post), \"\", \"\", 0, 0))\n",
    "                if count > 2:\n",
    "                    comments_only.append(post)\n",
    "                    post_data.append(Post(post,False,id, len(post), \"\", \"\", 0, 0))\n",
    "                count +=1\n",
    "        id += 1\n",
    "\n",
    "    # print(list_of_comments)\n",
    "    # print(f'OG Posts: {og_posts}')\n",
    "\n",
    "    for comment in list_of_comments:\n",
    "        if comment.strip() != \"Body\":\n",
    "            if comment.strip() == '\"deleted\"' or comment.strip() == '\"removed\"':\n",
    "                no_deleted_comments +=1\n",
    "                comment = \"\"\n",
    "            if comment.strip().startswith('\"https:'):\n",
    "                comment_urls.append(comment.replace('\"', \"\").strip())\n",
    "            else:\n",
    "                count_proper_comments += 1 \n",
    "                corpus = corpus + \" \" + comment.replace(\"**\", \"\").replace(\"#\", \"\").strip()[1:-1] \n",
    "    print(f'Number of comments yielded for the corpus (that are not urls or deleted): {count_proper_comments}.') \n",
    "    print(f'Number of removed/deleted comments (has been filetered from corpus): {no_deleted_comments}.\\n')                  \n",
    "                \n",
    "create_corpus(all_post_titles)\n",
    "# print(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_92405/3832054853.py:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of total sentence tokens: 507.\n",
      "Amount of total word token: 4821.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class entities:\n",
    "    name: str\n",
    "    label: str\n",
    "\n",
    "#strip out the pronouns, conjunctions, etc.!\n",
    "# f = open('stop words.txt', 'r')\n",
    "# stopwords = f.read()\n",
    "# stopwords = stopwords.split('\\n')\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "# Load the spaCy English & Portuguese models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "pt_nlp = spacy.load('pt_core_news_sm')\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "#separate into tokenized sentences\n",
    "tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n",
    "sentences_token = tokenizer.tokenize(corpus)\n",
    "sentences = []\n",
    "for sentence in sentences_token:\n",
    "    if sentence.strip() not in stopwords:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "#separate corpus in words\n",
    "words_token = word_tokenize(corpus)\n",
    "words = []\n",
    "#remove any conjunctions, articles, particles, etc.\n",
    "for word in words_token:\n",
    "    if word.lower().strip() not in stopwords:\n",
    "        words.append(word)\n",
    "\n",
    "def checkW(x: int):\n",
    "    return (x/len(words))*100\n",
    "\n",
    "def checkS(x: int):\n",
    "    return (x/len(sentences))*100\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(f'Amount of total sentence tokens: {len(sentences)}.')\n",
    "print(f'Amount of total word token: {len(words)}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3: Lingua sentence by sentence\n",
    "\n",
    "#import English, Portuguese, Spanish detector\n",
    "languages = [Language.ENGLISH, Language.PORTUGUESE, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentences = []\n",
    "\n",
    "discarded_l = 0\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(sentence, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(sentence, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(sentence, Language.SPANISH)\n",
    "        if en_l > 0.8:\n",
    "            english_sentences.append(sentence)\n",
    "        elif pt_l > 0.8:\n",
    "            portuguese_sentences.append(sentence)\n",
    "        elif es_l > 0.8:\n",
    "            spanish_sentences.append(sentence)\n",
    "        else:\n",
    "            mixed_sentences.append(sentence)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discarded_l +=1\n",
    "        continue\n",
    "\n",
    "# print(\"3. Lingua sentence by sentence\")\n",
    "# print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "# print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "# print(f'Spanish sentences: {len(spanish_sentences)} - {checkS(len(spanish_sentences)):.2f}%.')\n",
    "# print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "# print(f'Discarded: {discarded_l} - {checkS(discarded_l):.2f}%.')\n",
    "# print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentences)+ len(mixed_sentences)):.2f}%.\\n')\n",
    "\n",
    "# METHOD 4: Lingua word by word\n",
    "\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "\n",
    "discard_w = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(word, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(word, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(word, Language.SPANISH)\n",
    "        if en_l > 0.5:\n",
    "            en_w.append(word)\n",
    "        elif pt_l > 0.5:\n",
    "            pt_w.append(word)\n",
    "        elif es_l > 0.5:\n",
    "            es_w.append(word)\n",
    "        else:\n",
    "            mixed_w.append(word)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discard_w +=1\n",
    "        continue\n",
    "\n",
    "# print(\"Lingua word by word\")\n",
    "# print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "# print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "# print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "# print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "# print(f'Discarded: {discard_w} - {checkW(discard_w):.2f}%.')\n",
    "# print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')\n",
    "\n",
    "# print(f'English sentences: {english_sentences}')\n",
    "# print(f'Portuguese sentences: {portuguese_sentences}')\n",
    "# print(f'Spanish sentences: {spanish_sentences}')\n",
    "# print(f'Mixed sentences: {mixed_sentences}')\n",
    "\n",
    "# print(f'English words: {en_w}')\n",
    "# print(f'Portuguese words: {pt_w}')\n",
    "# print(f'Spanish words: {es_w}')\n",
    "# print(f'Mixed words: {mixed_w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = english_sentences + mixed_sentences\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "\n",
    "def filter(words: list):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word.lower().strip() not in stopwords and len(word) != 1 and word.lower() not in result:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "english_words = filter(en_w)\n",
    "portuguese_words = filter(pt_w)\n",
    "spanish_words = filter(es_w)\n",
    "mixed_words = filter(mixed_w)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bolsonaro', 'Lula', 'Erysipelas', 'Biden', 'Putin', 'Santos-Cruz', 'Hamilton Mourao', 'Jair Bolsonaro', 'Luiz Inacio Lula da Silva', 'Lulas', 'Silvinei Vasques', 'Lula Bolsonaro', 'Cerberusz', 'Storming', 'Bannon', 'Lyon', 'Netflix Neymar', 'Jr', 'Neymar Sr', 'Mbappé', 'Neymar', 'Neymar Pulling', 'Death Arrest Win', 'da Silva', 'GTFO']\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class entities:\n",
    "    name: str\n",
    "    label: str\n",
    "\n",
    "# #strip out the pronouns, conjunctions, etc.!\n",
    "# f = open('stop words.txt', 'r')\n",
    "# stopwords = f.read()\n",
    "# stopwords = stopwords.split('\\n')\n",
    "\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "# Load the spaCy English & Portuguese models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "pt_nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "PeopleS = []\n",
    "PeopleW = []\n",
    "\n",
    "en_doc = en_nlp(\" \".join(total_sentences))\n",
    "pt_doc = pt_nlp(\" \".join(spanish_sentences) + \" \".join(portuguese_sentences))\n",
    "\n",
    "for ent in en_doc.ents:\n",
    "    # The output displayed the names of the entities and their predicted labels.\n",
    "    if ent.text not in PeopleS and ent.label_ == 'PERSON':\n",
    "        # PeopleS.append(entities(ent.text, ent.label_))\n",
    "        PeopleS.append(ent.text)\n",
    "\n",
    "print(PeopleS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(re.escape(people) for people in PeopleS)\n",
    "\n",
    "# Use re.sub to remove the phrases from the corpus\n",
    "cleaned_corpus = re.sub(pattern, '', corpus)\n",
    "\n",
    "corpus = en_nlp(cleaned_corpus)\n",
    "noun_chunks = []\n",
    "for chunk in corpus.noun_chunks:\n",
    "    noun_chunks.append(chunk.text)\n",
    "\n",
    "\n",
    "unwanted_patterns = r'\\b(my|them|me|everyone|our|even|him|her|us|itself|people|a|an|the|he|she|it|i|you|we|they|his|her|hers|its|their|theirs|this|that|these|those|there|where|who|whom|which|what|when|why|how|am|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|shall|should|can|could|may|might|must|ought|and|but|or|nor|for|yet|so|because|as|if|once|since|unless|until|while|although|though|after|before|until|by|on|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|not|only|own|same|so|than|too|very|s|t|can|will|just|don|should|now)\\b|[.,!?;:[]()]'\n",
    "\n",
    "filtered_words = []\n",
    "for noun in noun_chunks:\n",
    "    filtered_phrase = ' '.join(word for word in re.split(r'\\s+', noun) if not re.fullmatch(unwanted_patterns, word, re.I))\n",
    "    if filtered_phrase:  # Ensure it's not empty\n",
    "        filtered_words.append(filtered_phrase)\n",
    "\n",
    "leftover = []\n",
    "for word in filtered_words:\n",
    "    en_word = en_nlp(word)\n",
    "    for ent in en_word.ents:\n",
    "        #Should I add in 'ORG' tags? 'Trump' is flagged as ORG, but so does 'Congress', 'Rulers', 'FAQ'\n",
    "        if ent.text not in leftover and ent.label_ == 'PERSON' or ent.label_ == 'ORG':\n",
    "            # print(ent.text + \" \" + ent.label_)\n",
    "            leftover.append(ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of influencers: ['Bolsonaro', 'Lula', 'Erysipelas', 'Biden', 'Putin', 'Santos-Cruz', 'Hamilton Mourao', 'Jair Bolsonaro', 'Luiz Inacio Lula da Silva', 'Lulas', 'Silvinei Vasques', 'Lula Bolsonaro', 'Cerberusz', 'Storming', 'Bannon', 'Lyon', 'Netflix Neymar', 'Jr', 'Neymar Sr', 'Mbappé', 'Neymar', 'Neymar Pulling', 'Death Arrest Win', 'da Silva', 'GTFO', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'CGP Greys', 'Rulers', 'Trump', 'Congress', 'CGP Greys', 'Rulers', 'Trump', 'Congress', 'CGP Greys', 'Rulers', 'Trump', 'Congress', 'CGP Greys', 'Rulers', 'Trump', 'Congress', 'Rio de Janeiro - Truckers', 'FAQ', 'Rio', 'FAQ', 'Rio', 'FAQ', 'Rio', 'FAQ', 'Rio', 'FAQ', 'Rio', 'Chad Juninho', 'Chad Juninho', 'Luiz Inácio', 'Trump', 'Steal']\n"
     ]
    }
   ],
   "source": [
    "print(f'List of influencers: {PeopleS + leftover}')\n",
    "\n",
    "filtered_words = [word for word in filtered_words if word not in leftover]\n",
    "\n",
    "# Print the updated list\n",
    "# print(f'Final leftover words: {filtered_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Define a function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.softmax(logits, dim=1).detach().numpy()[0]\n",
    "\n",
    "    # Determine sentiment label\n",
    "    label_mapping = {\n",
    "        0: \"Very negative\",\n",
    "        1: \"Negative\",\n",
    "        2: \"Neutral\",\n",
    "        3: \"Positive\",\n",
    "        4: \"Very positive\"\n",
    "    }\n",
    "    sentiment_label = label_mapping[int(probs.argmax())]\n",
    "\n",
    "    return sentiment_label, probs\n",
    "\n",
    "negative_sentences = []\n",
    "positive_sentences = []\n",
    "\n",
    "for post in post_data:\n",
    "    # Perform sentiment analysis\n",
    "    text = post.post\n",
    "    sentiment, probabilities = analyze_sentiment(text)\n",
    "    if text.startswith('\"http') or text == '\"[deleted]\"' or text == '\"deleted\"':\n",
    "        post.sentiment = 'Undefined'\n",
    "    else:\n",
    "        post.sentiment = sentiment\n",
    "\n",
    "# non_url_posts = [post for post in post_data if not post.post.startswith('\"http')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(en_nlp.vocab)\n",
    "\n",
    "patterns = [\n",
    "    [{'POS':'ADJ'}, {'POS':'NOUN'}],\n",
    "    ]\n",
    "matcher.add(\"demo\", patterns)\n",
    "\n",
    "for post in post_data:\n",
    "    doc = en_nlp(post.post)\n",
    "    x = \"\"\n",
    "    count = 0\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = en_nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        x = x + span.text + \", \"\n",
    "        count += 1\n",
    "        # print(x + \" \" + str(count))\n",
    "    # Remove the last comma and space\n",
    "    post.adj = x[:-2]  # Remove the last comma and space\n",
    "    post.adj_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profanity_check import predict, predict_prob\n",
    "\n",
    "for post in post_data:\n",
    "    score = predict_prob([post.post])\n",
    "    for s in score:\n",
    "        post.profanity_score = s\n",
    "# print(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the CSV file\n",
    "csv_file_path = folder_name + \" Post-NLP.csv\"\n",
    "\n",
    "# Define the fieldnames for the CSV file\n",
    "fieldnames = [\"post\", \"is_og\", \"id\", \"length\", \"sentiment\", \"adj\", \"adj_count\", \"profanity_score\"]\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write each post as a row in the CSV file\n",
    "    for post in post_data:\n",
    "        writer.writerow({\n",
    "            \"post\": post.post,\n",
    "            \"is_og\": post.is_og,\n",
    "            \"id\": post.id,\n",
    "            \"length\": post.length,\n",
    "            \"sentiment\": post.sentiment,\n",
    "            \"adj\": post.adj,\n",
    "            \"adj_count\": post.adj_count,\n",
    "            \"profanity_score\": post.profanity_score\n",
    "        })\n",
    "\n",
    "print(\"CSV file has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nspan = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\\nwith doc.retokenize() as retokenizer:\\n    retokenizer.merge(span)\\nnegative_adj = []\\nfor token in doc:\\n    if token.pos_ == 'ADJ':\\n        print(token.text, token.pos_, token.dep_, token.head.text)\\n        negative_adj.append(token.text)\\n\""
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "negative_adj = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "        print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "        negative_adj.append(token.text)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
