{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \\nDictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\\n\\nOnce we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: string):\n",
    "    return ' '.join([item for item in text.split() if item not in stopwords.words('english')])\n",
    "\n",
    "def remove_punctuations(text: string):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "def delete_usernames(text: str) -> str:\n",
    "    pattern = r'@\\w+'\n",
    "    return re.sub(pattern, '@user', text)\n",
    "\n",
    "# Function to split \"After Corpus\" into individual sentences and create a new DataFrame\n",
    "def split_sentences(df, status: str):\n",
    "\n",
    "    abbreviations = {\n",
    "        \"U.S.\": \"United States\",\n",
    "        \"U.K.\": \"United Kingdom\",\n",
    "        \"e.g.\": \"for example,\",\n",
    "        \"i.e.\": \"such as,\",\n",
    "        # \"Dr.\": \"DR_PLACEHOLDER\",\n",
    "        # \"Mr.\": \"MR_PLACEHOLDER\",\n",
    "        # \"Ms.\": \"MS_PLACEHOLDER\",\n",
    "        # \"Mrs.\": \"MRS_PLACEHOLDER\",\n",
    "        # \"Jr.\": \"JR_PLACEHOLDER\",\n",
    "        # \"Sr.\": \"SR_PLACEHOLDER\",\n",
    "        \"etc.\": \"...\"\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the rows of the new DataFrame\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the relevant data\n",
    "        subject_id = row['Subject ID']\n",
    "        tweet_id = row['Tweet ID']\n",
    "        corpus = row[f'{status} Corpus']\n",
    "\n",
    "        # Replace abbreviations with placeholders\n",
    "        for abbr, placeholder in abbreviations.items():\n",
    "            corpus = corpus.replace(abbr, placeholder)\n",
    "        \n",
    "        # Split the corpus into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus)\n",
    "\n",
    "        # Add each sentence as a new row in the list, keeping track of the tweet ID and subject ID\n",
    "        for sentence in sentences:\n",
    "            rows.append({'Subject ID': subject_id, 'Tweet ID': tweet_id, 'Sentence': sentence})\n",
    "\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    return new_df\n",
    "\n",
    "# def stem_text(text: string):\n",
    "#     return [ps.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process Text Data\n",
    "clean_data_path = \"Cleaned Data\"\n",
    "after = pd.read_csv(f\"{clean_data_path}/After_NN_Cleaned.csv\")\n",
    "before = pd.read_csv(f\"{clean_data_path}/Before_NN_Cleaned.csv\")\n",
    "\n",
    "# after_corpus = after[[\"After Corpus\", \"Tweet ID\"]].copy()\n",
    "# before_corpus = before[[\"Before Corpus\", \"Tweet ID\"]].copy()\n",
    "\n",
    "# # Apply all cleaning functions to the After Corpus column\n",
    "# after_corpus[\"Clean Text\"] = (after_corpus[\"After Corpus\"]\n",
    "#                              .apply(delete_usernames)\n",
    "#                              .apply(remove_punctuations)\n",
    "#                              .apply(remove_stop_words))\n",
    "# before_corpus[\"Clean Text\"] = (before_corpus[\"Before Corpus\"]\n",
    "#                              .apply(delete_usernames)\n",
    "#                              .apply(remove_punctuations)\n",
    "#                              .apply(remove_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>hey, remember when you phoned up your despot b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>ace job, homeslice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>609</td>\n",
       "      <td>good thing your meddling caused no serious con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>705</td>\n",
       "      <td>hey, remember when Jared Kushner brought peace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>705</td>\n",
       "      <td>me neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject ID  Tweet ID                                           Sentence\n",
       "0          74       609  hey, remember when you phoned up your despot b...\n",
       "1          74       609                                ace job, homeslice.\n",
       "2          74       609  good thing your meddling caused no serious con...\n",
       "3          74       705  hey, remember when Jared Kushner brought peace...\n",
       "4          74       705                                         me neither"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to split sentences\n",
    "after_sentences = split_sentences(after, \"After\")\n",
    "before_sentences = split_sentences(before, \"Before\")\n",
    "after_sentences.to_csv(\"Tokenizing testing.csv\")\n",
    "\n",
    "# Show the first few rows of the resulting DataFrame\n",
    "after_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# after_corpus[\"Tokenized Text\"] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
