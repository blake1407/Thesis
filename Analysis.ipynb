{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import csv\n",
    "import os\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Every resident in East Palestine, OH should receive Medicare For All. Libby, Montana, was able to do this due to asbestos. Their property value has also decreased and theyve experienced pain and suffering.  FYI, Norfolk Southern donates to BOTH parties!Fox News just threw Trump under the bus (or in this case, train) — pinning him with responsibility for the East Palestine disaster.Republican leaders are telling 2 big lies about East Palestine — here’s the truth:#NeverForget #NeverForgive   No plea deals for 9/11 terrorists!Im sorry but the Democrats you voted for are the reason for the Crime Bill.  The Dems You voted for are the reason for the funding for more cops.  The Dems you voted for are funding the Palestinian genocide. The GOP doesnt care and funded it, too. None of them care.#HeyNext Commentary: Its not fair to expect law enforcement to be perfect - or to predict the future. It is fair to expect law enforcement to use the tools at their disposal to keep people from getting predictably massacred.Leftists in Israel trying to destroy conservative government through \"protests\" for daring to slightly constrain radical left judiciary power grabs.Biden gang walks back claim drone strike killed senior al-Qaeda leader  \"U.S. military officials are walking back claims that a recent strike in Syria killed an influential al-Qaeda figure, following assertions by the dead man’s family that he had no ties to terrorists but was aBush admin allowed 9/11-connected terrorist to speak at Pentagon!  … …Ramadan Mubarak to all of our Muslim friends and neighbors.Happy Passover to all of our Jewish friends and neighbors.The U.S. is doing what it always does, terrorize the world for corporate profits.   Here Seymour Hersh explains EXACTLY how the United States blew up The Nord Stream pipeline so US companies could sell more Natural Gas to Europe.    Were The Terrorists.  …Dershowitz said Trump asked him at dinner why only 27% of Jews voted for him when he strongly supported Israel. Dersh said he told Trump Jews also care about gay rights, abortion, climate change, gun control and separation of church and state, and he was terrible on those.For every Repub trying to blame the Admin for E. Palestine, I continue to cite Steve Bannon, who again today placed the blame every step of the way on DeWine & Norfolk Southern. Says the RR and DeWine did release & burn of chems because they wanted to clear tracks ASAP to getRep. Troy Nehls is in E. Palestine and assures everyone that the city’s water is safe to drink, then demonstrates it by drinking a cup of tap water from the China Cafe. “The Fire Chief, everybody says the water is clean to drink.”Right-wing “independent journalist” who flew in to Maui (he also went into E. Palestine) after the fire is confronted by angry residents while live on Steve Bannon’s show, who said he is exploiting them for politics and using up precious resources.Get this: They found dioxin far above safe levels in East Palestine water and the federal gov wont clean it up because Obama killed EPA proposal requiring the gov to lower the threshold for cleanup.  Democrats ignored the science and people will die because of it.You cannot say you care about women in Iran if you dont speak out for women in Palestine.There was nothing \"controlled\" about @nscorps un-controlled burn in East Palestine. That was a chemical weapons attack.I mean look at this vile shit from the NYT. Israels military killed TERRORISTS. Palestinian terrorists killed SYNAGOGUE ATTENDEES. To the NYT, its all just a cycle of violence:This is all part of a broader Leftist narrative that its the new right-wing Israeli government \"raising risk of escalation\" when 13-year-old terrorists shoot people going to synagogue, Hamas cheers, and the PA cuts off security cooperation. Look at this pathetic NYT headline:I met this Israeli soldier while taking a tour of the Western Wall tunnels. It turns out he turns weapons of war into meaningful art.A Palestinian Arab murders seven Jews going to synagogue in Jerusalem, the historic capital of the Jewish people.  Many Palestinians celebrate. Hamas takes credit.   Media response: This is a cycle of violence. After all, Jews WERE going to shul in Jerusalem.The juxtaposition of rabid anti-Semite Rashida Tlaib attempting to perform a \"Nakba Day\" lamenting the existence of the State of Israel while Hamas fires missiles into the heart of Tel Aviv is pretty clarifying.Harmeet laid out the possible legal options for the residents of East Palestine, OH, who were sickened due to possible negligence of Norfolk Southern and gov’t officials.Angle: East Palestine residents are still begging for answers while being forced to live in a cloud of uncertainty and fear.Just over a month ago, the train derailment in East Palestine, Ohio caused metric tons of toxic chemicals to spew into the air and water.   Many locals are already suffering health issues as a result, and tens of thousands pets and wildlife have been reported dead.I am infinitely more concerned with the raging chemical fire in East Palestine OH then I am with NORAD playing whack-a-mole with balloons.Jew here.  Words mean things.  When everything is called antisemitic, it helps provide cover for actual antisemitism.RFK Jr: \"If you live in any of these other countries & youre gay, for example, you can be kiled for that. Israel is the only place where you have freedom. If youre a transvestite, if you have any other disident views, youd much rather be in Isreal. Isreal is going into theImperialists often raise the example of WWII & accusations of genocide to justify war. (E.g. incubator babies in Kuwait). But does the left have a response to the question of how to identify a Hitler-level threat, & what to do about it? Must it?  …Watch my full exchange with RFK Jr. in which I challenge his “unconditional” support of Israel in light of the international consensus that Israel is an apartheid state.If you were on a mission to create more antisemitism in this country, you couldn’t do a better job than the ADL.Isn’t it crazy that almost no one in the political or media class talk about radical Islamic terrorism anymore. There’s no concern about it, and yet every emergency power that the government took in the name of terrorism, DHL, TSA, Patriot act, is just part of America now.You cannot possibly compare NATOs interventions to Russias invasions. It requires both a deliberate ignorance of the scale and to deliberately ignore the history of Russian genocide in Ukraine.  Dont deliberately ignore the history of Russian genocide in Ukraine.I know a few people in Israel right now wondering something.  Ill just say this: WE FUCKING WARNED YOU.The purpose of terrorism is to terrorize.East Palestine Mayor Trent Conaway: I really feel for our residentsPothole Pete finally says he will travel to East PalestineHannity Grills Ramaswamy Denying He Said Aid to Israel Would Stop in 2028: ‘You Want Me to Read It?’Even Fox News knows that the deregulation by the trump admin were the cause of the disaster in East Palestine, OH. And Elaine Chao never visited train derailment sites, even when there were fatalities.OOPS! Looks like Fox News allowed a bit of truth to slip into their coverage of the train derailment story in East Palestine, OH — theyre blaming trump!!!  Be a real shame if everyone retweeted this.MUST WATCH: Senator JD Vance visits a creek flowing near the East Palestine train derailment and chemical burn. See what happens when he drags a stick along the creek bed.An East Palestine womans chickens all die following the controlled burn and train derailment.   Whats going on?East Palestine, Ohio’s latest tourism video. Everything is fine!Joy Behar says the residents of East Palestine got what they deserved because they voted for Trump.   For the record, this is 1000x worse than anything Don Lemon said. Will she be kicked off her show and forced to undergo sensitivy training?A month ago, @LeverNews broke open the story of governmental decisions that deregulated the rail industry before the East Palestine disaster.  Today, @SenWhitehouse spotlighted some of those revelations in the Senate hearing with Norfolk Southerns CEO.Corporate media has been lying about the East Palestine disaster by pretending Trump is the only perpetrator of rail deregulation.  The actual truth: The Obama, Trump and Biden administrations all played pivotal roles in that dereg.Imagine if wealthy investors had spoken out as forcefully for rail regulation as they spoke out for special depositor insurance.  Imagine if the government responded as quickly & forcefully to help East Palestine residents as it responded to help rich VCs.The company that made the toxic chemical that exploded in East Palestine funneled $2 million to Senate Republicans super PAC, and now the bill is stalled in the Senate: …Literally everyone in Congress in February: “We must pass rail safety legislation.”  Six months later, after $2 million was funneled to the GOP from the maker of the toxic chemical that exploded in East PalestineCNN had a literal lobbyist for Norfolk Southern on their panel to talk about East Palestine and didnt disclose it.A provision in Obamacare allows residents in areas deemed a public health disaster to be covered by Medicare for life. At the very least, East Palestine residents deserve this universal coverage after being exposed to a known carcinogen.Great work on this @heyjohnrussell. Thanks to @moreperfectunion for giving him this platform and not forgetting about East Palestine!On The Warning podcast this week, I spoke with @JGreenblattADL, the executive director of the @ADL, about the alarming rise of extremism, and antisemitism specifically, in the United States.  Catch our full conversation here:4. Hatred, Lies and scapegoating are on the ballot as is a growing and dangerous menace of antisemitism.   The American experiment has been under a growing assault for seven years. It is not abating and it has become a frontal assault that is occurring in broad daylight.CONGRATULATIONS INDIANA !   The ATTORNEY GENERAL of INDIANA embraces ANTISEMITISM and KANYE WEST who promised to attack the JEWS.  THE MAGA MOVEMENT IS AMERICAN CANCERJimmy Dore calling out @RobertKennedyJr on his refusal to stand up to Israel:  “Why is it that you can stand up to the military industrial complex, you can stand up to Big Pharma, you can stand up to your own party, but you cant stand up to Israel?\"American Jews will not be pushed around by a Fascist Criminal.Leftist-run NY Times and WaPo Continue Smear Campaign Against Orthodox Jews As Antisemitic Hate Crimes SurgeWhen the Biden administration sided with rail companies to prevent the workers from striking, they were siding with companies like Norfolk Southern, whose train derailment in East Palestine, Ohio sent plumes of poison gas into the air for days, putting all residents at risk.Youre telling me -- not Bernie Sanders, who is making kissy-face with Rashida Tlaib over an event declaring Israels creation a disaster WHILE ROCKETS FALL ON TEL AVIV -- that its antisemitic to call out Bernies anti-Jewish garbage? GFY.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Influencer:\n",
    "    name: str\n",
    "    affiliation: str\n",
    "    no_followers: int = 0\n",
    "    before_corpus: str = \"\"\n",
    "    after_corpus: str = \"\"\n",
    "\n",
    "data = []\n",
    "account_list = []\n",
    "before_corpus = \"\"\n",
    "after_corpus = \"\"\n",
    "\n",
    "#Loading File paths\n",
    "supplementary_folder = \"Supplementary Materials\"\n",
    "raw_data_folder = os.path.join(\"Raw Data\", \"Before\")\n",
    "parsed_folder = os.path.join(\"Parsed Data\", \"Before\")\n",
    "parsed_run1_folder = os.path.join(\"Parsed Data\", \"After\", \"Run 1\")\n",
    "parsed_run2_folder = os.path.join(\"Parsed Data\", \"After\", \"Run 2\")\n",
    "parsed_before_folder = os.path.join(\"Parsed Data\", \"Before\")\n",
    "\n",
    "influencers_path = os.path.join(supplementary_folder, \"Followers List & Categories - Accounts Kept.csv\")\n",
    "\n",
    "\n",
    "#Population the data file with initial data of the available influencers\n",
    "with open(influencers_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) #skip header\n",
    "    for line in reader:\n",
    "        name = line[0]\n",
    "        account_list.append(name[1:]) #Creating a list of influencers account names\n",
    "\n",
    "        affiliation = line[1]\n",
    "        if affiliation == \" Libertarian Party\":\n",
    "            affiliation = affiliation[1:]\n",
    "\n",
    "        followers = line[2]\n",
    "        if affiliation or followers:\n",
    "            data.append(Influencer(name[1:], affiliation, followers))\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    csv_file = os.path.join(parsed_run1_folder, f\"{i+1} - Parsed_2024-09-22_2023-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2].strip()\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if tweet not in i.after_corpus:\n",
    "                        i.after_corpus = i.after_corpus + tweet\n",
    "                        after_corpus = after_corpus + tweet\n",
    "for i in range(2):\n",
    "    csv_file = os.path.join(parsed_run2_folder, f\"{i+7} - Parsed_2024-09-22_2023-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2].strip()\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if tweet not in i.after_corpus:\n",
    "                        i.after_corpus = i.after_corpus + tweet\n",
    "                        after_corpus = after_corpus + tweet\n",
    "\n",
    "for i in range(2):\n",
    "    csv_file = os.path.join(parsed_before_folder, f\"{i+1} - Parsed_2023-10-07_2022-10-07.csv\")\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) #skip header\n",
    "        \n",
    "        for line in reader:\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line[0] == \"|RUN STATISTICS|\":\n",
    "                continue\n",
    "        \n",
    "            name = line[0].strip() if line else \"\"\n",
    "            tweet = line[2].strip()\n",
    "            for i in data:\n",
    "                if name == i.name:\n",
    "                    if tweet not in i.before_corpus:\n",
    "                        i.before_corpus = i.before_corpus + tweet\n",
    "                        before_corpus = before_corpus + tweet\n",
    "\n",
    "count = 0\n",
    "for line in data:\n",
    "    if line.before_corpus and line.after_corpus:\n",
    "        # print(line)\n",
    "        count+=1\n",
    "print(count)\n",
    "print(before_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'before' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     adj_count: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mall\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbefore\u001b[49m: \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mall\u001b[39m\u001b[38;5;241m.\u001b[39mappend(Post(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore\u001b[39m\u001b[38;5;124m'\u001b[39m, post, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m after: \n",
      "\u001b[0;31mNameError\u001b[0m: name 'before' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Post:\n",
    "    status: str\n",
    "    text: str\n",
    "    sentiment: str \n",
    "    toxicity: str \n",
    "    adj: str\n",
    "    adj_count: int\n",
    "\n",
    "all = []\n",
    "\n",
    "for post in before: \n",
    "    all.append(Post('Before', post, '', '', '', 0))\n",
    "for post in after: \n",
    "    all.append(Post('After', post, '', '', '', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     logging,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     51\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/__init__.py:32\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     26\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     ContextManagers,\n\u001b[1;32m     34\u001b[0m     ExplicitEnum,\n\u001b[1;32m     35\u001b[0m     ModelOutput,\n\u001b[1;32m     36\u001b[0m     PaddingStrategy,\n\u001b[1;32m     37\u001b[0m     TensorType,\n\u001b[1;32m     38\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     39\u001b[0m     cached_property,\n\u001b[1;32m     40\u001b[0m     can_return_loss,\n\u001b[1;32m     41\u001b[0m     expand_dims,\n\u001b[1;32m     42\u001b[0m     find_labels,\n\u001b[1;32m     43\u001b[0m     flatten_dict,\n\u001b[1;32m     44\u001b[0m     infer_framework,\n\u001b[1;32m     45\u001b[0m     is_jax_tensor,\n\u001b[1;32m     46\u001b[0m     is_numpy_array,\n\u001b[1;32m     47\u001b[0m     is_tensor,\n\u001b[1;32m     48\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     49\u001b[0m     is_tf_tensor,\n\u001b[1;32m     50\u001b[0m     is_torch_device,\n\u001b[1;32m     51\u001b[0m     is_torch_dtype,\n\u001b[1;32m     52\u001b[0m     is_torch_tensor,\n\u001b[1;32m     53\u001b[0m     reshape,\n\u001b[1;32m     54\u001b[0m     squeeze,\n\u001b[1;32m     55\u001b[0m     strtobool,\n\u001b[1;32m     56\u001b[0m     tensor_size,\n\u001b[1;32m     57\u001b[0m     to_numpy,\n\u001b[1;32m     58\u001b[0m     to_py_obj,\n\u001b[1;32m     59\u001b[0m     transpose,\n\u001b[1;32m     60\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     63\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     64\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[1;32m     94\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     torch_only_method,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/generic.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, Iterable, List, Tuple\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/__init__.py:130\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    132\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m    your python interpreter from there.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/__config__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This file is generated by numpy's build process\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# It contains system_info results at the time of building this package.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     __cpu_features__,\n\u001b[1;32m      6\u001b[0m     __cpu_baseline__,\n\u001b[1;32m      7\u001b[0m     __cpu_dispatch__,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m _built_with_meson \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m         env_added\u001b[38;5;241m.\u001b[39mappend(envkey)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/multiarray.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCreate the numpy.core.multiarray namespace for backward compatibility. In v1.16\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mthe multiarray and umath c-extension modules were merged into a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/overrides.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[1;32m     12\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     14\u001b[0m array_function_like_doc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"like : array_like, optional\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        Reference object to allow the creation of arrays which are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m        compatible with that passed in via this argument.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:463\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Define a function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.softmax(logits, dim=1).detach().numpy()[0]\n",
    "\n",
    "    # Determine sentiment label\n",
    "    label_mapping = {\n",
    "        0: \"Very negative\",\n",
    "        1: \"Negative\",\n",
    "        2: \"Neutral\",\n",
    "        3: \"Positive\",\n",
    "        4: \"Very positive\"\n",
    "    }\n",
    "    sentiment_label = label_mapping[int(probs.argmax())]\n",
    "\n",
    "    return sentiment_label, probs\n",
    "\n",
    "negative_sentences = []\n",
    "positive_sentences = []\n",
    "\n",
    "for post in all:\n",
    "    # Perform sentiment analysis\n",
    "    text = post.text\n",
    "    sentiment, probabilities = analyze_sentiment(text)\n",
    "    post.sentiment = sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline\n",
    "\n",
    "model_path = \"JungleLee/bert-toxic-comment-classification\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "# print(pipeline(\"Whatever happens I love you all and the sun will come up tomorrow. Now lets see those memes!.\"))\n",
    "\n",
    "for post in all:\n",
    "    # Perform sentiment analysis\n",
    "    text = post.text\n",
    "    result = pipeline(text)\n",
    "    for r in result: \n",
    "        label, score = r['label'], r['score']\n",
    "        post.toxicity = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['less, Jewish, expensive', 'sure, Jewish, murderous, own', 'Israeli, Eritrean, single, White, Jewish', 'middle, eastern', 'Jewish, middle', 'queer, white, accustomed, progressive, social, Jewish, constructive', 'european, progressive, jewish, old', 'Jewish, antisemitic, valid', 'Jewish', 'easy, arab', 'full, fledged, ONLY, non, Jewish, White', 'jewish, undefeated, endless, foolish', 'cool, Ashkenazi, Jewish, great', 'British, much, other', 'non, -, Jewish', 'Jewish, entire']\n",
      "After: ['weird, pro, -, Israel, uhhhh, Jewish', 'Jewish, absolute, moral', 'Middle', 'weird, own, unrelated, Jewish', 'most, dispensable, unworthy', 'equal, Jewish', 'crazy, Arab, Jewish, half, anti, -, Muslim, justified', 'free, Jewish, deranged, terroristic, military, serious', 'pro, -, israel, jewish, normal, regular', 'Arab, Muslim, general, Saudi', 'nasty, Jewish, literal, Palestinian', 'Jewish, collective, intentional, ethnic', 'British, other, European, Jewish, Palestinian', 'Jewish, Israeli, second, ethnic', 'Jewish, safe, privileged']\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(en_nlp.vocab)\n",
    "\n",
    "# patterns = [\n",
    "#     [{'POS':'ADJ'}, {'POS':'NOUN'}], [{'POS':'AUX'}, {'POS':'ADJ'}], [{'POS':'ADJ'}]\n",
    "#     ]\n",
    "patterns = [\n",
    "    [{'POS':'ADJ'}]\n",
    "    ]\n",
    "matcher.add(\"demo\", patterns)\n",
    "\n",
    "before_adj = []\n",
    "after_adj = []\n",
    "\n",
    "for post in all:\n",
    "    doc = en_nlp(post.text)\n",
    "    x = \"\"\n",
    "    count = 0\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = en_nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        if 'http' not in span.text and span.text not in x:\n",
    "            x = x + span.text + \", \"\n",
    "            count += 1\n",
    "        # print(x + \" \" + str(count))\n",
    "    # Remove the last comma and space\n",
    "    post.adj = x[:-2]  # Remove the last comma and space\n",
    "    post.adj_count = count\n",
    "    \n",
    "    if post.status == \"Before\":\n",
    "        before_adj.append(post.adj)\n",
    "    if post.status == 'After':\n",
    "        after_adj.append(post.adj)\n",
    "\n",
    "print(f'Before: {before_adj}')\n",
    "print(f'After: {after_adj}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'Sentiment_analyzed'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['Status', 'Text', 'Sentiment', 'Toxicity'])\n",
    "            for post in all:\n",
    "                   writer.writerow([post.status, post.text, post.sentiment, post.toxicity, post.adj, post.adj_count])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
