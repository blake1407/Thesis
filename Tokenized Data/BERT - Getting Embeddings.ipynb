{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import ast\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT model and tokenizer once to avoid redundant loading\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify and regenerate tokens\n",
    "def verify_and_fix_tokens(input_df):\n",
    "    for index, row in input_df.iterrows():\n",
    "        tokens = eval(row['Tokens'])  # Convert string to list\n",
    "        regenerated_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Check if Indexed_Tokens match regenerated IDs\n",
    "        if eval(row['Indexed_Tokens']) != regenerated_ids:\n",
    "            print(f\"Mismatch found at index {index}. Fixing...\")\n",
    "            input_df.at[index, 'Indexed_Tokens'] = str(regenerated_ids)  # Fix Indexed_Tokens\n",
    "            \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk long sequences\n",
    "def chunk_long_sequences(tokens, token_ids, segment_ids, max_length=512):\n",
    "    \"\"\"\n",
    "    Splits long sequences into chunks of max_length for processing with BERT.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(token_ids), max_length):\n",
    "        chunk_tokens = tokens[i:i+max_length]\n",
    "        chunk_token_ids = token_ids[i:i+max_length]\n",
    "        chunk_segment_ids = segment_ids[i:i+max_length]\n",
    "        \n",
    "        # Ensure special tokens are handled correctly\n",
    "        if i > 0 and chunk_tokens[0] != '[CLS]':\n",
    "            chunk_tokens = ['[CLS]'] + chunk_tokens\n",
    "            chunk_token_ids = [101] + chunk_token_ids\n",
    "            chunk_segment_ids = [segment_ids[i]] + chunk_segment_ids\n",
    "        \n",
    "        if i + max_length < len(token_ids) and chunk_tokens[-1] != '[SEP]':\n",
    "            chunk_tokens = chunk_tokens + ['[SEP]']\n",
    "            chunk_token_ids = chunk_token_ids + [102]\n",
    "            chunk_segment_ids = chunk_segment_ids + [segment_ids[i]]\n",
    "        \n",
    "        yield chunk_tokens, chunk_token_ids, chunk_segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_export_by_subject(input_df, output_dir=\"output_embeddings\", max_length=512):\n",
    "    \"\"\"\n",
    "    Processes text by grouping all tokens for the same Subject_ID.\n",
    "    Generates BERT embeddings for each subject and exports them individually to CSV files.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pd.DataFrame): Input DataFrame with tokens, token IDs, and segment IDs.\n",
    "        output_dir (str): Directory to save the individual CSV files.\n",
    "        max_length (int): Maximum sequence length for BERT processing.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    grouped = input_df.groupby('Subject_ID')\n",
    "    for subject_id, group in grouped:\n",
    "        print(f\"Processing Subject_ID: {subject_id}\")\n",
    "        \n",
    "        # Concatenate tokens, token IDs, and segment IDs for the same Subject_ID\n",
    "        all_tokens = sum(group['Tokens'].apply(eval).tolist(), [])\n",
    "        all_token_ids = sum(group['Indexed_Tokens'].apply(eval).tolist(), [])\n",
    "        all_segment_ids = sum(group['Segments_IDs'].apply(eval).tolist(), [])\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Use chunk_long_sequences to split long sequences\n",
    "        for chunk_tokens, chunk_token_ids, chunk_segment_ids in chunk_long_sequences(\n",
    "            all_tokens, all_token_ids, all_segment_ids, max_length=max_length\n",
    "        ):\n",
    "            # Convert inputs to tensors\n",
    "            tokens_tensor = torch.tensor([chunk_token_ids])\n",
    "            segments_tensor = torch.tensor([chunk_segment_ids])\n",
    "            \n",
    "            # Get BERT embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor, segments_tensor)\n",
    "                hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "            \n",
    "            # Process each token in the chunk\n",
    "            for j, (token, embedding) in enumerate(zip(chunk_tokens, hidden_states)):\n",
    "                if token in ['[CLS]', '[SEP]']:\n",
    "                    continue\n",
    "                \n",
    "                token_embedding = embedding.numpy().tolist()\n",
    "                \n",
    "                # Store all token information\n",
    "                results.append({\n",
    "                    'Subject_ID': subject_id,\n",
    "                    'Token': token,\n",
    "                    'Token_ID': chunk_token_ids[j],\n",
    "                    'Segment_ID': chunk_segment_ids[j],\n",
    "                    'Position': j,  # Position within the chunk\n",
    "                    'Embedding': token_embedding\n",
    "                })\n",
    "        \n",
    "        # Export results for the current Subject_ID\n",
    "        subject_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(output_dir, f\"{subject_id}_embeddings.csv\")\n",
    "        subject_df.to_csv(output_path, index=False)\n",
    "        print(f\"Exported embeddings for Subject_ID {subject_id} to {output_path}\")\n",
    "\n",
    "# This function now incorporates chunk_long_sequences to handle long sequences effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject_ID: 16\n",
      "Exported embeddings for Subject_ID 16 to subject_embeddings/16_embeddings.csv\n",
      "Processing Subject_ID: 17\n",
      "Exported embeddings for Subject_ID 17 to subject_embeddings/17_embeddings.csv\n",
      "Processing Subject_ID: 20\n",
      "Exported embeddings for Subject_ID 20 to subject_embeddings/20_embeddings.csv\n",
      "Processing Subject_ID: 23\n",
      "Exported embeddings for Subject_ID 23 to subject_embeddings/23_embeddings.csv\n",
      "Processing Subject_ID: 26\n",
      "Exported embeddings for Subject_ID 26 to subject_embeddings/26_embeddings.csv\n",
      "Processing Subject_ID: 28\n",
      "Exported embeddings for Subject_ID 28 to subject_embeddings/28_embeddings.csv\n",
      "Processing Subject_ID: 30\n",
      "Exported embeddings for Subject_ID 30 to subject_embeddings/30_embeddings.csv\n",
      "Processing Subject_ID: 32\n",
      "Exported embeddings for Subject_ID 32 to subject_embeddings/32_embeddings.csv\n",
      "Processing Subject_ID: 39\n",
      "Exported embeddings for Subject_ID 39 to subject_embeddings/39_embeddings.csv\n",
      "Processing Subject_ID: 41\n",
      "Exported embeddings for Subject_ID 41 to subject_embeddings/41_embeddings.csv\n",
      "Processing Subject_ID: 48\n",
      "Exported embeddings for Subject_ID 48 to subject_embeddings/48_embeddings.csv\n",
      "Processing Subject_ID: 52\n",
      "Exported embeddings for Subject_ID 52 to subject_embeddings/52_embeddings.csv\n",
      "Processing Subject_ID: 60\n",
      "Exported embeddings for Subject_ID 60 to subject_embeddings/60_embeddings.csv\n",
      "Processing Subject_ID: 62\n",
      "Exported embeddings for Subject_ID 62 to subject_embeddings/62_embeddings.csv\n",
      "Processing Subject_ID: 63\n",
      "Exported embeddings for Subject_ID 63 to subject_embeddings/63_embeddings.csv\n",
      "Processing Subject_ID: 71\n",
      "Exported embeddings for Subject_ID 71 to subject_embeddings/71_embeddings.csv\n",
      "Processing Subject_ID: 74\n",
      "Exported embeddings for Subject_ID 74 to subject_embeddings/74_embeddings.csv\n",
      "Processing Subject_ID: 75\n",
      "Exported embeddings for Subject_ID 75 to subject_embeddings/75_embeddings.csv\n",
      "Processing Subject_ID: 77\n",
      "Exported embeddings for Subject_ID 77 to subject_embeddings/77_embeddings.csv\n",
      "Processing Subject_ID: 78\n",
      "Exported embeddings for Subject_ID 78 to subject_embeddings/78_embeddings.csv\n",
      "Processing Subject_ID: 79\n",
      "Exported embeddings for Subject_ID 79 to subject_embeddings/79_embeddings.csv\n",
      "Processing Subject_ID: 83\n",
      "Exported embeddings for Subject_ID 83 to subject_embeddings/83_embeddings.csv\n",
      "Processing Subject_ID: 87\n",
      "Exported embeddings for Subject_ID 87 to subject_embeddings/87_embeddings.csv\n",
      "Processing Subject_ID: 92\n",
      "Exported embeddings for Subject_ID 92 to subject_embeddings/92_embeddings.csv\n",
      "Processing Subject_ID: 93\n",
      "Exported embeddings for Subject_ID 93 to subject_embeddings/93_embeddings.csv\n",
      "Processing Subject_ID: 94\n",
      "Exported embeddings for Subject_ID 94 to subject_embeddings/94_embeddings.csv\n",
      "Processing Subject_ID: 95\n",
      "Exported embeddings for Subject_ID 95 to subject_embeddings/95_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Load your tokenized dataset\n",
    "input_df = pd.read_csv(\"Before_Tokenized.csv\")\n",
    "\n",
    "# Apply the function\n",
    "input_df = verify_and_fix_tokens(input_df)\n",
    "\n",
    "# Process and export embeddings\n",
    "process_and_export_by_subject(input_df, output_dir=\"subject_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
