{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "2 corpora - before and after. split().-> bag of words - remove their names to @user for all of them within the content of the tweet not the person who tweeted. Pull the pre-trained bert vectors - hugging face - interate through the list - get the vectors of each words, dont remove duplicates (if dont find vectors for a word - have an output for those words - see if theres any spelling). we can also lemantize them, try to get hte vectors for those lemna, if still no vector - output, keep them somewhere. \n",
    "Dictionary: {Day: {tweet_id: {word}: []}, {word []}, etc.}\n",
    "\n",
    "Once we have the list of words of dehumanizing language, count/number of words in your corpus: (e.g.: 2% were dehumanizing before, 7% after, frequency for each day and have a time-series analysis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: string):\n",
    "    return ' '.join([item for item in text.split() if item not in stopwords.words('english')])\n",
    "\n",
    "def remove_punctuations(text: string):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "def delete_usernames(text: str) -> str:\n",
    "    pattern = r'@\\w+'\n",
    "    return re.sub(pattern, '@user', text)\n",
    "\n",
    "# Function to split \"After Corpus\" into individual sentences and create a new DataFrame\n",
    "def split_sentences(df, status: str):\n",
    "\n",
    "    abbreviations = {\n",
    "        \"U.S.\": \"United States\",\n",
    "        \"U.K.\": \"United Kingdom\",\n",
    "        \"e.g.\": \"for example,\",\n",
    "        \"i.e.\": \"such as,\",\n",
    "        # \"Dr.\": \"DR_PLACEHOLDER\",\n",
    "        # \"Mr.\": \"MR_PLACEHOLDER\",\n",
    "        # \"Ms.\": \"MS_PLACEHOLDER\",\n",
    "        # \"Mrs.\": \"MRS_PLACEHOLDER\",\n",
    "        # \"Jr.\": \"JR_PLACEHOLDER\",\n",
    "        # \"Sr.\": \"SR_PLACEHOLDER\",\n",
    "        \"etc.\": \"...\"\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the rows of the new DataFrame\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the relevant data\n",
    "        subject_id = row['Subject ID']\n",
    "        tweet_id = row['Tweet ID']\n",
    "        corpus = row[f'{status} Corpus']\n",
    "\n",
    "        # Replace abbreviations with placeholders\n",
    "        for abbr, placeholder in abbreviations.items():\n",
    "            corpus = corpus.replace(abbr, placeholder)\n",
    "        \n",
    "        # Split the corpus into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus)\n",
    "\n",
    "        # Add each sentence as a new row in the list, keeping track of the tweet ID and subject ID\n",
    "        for sentence in sentences:\n",
    "            rows.append({'Subject ID': subject_id, 'Tweet ID': tweet_id, 'Sentence': sentence})\n",
    "\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    return new_df\n",
    "\n",
    "# def stem_text(text: string):\n",
    "#     return [ps.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process Text Data\n",
    "clean_data_path = \"Cleaned Data\"\n",
    "after = pd.read_csv(f\"{clean_data_path}/After_NN_Cleaned.csv\")\n",
    "before = pd.read_csv(f\"{clean_data_path}/Before_NN_Cleaned.csv\")\n",
    "\n",
    "# after_corpus = after[[\"After Corpus\", \"Tweet ID\"]].copy()\n",
    "# before_corpus = before[[\"Before Corpus\", \"Tweet ID\"]].copy()\n",
    "\n",
    "# # Apply all cleaning functions to the After Corpus column\n",
    "# after_corpus[\"Clean Text\"] = (after_corpus[\"After Corpus\"]\n",
    "#                              .apply(delete_usernames)\n",
    "#                              .apply(remove_punctuations)\n",
    "#                              .apply(remove_stop_words))\n",
    "# before_corpus[\"Clean Text\"] = (before_corpus[\"Before Corpus\"]\n",
    "#                              .apply(delete_usernames)\n",
    "#                              .apply(remove_punctuations)\n",
    "#                              .apply(remove_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "look-behind requires fixed-width pattern",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the function to split sentences\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m after_sentences \u001b[38;5;241m=\u001b[39m \u001b[43msplit_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAfter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m before_sentences \u001b[38;5;241m=\u001b[39m split_sentences(before, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m after_sentences\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing testing.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 27\u001b[0m, in \u001b[0;36msplit_sentences\u001b[0;34m(df, status)\u001b[0m\n\u001b[1;32m     24\u001b[0m corpus \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Corpus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Split the corpus into sentences using a regex that captures sentence-ending punctuation\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Add each sentence as a new row in the list, keeping track of the tweet ID and subject ID\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/__init__.py:207\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(string, maxsplit)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/__init__.py:307\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m--> 307\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_compiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m DEBUG:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/_compiler.py:749\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43m_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_DEBUG:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/_compiler.py:582\u001b[0m, in \u001b[0;36m_code\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    579\u001b[0m _compile_info(code, p, flags)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# compile the pattern\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m code\u001b[38;5;241m.\u001b[39mappend(SUCCESS)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/re/_compiler.py:155\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, pattern, flags)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlooks too much behind\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lo \u001b[38;5;241m!=\u001b[39m hi:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlook-behind requires fixed-width pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     emit(lo) \u001b[38;5;66;03m# look behind\u001b[39;00m\n\u001b[1;32m    157\u001b[0m _compile(code, av[\u001b[38;5;241m1\u001b[39m], flags)\n",
      "\u001b[0;31merror\u001b[0m: look-behind requires fixed-width pattern"
     ]
    }
   ],
   "source": [
    "# Apply the function to split sentences\n",
    "after_sentences = split_sentences(after, \"After\")\n",
    "before_sentences = split_sentences(before, \"Before\")\n",
    "after_sentences.to_csv(\"Tokenizing testing.csv\")\n",
    "\n",
    "# Show the first few rows of the resulting DataFrame\n",
    "after_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# after_corpus[\"Tokenized Text\"] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
