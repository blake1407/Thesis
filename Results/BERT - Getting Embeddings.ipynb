{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e6cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c714c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable', 'supernatural', 'uneconomical', 'flat', 'unconvinced', 'artless', 'retarded', 'nonprehensile', 'unreasonable', 'foolish', 'uncertain', 'ignorant', 'artificial', 'aft', 'sharp', 'cockamamie', 'unintelligent', 'inexperienced', 'backward', 'decertify', 'unentitled', 'ineligible', 'weak', 'bungling', 'incapable', 'unsusceptible', 'semiskilled', 'uninformed', 'unable', 'unnatural', 'imprudent', 'stale', 'awkward', 'humble', 'worry', 'impolitic', 'derestrict', 'dense', 'naive', 'bad', 'chancy', 'disqualify', 'cool', 'nontechnical', 'indeterminate', 'harebrained', 'nasty', 'brainless', 'clean', 'up_in_the_air', 'cold', 'unsealed', 'unhealthy', 'mild', 'indefinite', 'contingent', 'ineffective', 'asinine', 'doubtful', 'rattlebrained', 'dimmed', 'unskilled', 'inefficient', 'inadvisable', 'rugged', 'styleless', 'helpless', 'outright', 'ambivalent', 'ill-advised', 'unfashionable', 'speechless', 'maladroit', 'hopeless', 'salty', 'manual', 'anserine', 'unmodified', 'imprecise', 'unconditional', 'unqualified', 'dependent', 'dull', 'preserved', 'infelicitous', 'groping', 'unworkmanlike', 'out_of_practice', 'inadequate', 'botchy', 'amateurish', 'inept', 'local', 'categoric', 'dumb', 'diffident', 'feckless', 'unpredictable', 'changeable', 'unrestricted', 'variable', 'unsure', 'lubberly', 'inarticulate', 'independent', 'fond', 'ill-conceived', 'uncertified', 'crude', 'unwise', 'incompetent', 'reverse', 'bungled', 'irrational', 'unprofessional', 'quack', 'unlimited', 'stupid', 'back', 'slow', 'unregistered', 'disagree', 'uneducated', 'ambiguous', 'unfit', 'powerless', 'absurd', 'benignant', 'passionate', 'comradely', 'goodness', 'strong', 'defensive', 'euphemistic', 'hot', 'equable', 'right', 'right-handed', 'authentic', 'humane', 'rich_people', 'synergistic', 'warm', 'nice', 'unfrozen', 'forgiving', 'creditworthy', 'congenial', 'charitable', 'responsible', 'tested', 'rich', 'uncomfortable', 'kind', 'unmalicious', 'peaceful', 'trustworthy', 'noble', 'dear', 'certain', 'faithful', 'benign', 'positive', 'enthusiastic', 'dainty', 'social', 'well', 'amicable', 'supportive', 'left-handed', 'encouraging', 'ample', 'conciliatory', 'cordial', 'certificatory', 'loving', 'nonslippery', 'consistent', 'clean', 'ardent', 'correctly', 'well-intentioned', 'mild', 'alive', 'reconcilable', 'emotional', 'cozy', 'boil', 'hotness', 'desirable', 'heart-whole', 'pleasant', 'affectionate', 'conscious', 'affable', 'soft', 'earnest', 'intended', 'quick', 'unaggressive', 'unskilled', 'unfreeze', 'live', 'fiducial', 'savory', 'unselfish', 'reliable', 'serious', 'fresh', 'genuine', 'warmly', 'kindhearted', 'couthie', 'courteous', 'abnormal', 'collateral', 'honest', 'dependable', 'true', 'incorrupt', 'hearty', 'agonist', 'sociable', 'gentle', 'altruistic', 'amiable', 'accessary', 'unblock', 'bona_fide', 'lovable', 'patriotic', 'inoffensive', 'loyal', 'original', 'accordant', 'chummy', 'ingenuous', 'good', 'paranormal', 'undeviating', 'living', 'rightfulness', 'heat', 'neighborly', 'lively', 'steady', 'correct', 'fastidious', 'excitable', 'estimable', 'merciful', 'gracious', 'good-natured', 'respectable', 'straight', 'hospitable', 'near', 'virtuous', 'accessory', 'warmed', 'precise', 'decent', 'honorable', 'agreeable', 'demonstrative_of', 'sincere', 'generous', 'warming', 'sound', 'polite', 'nonviolent', 'real', 'companionate', 'sure', 'considerate', 'lukewarm', 'friendly', 'imperfect', 'untruthful', 'unreliable', 'unpleasant', 'malignant', 'unreal', 'picaresque', 'disloyal', 'stone-cold', 'nominal', 'insubstantial', 'violent', 'evil', 'barbarous', 'discordant', 'unfaithful', 'hard', 'vicious', 'uncertain', 'comfortable', 'unlovable', 'irresponsible', 'uncordial', 'deceptive', 'intense', 'beggarly', 'unoriginal', 'unsteady', 'frosty', 'inhumane', 'corrupt', 'deceitful', 'refrigerated', 'weak', 'unenthusiastic', 'malign', 'frivolous', 'refrigerant', 'beetle-browed', 'ungracious', 'unvoiced', 'impotent', 'uncharitable', 'bleak', 'aggressive', 'frore', 'hateful', 'egotistic', 'stale', 'unexcitable', 'unpatriotic', 'stingy', 'discourteous', 'disingenuous', 'unemotional', 'neutral', 'algid', 'bad', 'corruptible', 'unintended', 'cool', 'nasty', 'untrustworthy', 'cold', 'insincere', 'unsealed', 'forte', 'selfish', 'cerebral', 'slippery', 'playful', 'dishonest', 'hostile', 'uncongenial', 'inhospitable', 'ill-natured', 'undesirable', 'unsound', 'wicked', 'devious', 'inconsiderate', 'unsupportive', 'unloving', 'badness', 'frigorific', 'perfect', 'base', 'rebellious', 'recreant', 'unconscious', 'poisonous', 'offensive', 'contemptible', 'poor', 'mean', 'un-american', 'bitter', 'malicious', 'fly-by-night', 'unsocial', 'crisp', 'salty', 'emotionless', 'dirty', 'hate', 'chilly', 'abominable', 'unkind', 'at_loggerheads', 'egoistic', 'imprecise', 'head-on', 'acold', 'unsteadily', 'uncivil', 'mutinous', 'discourage', 'dull', 'preserved', 'negative', 'unrespectable', 'passionless', 'unreproducible', 'ambidextrous', 'incoherent', 'heatless', 'unfastidious', 'indecent', 'unneighborly', 'silence', 'impolite', 'frigid', 'evilness', 'rascally', 'false', 'unfriendly', 'wrong', 'faithless', 'opponent', 'arctic', 'merciless', 'unpeaceful', 'loud', 'unsure', 'crooked', 'discouraging', 'lineal', 'thieving', 'hardened', 'inconsistent', 'undependable', 'improperly', 'shivery', 'antagonistic', 'skilled', 'ice-cold', 'condemnable', 'unforgiving', 'dishonorable', 'unsociable', 'unheated', 'counterfeit', 'irreconcilable', 'devoice', 'ill', 'far', 'insurgent', 'average', 'dead', 'frozen', 'bastardly', 'ignoble', 'normal', 'self-serving', 'disagreeable', 'beguiling', 'modified', 'goodness', 'strong', 'educated', 'astute', 'immutable', 'hot', 'artful', 'healthy', 'warm', 'capable', 'nice', 'eligible', 'reassured', 'intense', 'hedged', 'powerful', 'ball-hawking', 'qualified', 'reasonable', 'delicate', 'definite', 'self-assured', 'certain', 'streamlined', 'prudent', 'experienced', 'precocious', 'adequate', 'determinate', 'stylish', 'skillful', 'advisable', 'disingenuous', 'economic', 'restricted', 'limited', 'arch', 'articulate', 'susceptible', 'forward', 'agile', 'confident', 'professional', 'rational', 'brainy', 'sure-handed', 'expeditious', 'workmanlike', 'informed', 'fast', 'cost-efficient', 'quick', 'predictable', 'reliable', 'proud', 'felicitous', 'adept', 'assured', 'adroit', 'certified', 'sophisticated', 'mean', 'unfairly', 'fresh', 'unequivocal', 'unclean', 'dirty', 'businesslike', 'convinced', 'cagey', 'invariable', 'sealed', 'scintillating', 'smart', 'dependent', 'dependable', 'registered', 'politic', 'efficient', 'refined', 'wise', 'good', 'competent', 'able', 'fit', 'well-qualified', 'cocksure', 'hopeful', 'seal', 'heat', 'consummate', 'conditional', 'natural', 'independent', 'versatile', 'intelligent', 'accomplished', 'well-advised', 'skilled', 'streetwise', 'automatic', 'technical', 'unambiguous', 'precise', 'resourceful', 'graceful', 'searching', 'prehensile', 'apt', 'accelerate', 'high-octane', 'unchangeable', 'bright', 'chic', 'proficient', 'unregretful', 'sure', 'effective', 'adequate_to', 'jewish', 'jew', 'jews', 'rabbi', 'rabbis', 'zionism', 'zionist', 'zionists', 'judaism', 'muslim', 'muslims', 'jihad', 'jihads', 'islamic', 'islam', 'arabic', 'arabian', 'gazan', 'gazans', 'palestinian', 'palestinians', 'bedouin', 'bedouins', 'mahmoud', 'abbas', 'fatah', 'israeli', 'israelis', 'israelite', 'benjamin', 'netanyahu', 'itamar', 'ben-gvir', 'idf', 'iof', 'yoav', 'gallant', 'hamas', 'islamist', 'ismail', 'haniyeh', 'mohammed', 'deif', 'yahya', 'sinwar', 'sunni']\n"
     ]
    }
   ],
   "source": [
    "# Load the stereotype dictionary\n",
    "dictionary = \"Create Dictionary/Stereotype_Dictionary.json\"\n",
    "with open(dictionary, 'r') as f:\n",
    "    stereotype_dict = json.load(f)\n",
    "\n",
    "stereotype_df = pd.DataFrame(list(stereotype_dict.items()), columns=['category', 'word'])\n",
    "stereotype_df = stereotype_df.explode(\"word\")\n",
    "stereotype_df[\"word\"] = stereotype_df[\"word\"].str.lower()\n",
    "\n",
    "stereotype_list = stereotype_df.word.tolist()\n",
    "\n",
    "print(stereotype_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc438434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(text: string):\n",
    "    return ' '.join([item for item in text.split() if item not in stopwords.words('english')])\n",
    "\n",
    "def remove_punctuations(text: string):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "def delete_usernames(text: str) -> str:\n",
    "    pattern = r'@\\w+'\n",
    "    return re.sub(pattern, '@user', text)\n",
    "\n",
    "def remove_non_words(text: string):\n",
    "    return re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                  \" \",          # Replace all non-letters with spaces\n",
    "                  str(text))    # Make sure the text input is in string format\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to split \"After Corpus\" into individual sentences and create a new DataFrame\n",
    "def split_sentences(df, status: str):\n",
    "    abbreviations = {\n",
    "        \"U.S.\": \"USA\",\n",
    "        \"U.K.\": \"UK\",\n",
    "        \"e.g.\": \"for example,\",\n",
    "        \"i.e.\": \"such as,\",\n",
    "        \"U.N.\": \"UN\",\n",
    "        \"Gov.\": \"Governor\",\n",
    "        \"etc.\": \"...\"\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the rows of the new DataFrame\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract the relevant data\n",
    "        subject_id = row['Subject_ID']\n",
    "        tweet_id = row['Tweet_ID']\n",
    "        corpus = row[f'{status}_Corpus']\n",
    "\n",
    "        # Replace abbreviations with placeholders\n",
    "        for abbr, placeholder in abbreviations.items():\n",
    "            corpus = corpus.replace(abbr, placeholder)\n",
    "        \n",
    "        # Split the corpus into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus)\n",
    "\n",
    "        # Add each sentence as a new row in the list, keeping track of the tweet ID and subject ID\n",
    "        for sentence in sentences:\n",
    "            rows.append({'Subject_ID': subject_id, 'Tweet_ID': tweet_id, 'Sentence': sentence})\n",
    "\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    return new_df\n",
    "\n",
    "def get_segments_ids(df, subid):\n",
    "    subjects = df[subid].unique()\n",
    "    for sub in subjects:\n",
    "        segments_ids = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Create a list of repeated IDs for each sentence\n",
    "            segment_id_list = [idx] * len(row['Tokens'])\n",
    "            segments_ids.append(segment_id_list)\n",
    "        df[\"Segments_IDs\"] = segments_ids\n",
    "    return df[\"Segments_IDs\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f9d4c",
   "metadata": {},
   "source": [
    "GETTING EMBEDDINGS AT THE SENTENCE LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018bfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bert_sentences(sentences_df):\n",
    "    \"\"\"\n",
    "    Process sentences through BERT after padding, with CUDA acceleration.\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = {} # dictionary to store all the outputs per subject\n",
    "    \n",
    "    # Get all unique subjects\n",
    "    subjects = sentences_df['Subject_ID'].unique()\n",
    "\n",
    "    # Check and set up CUDA device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    for subj in subjects:\n",
    "        print(f\"Subject: {subj}\")\n",
    "        # Subset df to just that subject\n",
    "        df_subj = sentences_df[sentences_df[\"Subject_ID\"] == subj]\n",
    "        \n",
    "        sentences = df_subj['Sentence'].tolist()\n",
    "\n",
    "        # Initialize model and move to GPU\n",
    "        model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                          output_hidden_states=True,\n",
    "                                          return_dict=True)\n",
    "        model.to(device)\n",
    "    \n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Tokenize with padding and move to GPU\n",
    "        inputs = tokenizer(sentences, \n",
    "                           padding='max_length', \n",
    "                           truncation=True, \n",
    "                           max_length=512, \n",
    "                           return_tensors=\"pt\")\n",
    "        \n",
    "        # Move input tensors to GPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Process through BERT\n",
    "        with torch.no_grad():\n",
    "            # When return_dict=True, this returns a BaseModelOutputWithPoolingAndCrossAttentions object\n",
    "            output = model(**inputs)\n",
    "            \n",
    "        # Handle the output based on its type\n",
    "        if isinstance(output, dict):\n",
    "            # If it's already a dictionary (with return_dict=True)\n",
    "            processed_output = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in output.items()}\n",
    "        else:\n",
    "            # If it's a tuple (default BERT output)\n",
    "            processed_output = {\n",
    "                'last_hidden_state': output[0].cpu(),\n",
    "                'pooler_output': output[1].cpu(),\n",
    "                'hidden_states': tuple(h.cpu() for h in output[2]) if len(output) > 2 else None\n",
    "            }\n",
    "        \n",
    "        outputs[subj] = processed_output\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "def tokenize_function(row, return_type, return_tensor=True):\n",
    "    \"\"\"\n",
    "    Updated tokenization function with CUDA support\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if return_tensor:\n",
    "        tokenizer_results = tokenizer(row, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = tokenizer_results['input_ids'].to(device)\n",
    "        token_type_ids = tokenizer_results['token_type_ids'].to(device)\n",
    "        attention_mask = tokenizer_results['attention_mask'].to(device)\n",
    "        \n",
    "        if return_type == 'input_ids':\n",
    "            return input_ids\n",
    "        elif return_type == 'token_type_ids':\n",
    "            return token_type_ids\n",
    "        elif return_type == 'attention_mask':\n",
    "            return attention_mask\n",
    "        else:\n",
    "            return 'return_type not recognized. Please enter \"input_ids\", \"token_type_ids\", or \"attention_mask\".'\n",
    "    else:\n",
    "        tokenizer_results = tokenizer(row, truncation=True)\n",
    "        input_ids = tokenizer_results['input_ids']\n",
    "        token_type_ids = tokenizer_results['token_type_ids']\n",
    "        attention_mask = tokenizer_results['attention_mask']\n",
    "        \n",
    "        if return_type == 'input_ids':\n",
    "            return input_ids\n",
    "        elif return_type == 'token_type_ids':\n",
    "            return token_type_ids\n",
    "        elif return_type == 'attention_mask':\n",
    "            return attention_mask\n",
    "        else:\n",
    "            return 'return_type not recognized. Please enter \"input_ids\", \"token_type_ids\", or \"attention_mask\".'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e516c",
   "metadata": {},
   "source": [
    "GETTING EMBEDDINGS AT THE WORD LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6d142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bert_words(sentences_df, status: str, batch_size=32, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Process sentences through BERT to get word-level representations, with CUDA acceleration and memory-efficient processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - sentences_df: DataFrame containing sentences\n",
    "    - status: String to identify the processing status\n",
    "    - batch_size: Number of sentences to process in each batch\n",
    "    - accumulation_steps: Number of batches to accumulate gradients (helps with memory management)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_outputs = {}  # dictionary to store all the word-level outputs per subject\n",
    "    \n",
    "    # Get all unique subjects\n",
    "    subjects = sentences_df['Subject_ID'].unique()\n",
    "\n",
    "    # Check and set up CUDA device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    for subj in subjects:\n",
    "        print(f\"Processing Subject: {subj}\")\n",
    "        \n",
    "        # Subset df to just that subject\n",
    "        df_subj = sentences_df[sentences_df[\"Subject_ID\"] == subj]\n",
    "        sentences = df_subj['Sentence'].tolist()\n",
    "\n",
    "        # Initialize model and move to GPU\n",
    "        model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                          output_hidden_states=True,\n",
    "                                          return_dict=True)\n",
    "        model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Process sentences in batches\n",
    "        for start in range(0, len(sentences), batch_size * accumulation_steps):\n",
    "            # Select batch of sentences\n",
    "            batch_sentences = sentences[start:start + batch_size * accumulation_steps]\n",
    "            \n",
    "            # Tokenize with padding and move to GPU\n",
    "            inputs = tokenizer(batch_sentences, \n",
    "                               padding='max_length', \n",
    "                               truncation=True, \n",
    "                               max_length=512, \n",
    "                               return_tensors=\"pt\")\n",
    "            \n",
    "            # Move input tensors to GPU\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Process through BERT\n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "            \n",
    "            # Extract word-level representations\n",
    "            word_level_outputs = output.last_hidden_state\n",
    "            \n",
    "            # Iterate over each sentence and get the word vectors\n",
    "            for i, sentence in enumerate(batch_sentences):\n",
    "                word_tokens = tokenizer.tokenize(sentence)\n",
    "                word_vectors = word_level_outputs[i, :len(word_tokens), :]\n",
    "                \n",
    "                # Store the word-level outputs per subject\n",
    "                for j, word in enumerate(word_tokens):\n",
    "                    if word in stereotype_list:\n",
    "                        if subj not in word_outputs:\n",
    "                            word_outputs[subj] = []\n",
    "                        word_outputs[subj].append({\n",
    "                            'subject_id': subj,\n",
    "                            'word': word,\n",
    "                            'vector': word_vectors[j].cpu().numpy()\n",
    "                        })\n",
    "            \n",
    "            # Free up GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame and save it to a CSV file\n",
    "    df = pd.DataFrame()\n",
    "    for subj, words in word_outputs.items():\n",
    "        df = pd.concat([df, pd.DataFrame(words)], ignore_index=True)\n",
    "    df.to_csv(f\"D:/BERT vectors/BERT_{status}_word_vectors.csv\", index=False)\n",
    "\n",
    "    return word_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bed629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Affiliation_Contrast</th>\n",
       "      <th>Before_Dates</th>\n",
       "      <th>Before_Corpus</th>\n",
       "      <th>Before_Likes</th>\n",
       "      <th>Before_Retweets</th>\n",
       "      <th>Before_Replies</th>\n",
       "      <th>Before_Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>1000000</td>\n",
       "      <td>5438</td>\n",
       "      <td>0.5</td>\n",
       "      <td>October 13, 2022 02:51 AM</td>\n",
       "      <td>He makes me sick to my stomach. Hadn’t those f...</td>\n",
       "      <td>3521</td>\n",
       "      <td>405</td>\n",
       "      <td>69</td>\n",
       "      <td>42919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>1500000</td>\n",
       "      <td>3168</td>\n",
       "      <td>0.5</td>\n",
       "      <td>September 21, 2023 12:54 PM</td>\n",
       "      <td>\"There is no deal between the United States, B...</td>\n",
       "      <td>1420</td>\n",
       "      <td>459</td>\n",
       "      <td>90</td>\n",
       "      <td>122221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>2700000</td>\n",
       "      <td>1502</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>May 19, 2023 05:39 AM</td>\n",
       "      <td>Biden gang walks back claim drone strike kille...</td>\n",
       "      <td>1873</td>\n",
       "      <td>575</td>\n",
       "      <td>75</td>\n",
       "      <td>115012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>437700</td>\n",
       "      <td>9781</td>\n",
       "      <td>0.5</td>\n",
       "      <td>November 04, 2022 05:00 PM</td>\n",
       "      <td>I try not to invoke my Jewish identity because...</td>\n",
       "      <td>1442</td>\n",
       "      <td>271</td>\n",
       "      <td>23</td>\n",
       "      <td>27317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>7000000</td>\n",
       "      <td>3261</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>July 24, 2023 02:18 PM</td>\n",
       "      <td>The current Israeli government had to pass the...</td>\n",
       "      <td>5228</td>\n",
       "      <td>892</td>\n",
       "      <td>827</td>\n",
       "      <td>1489217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_ID       Affiliation  Followers  Tweet_ID  Affiliation_Contrast  \\\n",
       "0          26  Democratic Party    1000000      5438                   0.5   \n",
       "1          92  Democratic Party    1500000      3168                   0.5   \n",
       "2          87  Republican Party    2700000      1502                  -0.5   \n",
       "3          52  Democratic Party     437700      9781                   0.5   \n",
       "4          41  Republican Party    7000000      3261                  -0.5   \n",
       "\n",
       "                  Before_Dates  \\\n",
       "0    October 13, 2022 02:51 AM   \n",
       "1  September 21, 2023 12:54 PM   \n",
       "2        May 19, 2023 05:39 AM   \n",
       "3   November 04, 2022 05:00 PM   \n",
       "4       July 24, 2023 02:18 PM   \n",
       "\n",
       "                                       Before_Corpus  Before_Likes  \\\n",
       "0  He makes me sick to my stomach. Hadn’t those f...          3521   \n",
       "1  \"There is no deal between the United States, B...          1420   \n",
       "2  Biden gang walks back claim drone strike kille...          1873   \n",
       "3  I try not to invoke my Jewish identity because...          1442   \n",
       "4  The current Israeli government had to pass the...          5228   \n",
       "\n",
       "   Before_Retweets  Before_Replies  Before_Views  \n",
       "0              405              69         42919  \n",
       "1              459              90        122221  \n",
       "2              575              75        115012  \n",
       "3              271              23         27317  \n",
       "4              892             827       1489217  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = pd.read_csv(f\"Cleaned Data/New_Before_NN_Cleaned.csv\")\n",
    "after = pd.read_csv(f\"Cleaned Data/New_After_NN_Cleaned.csv\")\n",
    "\n",
    "before.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747d00e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e699ef8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>5438</td>\n",
       "      <td>[CLS] He makes sick stomach [SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>5438</td>\n",
       "      <td>[CLS] Hadn t families suffered enough [SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>5438</td>\n",
       "      <td>[CLS] What kind monster needs exacerbate pain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>5438</td>\n",
       "      <td>[CLS] What parent planet stand someone like [SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>5438</td>\n",
       "      <td>[CLS] I don t get [SEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_ID  Tweet_ID                                           Sentence\n",
       "0          26      5438                  [CLS] He makes sick stomach [SEP]\n",
       "1          26      5438        [CLS] Hadn t families suffered enough [SEP]\n",
       "2          26      5438  [CLS] What kind monster needs exacerbate pain ...\n",
       "3          26      5438  [CLS] What parent planet stand someone like [SEP]\n",
       "4          26      5438                            [CLS] I don t get [SEP]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to split sentences\n",
    "before_sentences = split_sentences(before, \"Before\")\n",
    "\n",
    "\n",
    "# Apply all cleaning functions to the Before Corpus column\n",
    "before_sentences[\"Sentence\"] = (before_sentences[\"Sentence\"]\n",
    "                                .apply(delete_usernames)\n",
    "                                .apply(remove_punctuations)\n",
    "                                .apply(remove_stop_words)\n",
    "                                .apply(remove_non_words))\n",
    "\n",
    "# Apply sentence markers\n",
    "before_sentences[\"Sentence\"] = (\"[CLS] \"+ before_sentences[\"Sentence\"] + \" [SEP]\")\n",
    "\n",
    "# Show the first few rows of the resulting DataFrame\n",
    "before_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86db5755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>2038</td>\n",
       "      <td>[CLS] Today user user sued extremist leftist g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>2038</td>\n",
       "      <td>[CLS] Media Matters notorious trying suppress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>2038</td>\n",
       "      <td>[CLS] After reading complaint I reminded NBC N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>2038</td>\n",
       "      <td>[CLS] COMPLAINT   [SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>2038</td>\n",
       "      <td>[CLS] Defendant Media Matters America  Media M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_ID  Tweet_ID                                           Sentence\n",
       "0          87      2038  [CLS] Today user user sued extremist leftist g...\n",
       "1          87      2038  [CLS] Media Matters notorious trying suppress ...\n",
       "2          87      2038  [CLS] After reading complaint I reminded NBC N...\n",
       "3          87      2038                            [CLS] COMPLAINT   [SEP]\n",
       "4          87      2038  [CLS] Defendant Media Matters America  Media M..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_sentences = split_sentences(after, \"After\")\n",
    "\n",
    "# Apply all cleaning functions to the After Corpus column\n",
    "after_sentences[\"Sentence\"] = (after_sentences[\"Sentence\"]\n",
    "                                .apply(delete_usernames)\n",
    "                                .apply(remove_punctuations)\n",
    "                                .apply(remove_stop_words)\n",
    "                                .apply(remove_non_words))\n",
    "\n",
    "# Apply sentence markers\n",
    "after_sentences[\"Sentence\"] = (\"[CLS] \"+ after_sentences[\"Sentence\"] + \" [SEP]\")\n",
    "\n",
    "# Show the first few rows of the resulting DataFrame\n",
    "after_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d97150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "before_sentences[\"Tokens\"] = before_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"input_ids\", return_tensor = False)\n",
    "before_sentences[\"Token_type\"] = before_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"token_type_ids\", return_tensor = False)\n",
    "before_sentences[\"Attention_mask\"] = before_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"attention_mask\", return_tensor = False)\n",
    "\n",
    "# Get Segment IDs based on the number of sentences, all tokens in the same sentence get the same ID.\n",
    "before_sentences[\"Segments_IDs\"] = get_segments_ids(before_sentences, \"Subject_ID\")\n",
    "\n",
    "# before_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ce2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "after_sentences[\"Tokens\"] = after_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"input_ids\", return_tensor = False)\n",
    "after_sentences[\"Token_type\"] = after_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"token_type_ids\", return_tensor = False)\n",
    "after_sentences[\"Attention_mask\"] = after_sentences[\"Sentence\"].apply(tokenize_function, return_type = \"attention_mask\", return_tensor = False)\n",
    "\n",
    "# Get Segment IDs based on the number of sentences, all tokens in the same sentence get the same ID.\n",
    "after_sentences[\"Segments_IDs\"] = get_segments_ids(after_sentences, \"Subject_ID\")\n",
    "\n",
    "# after_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ed21da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing Subject: 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nhath\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subject: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# before_outputs = process_bert_words(before_sentences, \"Before\")\n",
    "after_outputs = process_bert_words(after_sentences, \"After\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d7f120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_sentences.to_csv(\"Tokenized Data/After_Tokenized.csv\", index=False)\n",
    "# before_sentences.to_csv(\"Tokenized Data/Before_Tokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cb7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def serialize_bert_outputs(outputs, max_subjects=1):\n",
    "#     \"\"\"\n",
    "#     Convert BERT outputs to a JSON-serializable format\n",
    "#     Limits to first few subjects to manage memory\n",
    "#     \"\"\"\n",
    "#     serializable_outputs = {}\n",
    "    \n",
    "#     # Limit to first max_subjects\n",
    "#     subject_keys = list(outputs.keys())[:max_subjects]\n",
    "    \n",
    "#     for subject in subject_keys:\n",
    "#         output = outputs[subject]\n",
    "#         serializable_subject = {}\n",
    "        \n",
    "#         for key, value in output.items():\n",
    "#             try:\n",
    "#                 if isinstance(value, torch.Tensor):\n",
    "#                     # Move to CPU and convert to list\n",
    "#                     # Add shape and dtype information for debugging\n",
    "#                     print(f\"Processing {key}: Tensor shape {value.shape}, dtype {value.dtype}\")\n",
    "#                     serializable_subject[key] = {\n",
    "#                         'data': value.cpu().numpy().tolist(),\n",
    "#                         'shape': list(value.shape),\n",
    "#                         'dtype': str(value.dtype)\n",
    "#                     }\n",
    "#                 elif isinstance(value, tuple):\n",
    "#                     # Handle tuple of tensors (like hidden_states)\n",
    "#                     processed_tuple = []\n",
    "#                     for t in value:\n",
    "#                         if torch.is_tensor(t):\n",
    "#                             print(f\"Tuple item shape {t.shape}, dtype {t.dtype}\")\n",
    "#                             processed_tuple.append({\n",
    "#                                 'data': t.cpu().numpy().tolist(),\n",
    "#                                 'shape': list(t.shape),\n",
    "#                                 'dtype': str(t.dtype)\n",
    "#                             })\n",
    "#                         else:\n",
    "#                             processed_tuple.append(t)\n",
    "#                     serializable_subject[key] = processed_tuple\n",
    "#                 elif isinstance(value, (np.ndarray, torch.Tensor)):\n",
    "#                     # Convert numpy array or remaining tensors to list\n",
    "#                     serializable_subject[key] = value.cpu().numpy().tolist()\n",
    "#                 elif isinstance(value, (np.int64, np.int32, int)):\n",
    "#                     # Convert integers\n",
    "#                     serializable_subject[key] = int(value)\n",
    "#                 else:\n",
    "#                     serializable_subject[key] = str(value)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing key {key}: {e}\")\n",
    "#                 serializable_subject[key] = str(value)\n",
    "        \n",
    "#         serializable_outputs[str(subject)] = serializable_subject\n",
    "    \n",
    "#     return serializable_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d55714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memory-efficient serialization\n",
    "# try:\n",
    "#     # Only process first subject\n",
    "#     before_outputs_serializable = serialize_bert_outputs(before_outputs)\n",
    "    \n",
    "#     # Saving to JSON with reduced indentation to save memory\n",
    "#     with open(\"Tokenized Data//BERT_Before_Tokenized.json\", \"w\") as f:\n",
    "#         json.dump(before_outputs_serializable, f, indent=2)\n",
    "    \n",
    "#     print(\"Serialization complete. Check the JSON file.\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during serialization: {e}\")\n",
    "#     # Print more detailed error information\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
